---
code-fold: show
---

# Przykład uczenia sieci splotowej

Konieczność trenowania modelu klasyfikacji obrazów przy użyciu bardzo małej ilości danych jest częstą sytuacją, z którą prawdopodobnie spotkasz się w praktyce.
"Kilka" próbek może oznaczać różnie od kilku setek do kilkudziesięciu tysięcy obrazów.
Jako praktyczny przykład, skupimy się na klasyfikacji obrazów jako psów albo kotów, w zbiorze danych zawierającym 4000 zdjęć kotów i psów (2000 kotów, 2000 psów).
Użyjemy 2000 zdjęć do treningu, 1000 do walidacji i 1000 do testów.

W tym rozdziale omówimy jedną z podstawowych strategii radzenia sobie z tym problemem: trenowanie nowego modelu od podstaw przy użyciu niewielkiej ilości danych.
Rozpoczniemy od naiwnego wytrenowania małej sieci splotowej na 2000 próbkach treningowych, bez żadnej regularyzacji, aby ustalić bazę tego, co można osiągnąć.
Pozwoli to osiągnąć dokładność klasyfikacji na poziomie 71%.
W tym momencie głównym problemem będzie *overfitting*.
Następnie przedstawimy augmentację danych, technikę łagodzenia *overfitting* w wizji komputerowej.
Używając augmentacji danych, poprawisz sieć do poziomu klasyfikacji 82%.

W dalszej części omówimy dwie kolejne techniki niezbędne do zastosowania głębokiego uczenia na małych zbiorach danych: ekstrakcję cech za pomocą wstępnie wytrenowanej sieci (co pozwoli osiągnąć dokładność od 90% do 96%) oraz dostrajanie wstępnie wytrenowanej sieci (co pozwoli osiągnąć ostateczną dokładność 97%).
Razem, te trzy strategie - trenowanie małego modelu od zera, ekstrakcja cech przy użyciu wstępnie wytrenowanego modelu i dostrajanie wstępnie wytrenowanego modelu - będą stanowić twój przyszły zestaw narzędzi do radzenia sobie z problemem wizji komputerowej z małymi zbiorami danych.

Czasami można usłyszeć, że głębokie uczenie działa tylko wtedy, gdy dostępna jest duża ilość danych.
Jest to tylko częściowo prawdziwe: jedną z fundamentalnych cech głębokiego uczenia jest to, że może ono znaleźć interesujące cechy w danych treningowych samodzielnie, bez potrzeby ręcznej inżynierii cech, a to można osiągnąć tylko wtedy, gdy dostępna jest duża ilość przykładów treningowych.
Jest to szczególnie prawdziwe dla problemów, w których próbki wejściowe są wielowymiarowe, jak obrazy.

To, co stanowi dużą ilość próbek, jest względne - na przykład w odniesieniu do rozmiaru i głębokości sieci, którą próbujesz wytrenować.
Nie jest możliwe wytrenowanie sieci splotowej do rozwiązania złożonego problemu przy użyciu zaledwie kilkudziesięciu próbek, ale kilkaset może potencjalnie wystarczyć, jeśli model jest mały i dobrze wyregulowany, a zadanie proste.
Ponieważ sieci splotowe uczą się lokalnych, niezmiennych w czasie cech, są bardzo wydajne w przetwarzaniu danych dla problemów percepcyjnych.
Trening sieci konwolucyjne od podstaw na bardzo małym zbiorze obrazów pozwoli na uzyskanie rozsądnych wyników pomimo względnego braku danych, bez potrzeby tworzenia własnych cech.

Co więcej, modele uczenia głębokiego są z natury bardzo uniwersalne: możesz wziąć, powiedzmy, model klasyfikacji obrazów lub mowy/tekstu wytrenowany na dużym zbiorze danych i użyć go ponownie do znacznie innego problemu z niewielkimi zmianami.
W przypadku wizji komputerowej, wiele wstępnie wytrenowanych modeli (zazwyczaj wytrenowanych na zbiorze danych ImageNet) jest publicznie dostępnych do pobrania i może być wykorzystanych do stworzenia potężnych modeli wizji z bardzo małej ilości danych.

## Pobieranie danych

Zestaw danych *Dogs vs. Cats*, z którego będziesz korzystać, nie jest dołączony do `keras`.
Został udostępniony przez *Kaggle* w ramach konkursu wizji komputerowej pod koniec 2013 roku, jeszcze w czasach, gdy sieci splotowe nie były głównym nurtem.
Oryginalny zbiór danych można pobrać ze strony <https://www.kaggle.com/competitions/dogs-vs-cats/data>[^example-1].Zdjęcia to kolorowe JPEG-i o średniej rozdzielczości.
Rysunek 5.8 pokazuje kilka przykładów.

[^example-1]: konieczne będzie założenie konta w Kaggle, jeśli jeszcze go nie masz - bez obaw, proces jest bezbolesny

Nie jest zaskoczeniem, że konkurs Kaggle *cat-versus-dogs* w 2013 roku został wygrany przez uczestników, którzy użyli sieci konwolucyjnych.
Najlepsze struktury osiągnęły do 95% dokładności.
W tym przykładzie uzyskasz wynik bliski tej wartości, mimo że będziesz trenował swoje modele na mniej niż 10% danych, które były dostępne dla konkurencji.

![Kilka przykładowych obrazów ze zbioru](images/Zrzut%20ekranu%202023-03-2%20o%2021.14.55.png){#fig-kaggle1 fig-align="center" width="600"}

Ten zbiór danych zawiera 25000 obrazów psów i kotów (12500 z każdej klasy) i ma rozmiar 569 MB (skompresowany)[^example-2].
Po pobraniu i rozpakowaniu, utworzysz nowy zbiór danych zawierający trzy podzbiory: zbiór treningowy z 1000 próbek każdej klasy, zbiór walidacyjny z 500 próbkami każdej klasy oraz zbiór testowy z 500 próbkami każdej klasy.
Poniżej znajduje się kod do wykonania tego zadania.

[^example-2]: w ściągniętym pliku `dogs-vs-cats.zip` są zawarte trzy pliki, a skupiamy się na `train.zip` i to ten rozpakowujemy

``` r
#| eval: false
original_dataset_dir <- "/Users/majerek/Downloads/dogs-vs-cats/train" # <1>
base_dir <- "/Users/majerek/Downloads/cats_and_dogs_small" # <2>
dir.create(base_dir) # <3>
train_dir <- file.path(base_dir, "train")
dir.create(train_dir)
validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)
test_dir <- file.path(base_dir, "test")
dir.create(test_dir)
train_cats_dir <- file.path(train_dir, "cats")
dir.create(train_cats_dir)
train_dogs_dir <- file.path(train_dir, "dogs")
dir.create(train_dogs_dir)
validation_cats_dir <- file.path(validation_dir, "cats")
dir.create(validation_cats_dir)
validation_dogs_dir <- file.path(validation_dir, "dogs")
dir.create(validation_dogs_dir)
test_cats_dir <- file.path(test_dir, "cats")
dir.create(test_cats_dir)
test_dogs_dir <- file.path(test_dir, "dogs")
dir.create(test_dogs_dir)
fnames <- paste0("cat.", 1:1000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_cats_dir))
fnames <- paste0("cat.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_cats_dir))
fnames <- paste0("cat.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir))
fnames <- paste0("dog.", 1:1000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))
fnames <- paste0("dog.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir))
fnames <- paste0("dog.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))
```

1.  tu ustaw swoją ścieżkę do katalogu z rozpakowanym katalogiem `train`
2.  ustaw ścieżkę gdzie chcesz zapisywać obrazy do uczenia sieci

Jako sprawdzenie poprawności, policzmy, ile zdjęć jest w każdym podziale treningowym (train/validation/test):

```{r}
#| echo: false

load("data/katalogi.rda")
```

```{r}
cat("total training cat images:", length(list.files(train_cats_dir)), "\n")

cat("total training dog images:", length(list.files(train_dogs_dir)), "\n")

cat("total validation cat images:", length(list.files(validation_cats_dir)), "\n")

cat("total validation dog images:", length(list.files(validation_dogs_dir)), "\n")

cat("total test cat images:", length(list.files(test_cats_dir)), "\n")

cat("total test dog images:", length(list.files(test_dogs_dir)), "\n")
```

Tak więc masz rzeczywiście 2000 obrazów treningowych, 1000 obrazów walidacyjnych i 1000 obrazów testowych.
Każdy podział zawiera taką samą liczbę próbek z każdej klasy: jest to zrównoważony problem klasyfikacji binarnej, co oznacza, że dokładność klasyfikacji będzie odpowiednią miarą dopasowania.

## Budowa sieci

W poprzednim przykładzie zbudowaliśmy małą sieć splotową dla MNIST.
Ponownie użyjemy tej samej ogólnej struktury: sieć będzie stosem naprzemiennych warstw `layer_conv_2d` (z aktywacją `relu`) i `layer_max_pooling_2d`.

Ale ponieważ mamy do czynienia z większymi obrazami i bardziej złożonym problemem, sprawimy, że nasza sieć będzie odpowiednio większa: będzie miała jeszcze jedną kombinację `layer_conv_2d` + `layer_max_pooling_2d`.
Służy to zarówno zwiększeniu pojemności sieci, jak i dalszemu zmniejszeniu rozmiaru map funkcji, aby nie były zbyt duże, gdy dojdziemy do `layer_flatten`.
Tutaj, ponieważ zaczynamy od wejść o rozmiarze 150 × 150 (nieco arbitralny wybór), kończymy z mapami cech o rozmiarze 7 × 7 tuż przed `layer_flatten`.

::: callout-important
Głębokość map cech stopniowo zwiększa się w sieci (od 32 do 128), natomiast rozmiar map cech maleje (od 148 × 148 do 7 × 7).
Jest to wzór, który zobaczysz w prawie wszystkich sieciach splotowych.
:::

Ponieważ zajmujemy się problemem klasyfikacji binarnej, zakończymy sieć pojedynczą warstwą (`layer_dense` o rozmiarze 1) i sigmoidalną aktywacją.
Ta warstwa będzie kodować prawdopodobieństwo wystąpienia jednej lub drugiej klasy.

```{r}
library(keras)

model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model
```

W kroku kompilacji, użyjemy optymalizatora `RMSprop`.
Ponieważ sieć kończy się pojedynczą jednostką sigmoidalną, użyjemy binarnej crossentropii jako funkcji straty.

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```

## Przygotowanie danych

Jak już wiemy, dane przed wprowadzeniem do sieci powinny być sformatowane w odpowiednio wstępnie przetworzone tensory zmiennoprzecinkowe.
Obecnie dane są zapisane na dysku w postaci plików JPEG, więc kroki w celu wprowadzenia ich do sieci wyglądają mniej więcej tak:

1.  Odczytaj pliki z obrazkami.
2.  Zdekoduj zawartość JPEG na siatki pikseli RGB.
3.  Przekształć je na tensory zmiennoprzecinkowe.
4.  Przeskaluj wartości pikseli (między 0 a 255) do przedziału \[0, 1\] (jak wiadomo, sieci neuronowe wolą mieć do czynienia z małymi wartościami wejściowymi).

To może wydawać się nieco zniechęcające, ale na szczęście `keras` ma narzędzia, które zajmują się tymi krokami automatycznie.
`keras` zawiera wiele narzędzi pomocniczych do przetwarzania obrazów.
W szczególności, zawiera funkcję `image_data_generator()`, która może automatycznie przekształcić pliki graficzne na dysku w partie wstępnie przetworzonych tensorów.
To właśnie z niej będziemy tutaj korzystać.

```{r}
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
```

Przyjrzyjmy się wyjściu jednego z takich generatorów: daje on partie obrazów RGB o wymiarach 150 × 150 (kształt `(20, 150, 150, 3)`) oraz binarne etykiety (kształt (20)).
W każdej partii znajduje się 20 próbek (rozmiar partii).
Zauważ, że generator tworzy te partie w nieskończoność: zapętla się bez końca nad obrazami w folderze docelowym.

```{r}
batch <- generator_next(train_generator)
str(batch)
```

Dopasujmy model do danych uzyskanych za pomocą generatora.
Robimy to za pomocą funkcji `fit`.
Jako pierwszy argument oczekuje ona generatora, który będzie generował partie danych wejściowych i docelowych.
Ponieważ dane są generowane w nieskończoność, generator musi wiedzieć, ile próbek pobrać z generatora, zanim zadeklaruje koniec epoki.
Taką rolę pełni argument `steps_per_epoch`: po pobraniu z generatora partii próbek - czyli po wykonaniu kroków spadku gradientu - proces dopasowania przejdzie do następnej epoki.
W tym przypadku, partie są 20-próbkowe, więc zajmie to 100 partii, aż do osiągnięcia celu 2000 próbek.

Kiedy używasz `fit`, możesz przekazać też argument `validation_data`.
Ważne jest, aby zauważyć, że ten argument może być zbiorem danych, ale może to być również lista tablic.
Jeśli przekażesz generator jako `validation_data`, to oczekuje się, że ten generator będzie dawał partie danych walidacyjnych w nieskończoność; dlatego powinieneś również określić argument `validation_steps`, który mówi procesowi ile partii ma pobrać z generatora walidacji do oceny.

```{r}
#| eval: false
history <- model %>% fit(
  train_generator,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator,
  validation_steps = 50
)

# można też zapisać model
save_model_hdf5(model, filepath = "models/mod_conv.h5")
```

```{r}
#| echo: false

model <- load_model_tf("models/exm_conv1/")
load("models/exm_hist1.rda")
```

```{r}
plot(history)
```

Te wykresy są charakterystyczne dla nadmiernego dopasowania.
Dokładność szkolenia rośnie liniowo w czasie, aż osiąga prawie 100%, natomiast dokładność walidacji zatrzymuje się na poziomie 70-74%.
Strata związana z walidacją osiąga swoje minimum już po pięciu epokach, a następnie zatrzymuje się, podczas gdy strata związana z treningiem zmniejsza się liniowo, aż osiągnie prawie 0.

Ponieważ mamy stosunkowo mało próbek treningowych (2,000), *overfitting* będzie naszym najczęstszym problemem.
Wiemy już o kilku technikach, które mogą pomóc złagodzić *overfitting*, takich jak *dropout* i regularyzacje L1 i L2.
Teraz wprowadzimy nową, specyficzną dla wizji komputerowej i używaną niemal powszechnie podczas przetwarzania obrazów za pomocą modeli głębokiego uczenia: augmentację danych.
