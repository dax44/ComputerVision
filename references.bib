@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97â€“111},
  numpages = {15}
}



@book{burgerDigitalImageProcessing2016,
  title = {Digital {{Image Processing}}: {{An Algorithmic Introduction Using Java}}},
  shorttitle = {Digital {{Image Processing}}},
  author = {Burger, Wilhelm and Burge, Mark J.},
  year = {2016},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer}},
  address = {{London}},
  doi = {10.1007/978-1-4471-6684-9},
  isbn = {978-1-4471-6683-2 978-1-4471-6684-9},
  langid = {english},
  keywords = {Digital Image Processing,Discrete Fourier Transform,GSM - Postponed Project,Image Processing,Image Processing System,ImageJ,Java,Medical Imaging}
}


@book{szeliskiComputerVisionAlgorithms2022,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2022},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-34372-9},
  isbn = {978-3-030-34371-2 978-3-030-34372-9},
  langid = {english},
  keywords = {3D Reconstruction,Computational Photography,Computer Vision,Deep Learning,Feature Detection and Matching,Image Processing,Image Segmentation,Image Stitching,Image-Based Rendering,Motion Estimation,Scene Recognition,Structure from Motion}
}


@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  edition = {Illustrated edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03561-3},
  langid = {english}
}


@book{ketkarDeepLearningPython2017,
  title = {Deep Learning with {{Python}}},
  author = {Ketkar, Nikhil and Santana, Eder},
  year = {2017},
  volume = {1},
  publisher = {{Springer}}
}


@misc{DeepLearningPython,
  title = {Deep {{Learning}} with {{Python}}, {{Second Edition}}},
  journal = {Manning Publications},
  abstract = {In this extensively revised new edition of the bestselling original, Keras creator offers insights for both novice and experienced machine learning practitioners.},
  howpublished = {https://www.manning.com/books/deep-learning-with-python-second-edition},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/NEFSKNHG/deep-learning-with-python-second-edition.html}
}


@book{cholletDeepLearning2018,
  title = {Deep {{Learning}} with {{R}}},
  author = {Chollet, Francois and Allaire, J. J.},
  year = {2018},
  month = feb,
  publisher = {{Manning Publications}},
  abstract = {SummaryDeep Learning with R introduces the world of deep learning using the powerful Keras library and its R language interface. The book builds your understanding of deep learning through intuitive explanations and practical examples. Continue your journey into the world of deep learning with Deep Learning with R in Motion, a practical, hands-on video course available exclusively at Manning.com (www.manning.com/livevideo/deep-\hspace{0pt}learning-with-r-in-motion).Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.About the TechnologyMachine learning has made remarkable progress in recent years. Deep-learning systems now enable previously impossible smart applications, revolutionizing image recognition and natural-language processing, and identifying complex patterns in data. The Keras deep-learning library provides data scientists and developers working in R a state-of-the-art toolset for tackling deep-learning tasks.About the BookDeep Learning with R introduces the world of deep learning using the powerful Keras library and its R language interface. Initially written for Python as Deep Learning with Python by Keras creator and Google AI researcher Fran\c{c}ois Chollet and adapted for R by RStudio founder J. J. Allaire, this book builds your understanding of deep learning through intuitive explanations and practical examples. You'll practice your new skills with R-based applications in computer vision, natural-language processing, and generative models.What's InsideDeep learning from first principlesSetting up your own deep-learning environmentImage classification and generationDeep learning for text and sequencesAbout the ReaderYou'll need intermediate R programming skills. No previous experience with machine learning or deep learning is assumed.About the AuthorsFran\c{c}ois Chollet is a deep-learning researcher at Google and the author of the Keras library.J.J. Allaire is the founder of RStudio and the author of the R interfaces to TensorFlow and Keras.Table of ContentsPART 1 - FUNDAMENTALS OF DEEP LEARNINGWhat is deep learning?Before we begin: the mathematical building blocks of neural networksGetting started with neural networksFundamentals of machine learningPART 2 - DEEP LEARNING IN PRACTICEDeep learning for computer visionDeep learning for text and sequencesAdvanced deep-learning best practicesGenerative deep learningConclusions},
  googlebooks = {xnIRtAEACAAJ},
  isbn = {978-1-61729-554-6},
  langid = {english},
  keywords = {Computers / Intelligence (AI) \& Semantics,Computers / Machine Theory,Computers / Natural Language Processing,Computers / Neural Networks}
}


@article{mindas2007,
	title = {Mind as machine: a history of cognitive science},
	year = {2007},
	month = {07},
	date = {2007-07-01},
	journal = {Choice Reviews Online},
	volume = {44},
	number = {11},
	doi = {10.5860/choice.44-6202},
	url = {http://dx.doi.org/10.5860/choice.44-6202},
	langid = {en}
}

@article{winston1976,
	title = {The psychology of computer vision},
	author = {Winston, Patrick Henry},
	year = {1976},
	month = {07},
	date = {1976-07},
	journal = {Pattern Recognition},
	pages = {193},
	volume = {8},
	number = {3},
	doi = {10.1016/0031-3203(76)90020-0},
	url = {http://dx.doi.org/10.1016/0031-3203(76)90020-0},
	langid = {en}
}

@book{hansonComputerVisionSystems1978,
  title = {Computer {{Vision Systems}}},
  author = {Hanson, Allen},
  year = {1978},
  month = jan,
  publisher = {{Elsevier}},
  abstract = {Computer Vision Systems is a collection of papers presented at the Workshop on Computer Vision Systems held at the University of Massachusetts in Amherst, Massachusetts, on June 1-3, 1977. Contributors discuss the breadth of problems that must be taken into account in the development of general computer vision systems. Topics covered include the application of system engineering techniques to the design of artificial intelligence systems; representation and segmentation of natural scenes; and pragmatic aspects of machine vision. Psychophysical measures of representation and interpretation are also considered. This monograph is divided into four sections: Issues and Research Strategies, Segmentation, Theory and Psychology, and Systems. The first chapter explores the problem of recovering the intrinsic characteristics of scenes from images, along with its implications for machine and human vision. The discussion then turns to special-purpose low-level vision systems that can be flexibly reconfigured as the need arises; design, development, and implementation of large systems from the human engineering point of view; and representation of visual information. The next section examines hierarchical relaxation for waveform parsing; the topology and semantics of intensity arrays; and visual images as spatial representations in active memory. The use of edge cues to recognize real-world objects is also analyzed. This text will be a useful resource for systems designers, computer engineers, and scientists as well as psychologists.},
  googlebooks = {gMGkDLLhnFUC},
  isbn = {978-0-323-15120-7},
  langid = {english},
  keywords = {Technology \& Engineering / Materials Science / General}
}


@book{robertsMachinePerceptionThreedimensional1980,
  title = {Machine {{Perception}} of {{Three-dimensional Solids}}},
  author = {Roberts, Lawrence G.},
  year = {1980},
  publisher = {{Garland Pub.}},
  googlebooks = {THWwAAAAIAAJ},
  isbn = {978-0-8240-4427-5},
  langid = {english}
}


@article{barrowComputationalVision1981,
  title = {Computational Vision},
  author = {Barrow, H.G. and Tenenbaum, J.M.},
  year = {1981},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {69},
  number = {5},
  pages = {572--595},
  issn = {1558-2256},
  doi = {10.1109/PROC.1981.12026},
  abstract = {Research is beginning to uncover fundamental computational principles underlying vision that apply equally to artificial and natural systems. These principles provide insights into the limitations of early machine vision systems and lay a foundation for building future systems capable of high performance in a broad range of visual domains. We present this emerging computational view of visual perception, discuss some early work in the field in its context, and put forward current thoughts on the overall organization and operation of a general-purpose computer vision system, synthesizing recent theoretical and experimental results.},
  keywords = {Computer graphics,Computer vision,Data structures,Image processing,Image segmentation,Layout,Machine vision,Pattern analysis,Pattern recognition,Velocity measurement},
  file = {/Users/majerek/Zotero/storage/HYP3R5IS/1456294.html}
}


@article{devPerceptionDepthSurfaces1975,
  title = {Perception of Depth Surfaces in Random-Dot Stereograms : A Neural Model},
  shorttitle = {Perception of Depth Surfaces in Random-Dot Stereograms},
  author = {Dev, Parvati},
  year = {1975},
  month = jul,
  journal = {International Journal of Man-Machine Studies},
  volume = {7},
  number = {4},
  pages = {511--528},
  issn = {0020-7373},
  doi = {10.1016/S0020-7373(75)80030-7},
  abstract = {A model has been presented of a neural process that segments the visual field into spatially disjoint regions, each region characterized by a specific feature such as a texture or color. The neural connectivity hypothesized to be necessary for the segmentation process has been formulated in mathematical terms and the corresponding neural network has been simulated on the digital computer. The properties of the network that result from the postulated patterns of excitatory and inhibitory connectivity have been investigated. It is shown that the required connectivity is that of excitatory connections only between neurons detecting similar features and inhibitory connections between all feature-detecting neurons. The resulting segmentation model is used to model the phenomenon of stereopsis as investigated through the use of random-dot stereograms. The process of depth perception through stereopsis can be viewed as a segmentation process with each segment, that is, each surface at a specific depth, characterized by a specific retinal disparity. It is shown that the segmentation model suffices to detect the different depth surfaces embedded in the random-dot patterns.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/GM96NCK2/S0020737375800307.html}
}


@misc{CooperativeComputationStereo,
  title = {Cooperative {{Computation}} of {{Stereo Disparity}} | {{Science}}},
  howpublished = {https://www.science.org/doi/10.1126/science.968482},
  file = {/Users/majerek/Zotero/storage/X764KYLU/science.html}
}


@article{barnardComputationalStereo1982,
  title = {Computational {{Stereo}}},
  author = {Barnard, Stephen T. and Fischler, Martin A.},
  year = {1982},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {14},
  number = {4},
  pages = {553--572},
  issn = {0360-0300},
  doi = {10.1145/356893.356896}
}


@article{hornObtainingShapeShading,
  title = {Obtaining {{Shape}} from {{Shading Information}}},
  author = {Horn, Berthold K P},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/AS2K9QDS/Horn - Obtaining Shape from Shading Information.pdf}
}


@article{blakeSurfaceDescriptionsStereo1985,
  title = {Surface Descriptions from Stereo and Shading},
  author = {Blake, Andrew and Zisserman, Andrew and Knowles, Greg},
  year = {1985},
  month = nov,
  journal = {Image and Vision Computing},
  series = {Papers from the 1985 {{Alvey Computer Vision}} and {{Image Interpretation Meeting}}},
  volume = {3},
  number = {4},
  pages = {183--191},
  issn = {0262-8856},
  doi = {10.1016/0262-8856(85)90006-X},
  abstract = {Surface reconstruction from stereo and shape-from-shading have both been discussed extensively in the literature. In this paper methods that attempt to supplement stereo with analysis of shading, are reviewed, with comments on their robustness. Theoretical results on uniqueness in shape from shading are presented, and on the difficulty of estimating surface shape with local intensity operators. Preliminary experiments based on the method of Koenderinck and van Doorn, for analysing variation of shading with viewpoint, are reported. It is concluded that there is a variety of techniques for deriving qualitative shape information, which might be sufficiently robust for use in computer vision systems.},
  langid = {english},
  keywords = {shading,stereo vision,surface shape},
  file = {/Users/majerek/Zotero/storage/XS3G7XJQ/026288568590006X.html}
}


@article{cannyComputationalApproachEdge1986,
  title = {A {{Computational Approach}} to {{Edge Detection}}},
  author = {Canny, John},
  year = {1986},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {679--698},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1986.4767851},
  abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
  keywords = {Detectors,Edge detection,feature extraction,Feature extraction,Gaussian approximation,Image edge detection,image processing,machine vision,Machine vision,multiscale image analysis,Performance analysis,Shape measurement,Signal synthesis,Signal to noise ratio,Uncertainty},
  file = {/Users/majerek/Zotero/storage/YU2A25YY/4767851.html}
}


@article{nalwaDetectingEdges1986,
  title = {On {{Detecting Edges}}},
  author = {Nalwa, Vishvjit S. and Binford, Thomas O.},
  year = {1986},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {699--714},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1986.4767852},
  abstract = {An edge in an image corresponds to a discontinuity in the intensity surface of the underlying scene. It can be approximated by a piecewise straight curve composed of edgels, i.e., short, linear edge-elements, each characterized by a direction and a position. The approach to edgel-detection here, is to fit a series of one-dimensional surfaces to each window (kernel of the operator) and accept the surface-description which is adequate in the least squares sense and has the fewest parameters. (A one-dimensional surface is one which is constant along some direction.) The tanh is an adequate basis for the stepedge and its combinations are adequate for the roofedge and the line-edge. The proposed method of step-edgel detection is robust with respect to noise; for (step-size/{$\sigma$}noise) {$\geq$} 2.5, it has subpixel position localization ({$\sigma$}position {$<$} {$\frac{1}{3}$}) and an angular localization better than 10\textdegree; further, it is designed to be insensitive to smooth shading. These results are demonstrated by some simple analysis, statistical data, and edgelimages. Also included is a comparison of performance on a real image, with a typical operator (Difference-of-Gaussians). The results indicate that the proposed operator is superior with respect to detection, localization, and resolution.},
  keywords = {Adequate basis for step-edges,Artificial intelligence,digital image processing,directional edge operator,edge-detection,Image edge detection,image segmentation,Information systems,Kernel,Laboratories,Layout,Least squares approximation,Least squares methods,Noise robustness,one-dimensional surface-fitting,subpixel edge localization,Surface fitting},
  file = {/Users/majerek/Zotero/storage/L9CFYNLY/4767852.html}
}


@article{woodhamAnalysingImagesCurved1981,
  title = {Analysing Images of Curved Surfaces},
  author = {Woodham, Robert J.},
  year = {1981},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {17},
  number = {1},
  pages = {117--140},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(81)90022-9},
  abstract = {A reflectance map makes the relationship between image intensity and surface orientation explicit. Trade-offs between image intensity and surface orientation emerge which cannot be resolved locally in a single view. Existing methods for determining surface orientation from a single view embody assumptions about surface curvature. The Hessian matrix is introduced to represent surface curvature. Properties of surface curvature are expressed as properties of the Hessian matrix. For several classes of surface, image analysis simplifies. This result has already been established for planar surfaces forming trihedral corners. Similar simplification is demonstrated for developable surfaces and for the subclass of surfaces known as generalized cones. These studies help to delineate shape information that can be determined from geometric measurements at object boundaries and shape information that can be determined from intensity measurements over sections of smoothly curved surface. A novel technique called photometric stereo is discussed. The idea of stereo is to obtain multiple images in order to determine the underlying scene precisely. In photometric stereo, the viewing direction is constant. Multiple images are obtained by varying the incident illumination. It is shown that this provides sufficient information to determine surface orientation at each image point.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/DX5LS72A/0004370281900229.html}
}


@article{witkinRecoveringSurfaceShape1981,
  title = {Recovering Surface Shape and Orientation from Texture},
  author = {Witkin, Andrew P.},
  year = {1981},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {17},
  number = {1},
  pages = {17--45},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(81)90019-9},
  abstract = {Texture provides an important source of information about the three-dimensional structure of visible surfaces, particularly for stationary monocular views. To recover 3d structure, the distorting effects of projection must be distinguished from properties of the texture on which the distortion acts. This requires that assumptions must be made about the texture, yet the unpredictability of natural textures precludes the use of highly restrictive assumptions. The recovery method reported in this paper exploits the minimal assumption that textures do not mimic projective effects. This assumption determines the strategy of attributing as much as possible of the variation observed in the image to projection. Equivalently, the interpretation is chosen for which the texture, prior to projection, is made as uniform as possible. This strategy was implemented using statistical methods, first for the restricted case of planar surfaces and then, by extension, for curved surfaces. The technique was applied successfully to natural images.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/E2MGG63Y/0004370281900199.html}
}


@inproceedings{fischlerReadingsComputerVision1987,
  title = {Readings in Computer Vision: Issues, Problems, Principles, and Paradigms},
  shorttitle = {Readings in Computer Vision},
  author = {Fischler, M. and Firschein, O.},
  year = {1987},
  abstract = {Each chapter of the book addresses a problem, provides a survey of major issues, ideas, and research projects, and presents reprints of key papers. In total, the book presents sixty research papers, most written since 1980.}
}


@book{mundyGeometricInvarianceComputer1992,
  title = {Geometric Invariance in Computer Vision},
  editor = {Mundy, Joseph L. and Zisserman, Andrew},
  year = {1992},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {978-0-262-13285-5}
}


@article{kassSnakesActiveContour1988,
  title = {Snakes: {{Active}} Contour Models},
  shorttitle = {Snakes},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/BF00133570},
  abstract = {A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/F45K5BPG/Kass et al. - 1988 - Snakes Active contour models.pdf}
}


@book{blakeActiveContoursApplication2012,
  title = {Active {{Contours}}: {{The Application}} of {{Techniques}} from {{Graphics}}, {{Vision}}, {{Control Theory}} and {{Statistics}} to {{Visual Tracking}} of {{Shapes}} in {{Motion}}},
  shorttitle = {Active {{Contours}}},
  author = {Blake, Andrew and Isard, Michael},
  year = {2012},
  month = dec,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Active Contours deals with the analysis of moving images - a topic of growing importance within the computer graphics industry. In particular it is concerned with understanding, specifying and learning prior models of varying strength and applying them to dynamic contours. Its aim is to develop and analyse these modelling tools in depth and within a consistent framework.},
  googlebooks = {yZPxBwAAQBAJ},
  isbn = {978-1-4471-1555-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Optical Data Processing,Computers / Software Development \& Engineering / Computer Graphics,Computers / Software Development \& Engineering / General}
}


@article{malladiShapeModelingFront1995,
  title = {Shape Modeling with Front Propagation: A Level Set Approach},
  shorttitle = {Shape Modeling with Front Propagation},
  author = {Malladi, R. and Sethian, J.A. and Vemuri, B.C.},
  year = {1995},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {17},
  number = {2},
  pages = {158--175},
  issn = {1939-3539},
  doi = {10.1109/34.368173},
  abstract = {Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which retains some of the attractive features of existing methods and overcomes some of their limitations. The authors' techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori assumption about the object's topology is made. A single instance of the authors' model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian (1988) to model propagating solid/liquid interfaces with curvature-dependent speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient field with constant speed or a speed that depends on the curvature. It is moved by solving a "Hamilton-Jacobi" type equation written for a function in which the interface is a particular level set. A speed term synthesized from the image is used to stop the interface in the vicinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. The authors present a variety of ways of computing the evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images.{$<>$}},
  keywords = {Biomedical imaging,Computer graphics,Computer vision,Difference equations,Finite difference methods,Level set,Narrowband,Shape,Solid modeling,Topology},
  file = {/Users/majerek/Zotero/storage/S69PW2RV/368173.html}
}


@book{ponceCategoryLevelObjectRecognition2007,
  title = {Toward {{Category-Level Object Recognition}}},
  author = {Ponce, Jean and Hebert, Martial and Schmid, Cordelia and Zisserman, Andrew},
  year = {2007},
  month = jan,
  publisher = {{Springer}},
  abstract = {Although research in computer vision for recognizing 3D objects in photographs dates back to the 1960s, progress was relatively slow until the turn of the millennium, and only now do we see the emergence of effective techniques for recognizing object categories with different appearances under large variations in the observation conditions. Tremendous progress has been achieved in the past five years, thanks largely to the integration of new data representations, such as invariant semi-local features, developed in the computer vision community with the effective models of data distribution and classification procedures developed in the statistical machine-learning community. This volume is a post-event proceedings volume and contains selected papers based on presentations given, and vivid discussions held, during two workshops held in Taormina in 2003 and 2004. The main goals of these two workshops were to promote the creation of an international object recognition community, with common datasets and evaluation procedures, to map the state of the art and identify the main open problems and opportunities for synergistic research, and to articulate the industrial and societal needs and opportunities for object recognition research worldwide. The 30 thoroughly revised papers presented are organized in the following topical sections: recognition of specific objects, recognition of object categories, recognition of object categories with geometric relations, and joint recognition and segmentation.},
  googlebooks = {vXQKBwAAQBAJ},
  isbn = {978-3-540-68795-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Optical Data Processing,Computers / Programming / Algorithms,Computers / Software Development \& Engineering / Computer Graphics,Computers / Software Development \& Engineering / General}
}


@article{fergusWeaklySupervisedScaleInvariant2007,
  title = {Weakly {{Supervised Scale-Invariant Learning}} of {{Models}} for {{Visual Recognition}}},
  author = {Fergus, R. and Perona, P. and Zisserman, A.},
  year = {2007},
  month = mar,
  journal = {International Journal of Computer Vision},
  volume = {71},
  number = {3},
  pages = {273--303},
  issn = {1573-1405},
  doi = {10.1007/s11263-006-8707-x},
  abstract = {We investigate a method for learning object categories in a weakly supervised manner. Given a set of images known to contain the target category from a similar viewpoint, learning is translation and scale-invariant; does not require alignment or correspondence between the training images, and is robust to clutter and occlusion. Category models are probabilistic constellations of parts, and their parameters are estimated by maximizing the likelihood of the training data. The appearance of the parts, as well as their mutual position, relative scale and probability of detection are explicitly described in the model. Recognition takes place in two stages. First, a feature-finder identifies promising locations for the model''s parts. Second, the category model is used to compare the likelihood that the observed features are generated by the category model, or are generated by background clutter. The flexible nature of the model is demonstrated by results over six diverse object categories including geometrically constrained categories (e.g. faces, cars) and flexible objects (such as animals).},
  langid = {english},
  keywords = {constellation model,object recognition,parts and structure model,semi-supervised learning}
}


@article{felzenszwalbPictorialStructuresObject2005,
  title = {Pictorial {{Structures}} for {{Object Recognition}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  year = {2005},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {61},
  number = {1},
  pages = {55--79},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000042934.15159.49},
  abstract = {In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.},
  langid = {english},
  keywords = {energy minimization,part-based object recognition,statistical models}
}


@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science,Mathematics and computing}
}


@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1505.04597},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/majerek/Zotero/storage/MIZG3DGR/1505.html}
}


@misc{zhouUNetNestedUNet2018,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10165},
  eprint = {1807.10165},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.10165},
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/majerek/Zotero/storage/AQVTJVXU/1807.html}
}


@misc{PictureProcessingPsychopictorics,
  title = {Picture {{Processing}} and {{Psychopictorics}} - 1st {{Edition}}},
  howpublished = {https://www.elsevier.com/books/picture-processing-and-psychopictorics/lipkin/978-0-12-451550-5},
  file = {/Users/majerek/Zotero/storage/GZENQHXM/978-0-12-451550-5.html}
}


@article{davisSurveyEdgeDetection1975,
  title = {A Survey of Edge Detection Techniques},
  author = {Davis, Larry S.},
  year = {1975},
  month = sep,
  journal = {Computer Graphics and Image Processing},
  volume = {4},
  number = {3},
  pages = {248--270},
  issn = {0146-664X},
  doi = {10.1016/0146-664X(75)90012-X},
  abstract = {Methods of detecting ``edges,'' i.e., boundaries between regions in a picture, are reviewed. Included are both parallel (linear, nonlinear, optimal) and sequential methods, as well as methods using planning or a priori knowledge.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/5B79D7KY/0146664X7590012X.html}
}


@misc{OpticalElectroOpticalInformation,
  title = {Optical and {{Electro-Optical Information Processing}}},
  journal = {MIT Press},
  abstract = {This book comprises the Proceedings of a Symposium on Optical and Electro-Optical Information Processing Technology held in Boston. The aims of the Symposium...},
  langid = {american},
  file = {/Users/majerek/Zotero/storage/KMSDZKAY/optical-and-electro-optical-information-processing.html}
}


@article{kirschComputerDeterminationConstituent1971,
  title = {Computer Determination of the Constituent Structure of Biological Images},
  author = {Kirsch, Russell A.},
  year = {1971},
  month = jun,
  journal = {Computers and Biomedical Research},
  volume = {4},
  number = {3},
  pages = {315--328},
  issn = {0010-4809},
  doi = {10.1016/0010-4809(71)90034-6},
  abstract = {A class of algorithms is described which enables computer quantized images to be decomposed into constituent reflecting the structure of the images. This decomposition is viewed as the morphological precursor to a higher level syntactic analysis. Numerical results for a typical biological image are presented.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/IQIS2QRE/0010480971900346.html}
}


@article{marrTheoryEdgeDetection1980,
  title = {Theory of Edge Detection},
  author = {Marr, D. and Hildreth, E.},
  year = {1980},
  month = feb,
  journal = {Proceedings of the Royal Society of London. Series B, Biological Sciences},
  volume = {207},
  number = {1167},
  pages = {187--217},
  issn = {0950-1193},
  doi = {10.1098/rspb.1980.0020},
  abstract = {A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of delta 2G(x,y)*I(x,y) for image I, where G(x,y) is a two-dimensional Gaussian distribution and delta 2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround delta 2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr \& Ullman 1979).},
  langid = {english},
  pmid = {6102765},
  keywords = {Animals,Form Perception,Humans,Mathematics,Vision; Ocular}
}


@article{zhang1984,
	title = {A fast parallel algorithm for thinning digital patterns},
	author = {Zhang, T. Y. and Suen, C. Y.},
	year = {1984},
	month = {03},
	date = {1984-03},
	journal = {Communications of the ACM},
	pages = {236--239},
	volume = {27},
	number = {3},
	doi = {10.1145/357994.358023},
	url = {http://dx.doi.org/10.1145/357994.358023},
	langid = {en}
}

@article{ciresanFlexibleHighPerformance,
  title = {Flexible, {{High Performance Convolutional Neural Networks}} for {{Image Classification}}},
  author = {Ciresan, Dan C and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, Jurgen},
  abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/SHX6DMQX/Ciresan et al. - Flexible, High Performance Convolutional Neural Ne.pdf}
}


@article{krizhevsky2017,
	title = {ImageNet classification with deep convolutional neural networks},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2017},
	month = {05},
	date = {2017-05-24},
	journal = {Communications of the ACM},
	pages = {84--90},
	volume = {60},
	number = {6},
	doi = {10.1145/3065386},
	url = {http://dx.doi.org/10.1145/3065386},
	langid = {en}
}

@article{Simonyan2014,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	date = {2014},
	doi = {10.48550/ARXIV.1409.1556},
	url = {https://arxiv.org/abs/1409.1556}
}
