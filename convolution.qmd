---
code-fold: show
---

# Sieci splotowe

W tym rozdziale przedstawimy konwolucyjne (lub splotowe) sieci neuronowe (ang. *convolutional neural networks*), znane również jako *CovNets*, rodzaj modelu głębokiego uczenia się, który jest niemal zawsze stosowany w aplikacjach widzenia komputerowego.

Wkrótce zagłębimy się w teorię tego, czym są sieci splotowe i dlaczego odniosły taki sukces w zadaniach związanych z widzeniem komputerowym.
Ale najpierw przyjrzyjmy się w praktyce prostemu przykładowi sieci splotowej.
Wykorzystuje on sieć kowolucyjną do klasyfikacji cyfr MNIST, czyli zadania, które wykonaliśmy wcześnej przy użyciu sieci gęsto połączonej (nasza dokładność testu wyniosła wtedy 97,8%).

Poniższe linie kodu pokazują, jak wygląda podstawowa sieć *CovNet*.
Jest to stos warstw `layer_conv_2d` i `layer_max_pooling_2d` o których zasadzie działania będzie jeszcze więcej za chwilę.

```{r}
library(keras)
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
  input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")
```

Co ważne, *CovNet* przyjmuje jako dane wejściowe tensory o kształcie `(image_height, image_width, image_channels)` (nie licząc wymiaru partii).
W tym przypadku skonfigurujemy sieć do przetwarzania danych wejściowych o rozmiarze `(28, 28, 1)`, czyli w formacie obrazów MNIST.
Zrobimy to przekazując argument `input_shape = c(28, 28, 1)` do pierwszej warstwy.

```{r}
model
```

Możesz zauważyć, że wyjście z warstw `layer_conv_2d` i `layer_max_pooling_2d` jest tensorem 3D kształtu `(wysokość, szerokość, filtry)`[^convolution-1].
Wymiary szerokości i wysokości mają tendencję do kurczenia się, gdy wchodzisz głębiej w sieć.
Liczba filtrów jest kontrolowana przez pierwszy argument przekazany do `layer_conv_2d` (32 lub 64).

[^convolution-1]: w niektórych publikacjach ostatni parametr jest nazywany kanałami, ale aby nie wprowadzać zamieszania, ponieważ nazwa kanał jest zarezerwowana do obrazów, to zostanę przy nazwie filtry

Następnym krokiem jest wprowadzenie ostatniego tensora wyjściowego (o kształcie `(3, 3, 64)`) do gęsto połączonej sieci klasyfikatorów, takich jak te, które już znasz - stosu gęstych warstw.
Te klasyfikatory przetwarzają wektory, które są 1D, podczas gdy bieżące wyjście jest tensorem 3D.
Najpierw musimy spłaszczyć wyjścia 3D do 1D, a następnie dodać kilka gęstych warstw na wierzchu.

```{r}
model <- model %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")
```

Zrobimy klasyfikację 10-kierunkową, używając warstwy końcowej z 10 wyjściami i aktywacją softmax.
Oto jak wygląda teraz sieć:

```{r}
model
```

Jak widać, wyjścia `(3, 3, 64)` są spłaszczane do wektorów o kształcie `(576)`[^convolution-2] przed przejściem przez dwie warstwy gęste.

[^convolution-2]: ponieważ 3\*3\*64=576

```{r}
#| cache: true
mnist <- dataset_mnist()

c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
train_images <- array_reshape(train_images, c(60000, 28, 28, 1))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

```{r}
#| eval: false
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

history <- model %>% fit(
  train_images, 
  train_labels,
  epochs = 5, 
  batch_size=64,
  validation_split = 0.2
)
```

```{r}
#| echo: false
model <- load_model_tf(filepath = "models/conv_mnist1/")
load("models/conv_hist1.rda")
plot(history)
```

Jak widać z powyższego wykresu model bardzo dobrze dopasował się do danych.
Sprawdźmy zatem jak sobie radzi na zbiorze testowym.

```{r}
# cache: true
results <- model %>% evaluate(test_images, test_labels)
results
```

W rezultacie otrzymaliśmy sieć o dokładności 0.9908.
Choć może to w stosunku do 0.9779, czyli dokładności dla sieci gęstej z pierwszego przykładu MNIST, to jednak względny błąd predykcji dla sieci splotowej zmalał o ponad 58% `r emo::ji("star_struck")`.
