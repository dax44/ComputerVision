[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automatyczna analiza obrazu",
    "section": "",
    "text": "Wstęp\nNiniejsza książka powstała na potrzeby prowadzenia wykładu z Automatycznej analizy obrazu. Jest wynikiem moich doświadczeń z automatyczną analizą obrazu. W pisaniu tego kompendium wiedzy na temat Computer Vision bardzo pomocne były dwie pozycje literaturowe (Burger i Burge 2016; Szeliski 2022). Oprócz wspomnianych książek poświęconych wizji komputerowej, ważne są również pozycje objaśniające arkana deep learning. Wśród nich należy wymienić (Goodfellow, Bengio, i Courville 2016; Ketkar i Santana 2017; „Deep Learning with Python, Second Edition”, b.d.; Chollet i Allaire 2018). Ponadto zostaną wykorzystane nieprzebrane zasoby internetu - na stronach takich jak https://stackoverflow.com czy https://github.com można znaleźć rozwiązania do niemal każdego zadania.\nNa potrzeby zajęć laboratoryjnych będą dodatkowo potrzebne pewne programy komputerowe i biblioteki:\n\nFiji - darmowy program będący nakładką na program ImageJ. Służy on do operacji na zdjęciach. Do pobrania ze strony https://fiji.sc/;\nPython - język programowania, w którym można wykonać niemal dowolne zadanie z zakresu automatycznej analizy obrazu. Przez instalacje Python-a rozumiem zainstalowanie odpowiedniej dystrybucji tego programu (np. dla Windows zaleca się instalację dystrybucji Anconda lub Miniconda). Po szczegóły dotyczące instalacji Pythona na Windows odsyłam na stronę https://support.posit.co/hc/en-us/articles/1500007929061-Using-Python-with-the-RStudio-IDE;\nPo zainstalowaniu Pythona, trzeba też zainstalować dwie bardzo ważne biblioteki pythonowe do budowania i uczenia sieci głębokiego uczenia:\n\ntensorflow - jest end-to-end platformą typu open-source do uczenia maszynowego. Jest to kompleksowy i elastyczny ekosystem narzędzi, bibliotek i innych zasobów, które zapewniają przepływy pracy z wysokopoziomowymi interfejsami API. Ramy oferują różne poziomy koncepcji, abyś mógł wybrać ten, którego potrzebujesz do budowania i wdrażania modeli uczenia maszynowego;\nkeras - jest wysokopoziomową biblioteką do budowy sieci neuronowych, która działa na bazie TensorFlow, CNTK i Theano. Wykorzystanie Keras w deep learningu pozwala na łatwe i szybkie prototypowanie, a także płynne działanie na CPU i GPU. Aby zainstalować zarówno tensorflow, jak i keras z obsługo CPU lub GPU polecam instrukcję w filmie https://youtu.be/PnK1jO2kXOQ;\n\nOpenCV - jest biblioteką (ale nie programu R) funkcji programistycznych skierowanych głównie do wizji komputerowej czasu rzeczywistego. Instalację w Windows można znaleźć pod adresem https://docs.opencv.org/4.x/d3/d52/tutorial_windows_install.html;\nBiblioteki R-owe potrzebne do budowy modeli i obsługi obrazów, to:\n\nreticulate - biblioteka pozwalająca na wykorzystanie bibliotek i funkcji Python-a w R;\nmagick - biblioteka potrzebna do różnego rodzaju transformacji obrazów;\ntensorflow - biblioteka R-owa pozwalająca na wykorzystanie funkcji tensorflow Pythona;\nkeras - biblioteka R-owa pozwalająca na korzystanie z funkcji pakietu keras Pythonowego.\n\n\n\n\n\n\nBurger, Wilhelm, i Mark J. Burge. 2016. Digital Image Processing: An Algorithmic Introduction Using Java. Texts w Computer Science. London: Springer. https://doi.org/10.1007/978-1-4471-6684-9.\n\n\nChollet, Francois, i J. J. Allaire. 2018. Deep Learning with R. Manning Publications.\n\n\n„Deep Learning with Python, Second Edition”. b.d. Manning Publications. https://www.manning.com/books/deep-learning-with-python-second-edition.\n\n\nGoodfellow, Ian, Yoshua Bengio, i Aaron Courville. 2016. Deep Learning. Illustrated edition. Cambridge, Massachusetts: The MIT Press.\n\n\nKetkar, Nikhil, i Eder Santana. 2017. Deep Learning with Python. T. 1. Springer.\n\n\nSzeliski, Richard. 2022. Computer Vision: Algorithms and Applications. Texts w Computer Science. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-34372-9."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Automatyczna analiza obrazu jest znana również pod inną nazwą wizja komputerowa (ang. Computer Vision). Można powiedzieć też, że AAO1 jest częścią jeszcze szerszej dziedziny automatycznej analizy sygnałów. Ponieważ różnice pomiędzy analizą obrazu i dźwięku w niektórych zadaniach będą się zacierać, to poznane metody w toku tego wykładu będzie można śmiało przenieść na inne dziedziny. Oczywiście uwzględniając szereg podobieństw pomiędzy analizą obrazu i analizą dźwięku, istniej wiele dedykowanych modeli stosowanych tylko w domenie fal dźwiękowych.1 skrót od Automatyczna Analiza Obrazu\n\n\n\n\n\nW ramach zadań realizowanych przez wizję komputerową można wymienić:\n\npozyskiwanie obrazów (opis procesu “robienia zdjęcia” cyfrowego);\nprzetwarzanie obrazów w celu zmiany ich parametrów (np. poprawy ostrości, usuwania szumów, itp);\nanalizowania zdjęć w celu poszukiwania wzorców:\n\nzastosowanie ML2 do klasyfikacji obiektów na zdjęciach;\nzastosowanie ML do zadań regresyjnych (np. wyznaczanie poziomu wylania na podstawie zdjęć satelitarnych);\nzastosowanie ML w lokalizacji obiektów na obrazie (np. wskazanie położenia samolotu na zdjęciu w postaci ujęcia go w prostokątną ramkę);\nautoidentyfikacja (np. rozpoznawanie twarzy czy odcisku palca);\ntworzenie obrazów na podstawie fraz (istnieją sieci np. GAN, które są w stanie wygenerować całkowicie fikcyjny obraz na podstawie zdania opisującego co ma się na nim znaleźć);\nśledzenie ruchów na podstawie obrazu wideo (np. automatyczne kadrowanie obrazu wideo na podstawie położenia twarzy podczas rozmowy przez komunikator);\nsegmentacja obrazu;\ni wiele innych\n\n\n2 Machine LearningŚmiało można stwierdzić, że AAO towarzyszy nam codziennie i na każdym kroku. Czasami nie jesteśmy nawet tego świadomi.\n\nPoniżej przedstawiam listę wybranych zastosowań AAO:\n\nTransport - Rosnące wymagania sektora transportowego napędzają rozwój technologiczny w tej branży, w którego centrum znajduje się wizja komputerowa. Od pojazdów autonomicznych po wykrywanie zajętości miejsc parkingowych, Inteligentny System Transportowy (ITS) stał się krytycznym obszarem promowania wydajności, efektywności i bezpieczeństwa transportu.\n\n\n\n\n\nMonitorowanie zajętości miejsc na parkingu\n\n\n\nMedycyna. Dane z obrazowania medycznego są jednym z najbogatszych źródeł informacji. Bez odpowiedniej technologii lekarze są zmuszeni spędzać godziny na ręcznym analizowaniu danych pacjentów i wykonywaniu prac administracyjnych. Na szczęście, wraz z upływem lat i rozwojem technologii, branża opieki zdrowotnej stała się jedną z najszybciej przyjmujących nowe rozwiązania automatyzacji, w tym wizję komputerową.\n\n\n\n\n\nSegmentacja obrazu MRI\n\n\n\nProdukcja. Przemysł produkcyjny przyjął już szeroką gamę rozwiązań automatyzacji z wizją komputerową w centrum. Pomaga ona zautomatyzować kontrolę jakości, zminimalizować zagrożenia bezpieczeństwa i zwiększyć wydajność produkcji. Oto niektóre z najczęstszych zastosowań wizji komputerowej w przemyśle produkcyjnym.\n\n\n\n\nAAO w kontroli jakości\n\n\n\n\n\nAAO w procesie magazynowania\n\n\n\nBudowa. Sektor budowlany szybko przyjmuje technologię wizji komputerowej i wykorzystuje ją do wykrywania sprzętu ochrony osobistej, kontroli aktywów infrastruktury, wykrywania zagrożeń w miejscu pracy lub konserwacji.\n\n\n\n\nWykrywanie zużytych części\n\n\n\n\n\nNaruszenia procedur bezpieczeństwa\n\n\n\nRolnictwo. Sektor rolniczy był świadkiem kilku przypadków zastosowania modeli sztucznej inteligencji (w tym wizji komputerowej) w takich dziedzinach, jak monitorowanie upraw i plonów, zautomatyzowane zbiory, analiza warunków pogodowych, monitorowanie zdrowia zwierząt gospodarskich czy wykrywanie chorób roślin. Technologia ta zdobyła już silną pozycję dzięki możliwościom automatyzacji i wykrywania, a jej zastosowania będą się tylko rozszerzać.\n\n\n\n\nWyrywanie nietypowych zachowań zwierząt\n\n\n\n\n\nChoroby roślin\n\n\n\nSprzedaż detaliczna. Kamery zainstalowane w sklepach detalicznych pozwalają sprzedawcom zbierać duże ilości danych wizualnych pomocnych w projektowaniu lepszych doświadczeń klientów i pracowników. Rozwój systemów wizji komputerowej do przetwarzania tych danych sprawia, że cyfrowa transformacja branży realnej staje się znacznie bardziej osiągalna.\n\n\n\n\nWykrywanie braków towaru\n\n\n\n\n\nWykrywanie nietypowych zachowań klientów lub badanie zatłoczenia\n\n\nPrzestrzeni do zastosowań AAO jest jeszcze dużo więcej ale nie sposób ich wszystkich opisać. Oto kilka przykładów z różnych kategorii.\n\n\n\nBadanie ruchu zawodnika\n\n\n\n\n\nWykrywanie naruszeń prawa\n\n\n\n\n\nWykrywanie twarzy\n\n\nMoże też służyć do zabawy\n\n\n\nKilka obrazów wygenerowanych jako wariacje na temat mojego zdjęcia z wakacji\n\n\n\n\n\nKilka przykładów obrazów wygenerowanych przez sieć DALL E2 jako odpowiedź na zdanie “narysuj bez odrywania ręki jedna linią misia na zakupach”\n\n\n\n\n\nTym razem sieć DALL E2 została poproszona o namalowanie kobiet w stylu Van Gogha"
  },
  {
    "objectID": "history.html#lata-70",
    "href": "history.html#lata-70",
    "title": "2  Historia wizji komputerowej",
    "section": "2.1 Lata ’70",
    "text": "2.1 Lata ’70\nKiedy wizja komputerowa po raz pierwszy pojawiła się na początku lat siedemdziesiątych, była postrzegana jako wizualny komponent percepcji ambitnego programu naśladowania ludzkiej inteligencji i obdarzenia robotów inteligentnym zachowaniem. W tym czasie niektórzy z pionierów sztucznej inteligencji i robotyki (w miejscach takich jak MIT, Stanford) wierzyli, że rozwiązanie problemu “wejścia wizualnego” będzie łatwym krokiem na drodze do rozwiązania trudniejszych problemów, takich jak rozumowanie na wyższym poziomie i planowanie. Według jednej ze znanych historii, w 1966 roku Marvin Minsky z MIT poprosił swojego studenta Geralda Jay Sussmana o “spędzenie lata na podłączeniu kamery do komputera i nakłonieniu komputera do opisania tego, co widział”. Obecnie wiemy, że problem jest nieco trudniejszy niż wówczas się wydawało.\nTym, co odróżniało widzenie komputerowe od istniejącej już dziedziny cyfrowej obróbki obrazów, była chęć odzyskania trójwymiarowej struktury świata z obrazów i wykorzystania tego jako kroku w kierunku pełnego zrozumienia prezentowanej sceny. Winston (1976) oraz Hanson (1978) dostarczają dwóch ładnych zbiorów klasycznych prac z tego wczesnego okresu. Wczesne próby zrozumienia sceny polegały na wyodrębnieniu krawędzi, a następnie wnioskowaniu o strukturze 3D obiektu lub “świata bloków” z topologicznej struktury linii 2D Roberts (1980).\nJakościowe podejście do rozumienia intensywności i zmienności cieniowania oraz wyjaśniania ich przez efekty zjawisk formowania się obrazu, takich jak orientacja powierzchni i cienie, zostało spopularyzowane przez Barrow i Tenenbaum (1981) w ich pracy na temat obrazów wewnętrznych. W tym czasie opracowano również bardziej ilościowe podejścia do wizji komputerowej, w tym pierwszy z wielu opartych na cechach algorytmów korespondencji stereo (Dev 1975; „Cooperative Computation of Stereo Disparity | Science”, b.d.; Barnard i Fischler 1982)."
  },
  {
    "objectID": "history.html#lata-80",
    "href": "history.html#lata-80",
    "title": "2  Historia wizji komputerowej",
    "section": "2.2 Lata ’80",
    "text": "2.2 Lata ’80\nW latach ’80 ubiegłego wieku wiele uwagi poświęcono bardziej wyrafinowanym technikom matematycznym służącym do przeprowadzania ilościowej analizy obrazów i scen. Piramidy obrazów zaczęły być powszechnie stosowane do wykonywania zadań takich jak mieszanie obrazów i wyszukiwanie korespondencji coarse-to-fine. Wykorzystanie stereo jako ilościowej wskazówki kształtu zostało rozszerzone o szeroką gamę technik shape-from-X, w tym shape from shading (Horn, b.d.; Blake, Zisserman, i Knowles 1985).\n\n\n\nRysunek 2.2: Przykład wykorzystania techniki piramid blending\n\n\nW tym okresie prowadzono również badania nad lepszym wykrywaniem krawędzi i konturów (Canny 1986; Nalwa i Binford 1986), stereografii fotometrycznej (Woodham 1981) oraz kształty z tekstur (Witkin 1981). W tym okresie prowadzono również badania nad lepszym wykrywaniem krawędzi i konturów, w tym wprowadzono dynamicznie ewoluujące trackery konturów, takie jak węże, a także trójwymiarowe modele oparte na fizyce. Naukowcy zauważyli, że wiele algorytmów detekcji stereoskopowej, przepływu, shape-from-X i krawędzi może być zunifikowanych lub przynajmniej opisanych przy użyciu tych samych ram matematycznych, jeśli zostaną one postawione jako problemy optymalizacji wariacyjnej i uodpornione (dobrze postawione) przy użyciu regularyzacji.\nNieco później wprowadzono warianty on-line algorytmów MRF (ang. Markov Random Field), które modelowały i aktualizowały niepewności za pomocą filtru Kalmana. Podjęto również próby odwzorowania zarówno algorytmów regularyzowanych jak i MRF na sprzęt zrównoleglony (ang. parallel). Książka (Fischler i Firschein 1987) zawiera zbiór artykułów skupiających się na wszystkich tych tematach (stereo, przepływ, regularność, MRF, a nawet widzenie wyższego poziomu)."
  },
  {
    "objectID": "history.html#lata-90",
    "href": "history.html#lata-90",
    "title": "2  Historia wizji komputerowej",
    "section": "2.3 Lata ’90",
    "text": "2.3 Lata ’90\nPodczas gdy wiele z wcześniej wymienionych tematów było nadal eksplorowanych, kilka z nich stało się znacznie bardziej aktywnych. Nagły wzrost aktywności w zakresie wykorzystania niezmienników projekcyjnych do celów rozpoznania (Mundy i Zisserman 1992) przerodził się w skoordynowane wysiłki zmierzające do rozwiązania problemu structure from motion. Wiele początkowych działań skierowanych było na rekonstrukcje rzutowe, które nie wymagają znajomości kalibracji kamery. Równolegle, techniki faktoryzacji zostały opracowane w celu efektywnego rozwiązywania problemów, dla których miały zastosowanie przybliżenia kamery ortograficznej, a następnie rozszerzone na przypadek perspektywiczny.\nW końcu zaczęto stosować pełną optymalizację globalną która później została uznana za tożsama z technikami dopasowania wiązki, tradycyjnie stosowanymi w fotogrametrii. W pełni zautomatyzowane systemy modelowania 3D zostały zbudowane przy użyciu tych technik.\nPrace rozpoczęte w latach 80-tych nad wykorzystaniem szczegółowych pomiarów barwy i natężenia światła w połączeniu z dokładnymi modelami fizycznymi transportu promieniowania i tworzenia kolorowych obrazów stworzyły własną dziedzinę znaną jako widzenie oparte na fizyce. Algorytmy stereo na podstawie wielu obrazów, które tworzą kompletne powierzchnie 3D były również aktywnym tematem badań, który jest aktualny do dziś.\nAlgorytmy śledzenia również uległy dużej poprawie, w tym śledzenie konturów z wykorzystaniem aktywnych konturów, takich jak węże (Kass, Witkin, i Terzopoulos 1988), filtry cząsteczkowe (Blake i Isard 2012) i zbiorów poziomnicowych (ang. level set) (Malladi, Sethian, i Vemuri 1995), a także techniki oparte na intensywności (bezpośrednie), często stosowane do śledzenia twarzy.\n\n\n\nRysunek 2.3: Przykład śledzenia twarzy przez algorytm\n\n\nSegmentacja obrazów, temat, który jest aktywny od początku wizji komputerowej, był również aktywnym tematem badań, produkując techniki oparte na minimalnej energii i minimalnej długości opisu, znormalizowanych cięciach i średnim przesunięciu.\nZaczęły pojawiać się techniki uczenia statystycznego, najpierw w zastosowaniu analizy składowych głównych, eigenface do rozpoznawania twarzy oraz liniowych systemów dynamicznych do śledzenia krzywych.\nByć może najbardziej zauważalnym rozwojem w dziedzinie widzenia komputerowego w tej dekadzie była zwiększona interakcja z grafiką komputerową, zwłaszcza w interdyscyplinarnym obszarze modelowania i renderowania opartego na obrazach. Pomysł manipulowania obrazami świata rzeczywistego bezpośrednio w celu tworzenia nowych animacji po raz pierwszy stał się znany dzięki technikom morfingu obrazu."
  },
  {
    "objectID": "history.html#lata-00",
    "href": "history.html#lata-00",
    "title": "2  Historia wizji komputerowej",
    "section": "2.4 Lata ’00",
    "text": "2.4 Lata ’00\nTa dekada kontynuowała pogłębianie interakcji pomiędzy dziedzinami wizji i grafiki, ale co ważniejsze, przyjęła podejścia oparte na danych i uczeniu się jako kluczowe komponenty wizji. Wiele z tematów wprowadzonych w rubryce renderingu opartego na obrazie, takich jak zszywanie obrazów, przechwytywanie i renderowanie pola świetlnego oraz przechwytywanie obrazów o wysokim zakresie dynamicznym (HDR) poprzez bracketing ekspozycji, zostało ponownie ochrzczonych mianem fotografii obliczeniowej, aby potwierdzić zwiększone wykorzystanie takich technik w codziennej fotografii cyfrowej. Na przykład, szybkie przyjęcie bracketingu ekspozycji do tworzenia obrazów o wysokim zakresie dynamicznym wymagało opracowania algorytmów kompresji dynamiki, aby przekształcić takie obrazy z powrotem do wyników możliwych do wyświetlenia. Oprócz łączenia wielu ekspozycji, opracowano techniki łączenia obrazów z lampą błyskową z ich odpowiednikami bez lampy błyskowej.\n\n\n\nRysunek 2.4: Przykład rozpoznawania obiektów\n\n\nDrugim wartym uwagi trendem w tej dekadzie było pojawienie się technik opartych na cechach (połączonych z uczeniem) do rozpoznawania obiektów. Niektóre z godnych uwagi prac w tej dziedzinie obejmują model konstelacji (Ponce i in. 2007; Fergus, Perona, i Zisserman 2007) oraz struktury obrazowe (Felzenszwalb i Huttenlocher 2005). Techniki oparte na cechach dominują również w innych zadaniach rozpoznawania, takich jak rozpoznawanie scen, panoram i lokalizacji. I chociaż cechy oparte na punktach zainteresowania (patch-based) dominują w obecnych badaniach, niektóre grupy zajmują się rozpoznawaniem na podstawie konturów i segmentacji regionów.\nInnym istotnym trendem tej dekady było opracowanie bardziej wydajnych algorytmów dla złożonych problemów optymalizacji globalnej. Chociaż trend ten rozpoczął się od prac nad cięciami grafów, duży postęp dokonał się również w algorytmach przekazywania wiadomości, takich jak loopy belief propagation (LBP).\nNajbardziej zauważalnym trendem tej dekady, który do tej pory całkowicie opanował rozpoznawanie obrazu i większość innych aspektów widzenia komputerowego, było zastosowanie zaawansowanych technik uczenia maszynowego do problemów widzenia komputerowego. Trend ten zbiegł się w czasie ze zwiększoną dostępnością ogromnych ilości częściowo oznakowanych danych w Internecie, a także ze znacznym wzrostem mocy obliczeniowej, co sprawiło, że uczenie się kategorii obiektów bez użycia starannego nadzoru człowieka stało się bardziej realne."
  },
  {
    "objectID": "history.html#lata-10",
    "href": "history.html#lata-10",
    "title": "2  Historia wizji komputerowej",
    "section": "2.5 Lata ’10",
    "text": "2.5 Lata ’10\nTrend do wykorzystywania dużych etykietowanych zbiorów danych do rozwoju algorytmów uczenia maszynowego stał się falą, która całkowicie zrewolucjonizowała rozwój algorytmów rozpoznawania obrazów, a także innych aplikacji, takich jak denoising i przepływ optyczny, które wcześniej wykorzystywały techniki Bayesa i optymalizacji globalnej. Tendencję tę umożliwił rozwój wysokiej jakości wielkoskalowych anotowanych zbiorów danych, takich jak ImageNet, Microsoft COCO i LVIS. Te zbiory danych dostarczyły nie tylko wiarygodnych metryk do śledzenia postępów algorytmów rozpoznawania i segmentacji semantycznej, ale co ważniejsze, wystarczającej ilości etykietowanych danych do opracowania kompletnych rozwiązań opartych na uczeniu maszynowym.\nInnym ważnym trendem był dramatyczny wzrost mocy obliczeniowej dostępny dzięki rozwojowi algorytmów ogólnego przeznaczenia (data-parallel) na jednostkach przetwarzania graficznego (GPGPU). Przełomowa głęboka sieć neuronowa SuperVision (“AlexNet”), która jako pierwsza wygrała coroczne zawody w rozpoznawaniu obrazów na dużą skalę ImageNet, opierała się na treningu na GPU, a także na szeregu usprawnień technicznych, które przyczyniły się dramatycznie do wzrostu jej wydajności. Po opublikowaniu tej pracy postęp w wykorzystaniu głębokich architektur konwolucyjnych gwałtownie przyspieszył, do tego stopnia, że obecnie są one jedyną architekturą braną pod uwagę w zadaniach rozpoznawania i segmentacji semantycznej, a także preferowaną architekturą w wielu innych zadaniach wizyjnych, w tym w zadaniach przepływu optycznego, denoisingu i wnioskowania o głębi monokularnej (LeCun, Bengio, i Hinton 2015).\nDuże zbiory danych i architektury GPU, w połączeniu z szybkim upowszechnianiem pomysłów poprzez pojawiające się w odpowiednim czasie publikacje na arXiv, a także rozwój języków głębokiego uczenia i otwarty dostęp do modeli sieci neuronowych, przyczyniły się do gwałtownego rozwoju tej dziedziny, zarówno pod względem szybkich postępów i możliwości, jak i samej liczby publikacji i badaczy zajmujących się obecnie tymi tematami. Umożliwiły one również rozszerzenie podejść do rozpoznawania obrazów na zadania związane z rozumieniem wideo, takie jak rozpoznawanie akcji, a także zadania regresji strukturalnej, takie jak estymacja w czasie rzeczywistym wieloosobowej pozy ciała.\nSpecjalistyczne czujniki i sprzęt do zadań związanych z widzeniem komputerowym również stale się rozwijały. Wprowadzona w 2010 r. kamera głębi Microsoft Kinect szybko stała się podstawowym elementem wielu systemów modelowania 3D i śledzenia osób. W ciągu dekady systemy modelowania i śledzenia kształtu ciała 3D nadal się rozwijały, do tego stopnia, że obecnie możliwe jest wnioskowanie o modelu 3D osoby wraz z gestami i ekspresją na podstawie jednego obrazu.\n\n\n\n\nBarnard, Stephen T., i Martin A. Fischler. 1982. „Computational Stereo”. ACM Computing Surveys 14 (4): 553–72. https://doi.org/10.1145/356893.356896.\n\n\nBarrow, H. G., i J. M. Tenenbaum. 1981. „Computational Vision”. Proceedings of the IEEE 69 (5): 572–95. https://doi.org/10.1109/PROC.1981.12026.\n\n\nBlake, Andrew, i Michael Isard. 2012. Active Contours: The Application of Techniques from Graphics, Vision, Control Theory and Statistics to Visual Tracking of Shapes in Motion. Springer Science & Business Media.\n\n\nBlake, Andrew, Andrew Zisserman, i Greg Knowles. 1985. „Surface Descriptions from Stereo and Shading”. Image and Vision Computing, Papers from the 1985 Alvey Computer Vision i Image Interpretation Meeting, 3 (4): 183–91. https://doi.org/10.1016/0262-8856(85)90006-X.\n\n\nCanny, John. 1986. „A Computational Approach to Edge Detection”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\n„Cooperative Computation of Stereo Disparity | Science”. b.d. https://www.science.org/doi/10.1126/science.968482.\n\n\nDev, Parvati. 1975. „Perception of Depth Surfaces in Random-Dot Stereograms : A Neural Model”. International Journal of Man-Machine Studies 7 (4): 511–28. https://doi.org/10.1016/S0020-7373(75)80030-7.\n\n\nFelzenszwalb, Pedro F., i Daniel P. Huttenlocher. 2005. „Pictorial Structures for Object Recognition”. International Journal of Computer Vision 61 (1): 55–79. https://doi.org/10.1023/B:VISI.0000042934.15159.49.\n\n\nFergus, R., P. Perona, i A. Zisserman. 2007. „Weakly Supervised Scale-Invariant Learning of Models for Visual Recognition”. International Journal of Computer Vision 71 (3): 273–303. https://doi.org/10.1007/s11263-006-8707-x.\n\n\nFischler, M., i O. Firschein. 1987. „Readings in Computer Vision: Issues, Problems, Principles, and Paradigms”. W.\n\n\nHanson, Allen. 1978. Computer Vision Systems. Elsevier.\n\n\nHorn, Berthold K P. b.d. „Obtaining Shape from Shading Information”.\n\n\nKass, Michael, Andrew Witkin, i Demetri Terzopoulos. 1988. „Snakes: Active Contour Models”. International Journal of Computer Vision 1 (4): 321–31. https://doi.org/10.1007/BF00133570.\n\n\nLeCun, Yann, Yoshua Bengio, i Geoffrey Hinton. 2015. „Deep Learning”. Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nMalladi, R., J. A. Sethian, i B. C. Vemuri. 1995. „Shape Modeling with Front Propagation: A Level Set Approach”. IEEE Transactions on Pattern Analysis and Machine Intelligence 17 (2): 158–75. https://doi.org/10.1109/34.368173.\n\n\n„Mind as Machine: A History of Cognitive Science”. 2007. Choice Reviews Online 44 (11). https://doi.org/10.5860/choice.44-6202.\n\n\nMundy, Joseph L., i Andrew Zisserman, red. 1992. Geometric Invariance in Computer Vision. Cambridge, MA, USA: MIT Press.\n\n\nNalwa, Vishvjit S., i Thomas O. Binford. 1986. „On Detecting Edges”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 699–714. https://doi.org/10.1109/TPAMI.1986.4767852.\n\n\nPonce, Jean, Martial Hebert, Cordelia Schmid, i Andrew Zisserman. 2007. Toward Category-Level Object Recognition. Springer.\n\n\nRoberts, Lawrence G. 1980. Machine Perception of Three-Dimensional Solids. Garland Pub.\n\n\nWinston, Patrick Henry. 1976. „The Psychology of Computer Vision”. Pattern Recognition 8 (3): 193. https://doi.org/10.1016/0031-3203(76)90020-0.\n\n\nWitkin, Andrew P. 1981. „Recovering Surface Shape and Orientation from Texture”. Artificial Intelligence 17 (1): 17–45. https://doi.org/10.1016/0004-3702(81)90019-9.\n\n\nWoodham, Robert J. 1981. „Analysing Images of Curved Surfaces”. Artificial Intelligence 17 (1): 117–40. https://doi.org/10.1016/0004-3702(81)90022-9."
  },
  {
    "objectID": "digt_img.html#skale",
    "href": "digt_img.html#skale",
    "title": "\n3  Obrazy cyfrowe\n",
    "section": "\n3.1 Skale",
    "text": "3.1 Skale\n\nSkala szarości - dane obrazu w skali szarości składają się z pojedynczego kanału (ang. channel), który reprezentuje intensywność, jasność lub gęstość obrazu. W większości przypadków sens mają tylko wartości dodatnie, ponieważ liczby reprezentują natężenie energii świetlnej lub gęstość filmu, a więc nie mogą być ujemne, więc zwykle używa się całych liczb całkowitych z zakresu \\(0, \\ldots , 2^{k - 1}\\) są używane. Na przykład typowy obraz w skali szarości wykorzystuje \\(k = 8\\) bitów (1 bajt) na piksel i wartości intensywności z zakresu \\(0,\\ldots,255\\), gdzie wartość 0 oznacza minimalną jasność (czerń), a 255 maksymalną jasność (biel). W wielu zastosowaniach profesjonalnej fotografii i druku, a także w medycynie i astronomii, 8 bitów na piksel nie jest wystarczające. W tych dziedzinach często spotyka się głębię obrazu 12, 14, a nawet 16 bitów. Zauważ, że głębia bitowa zwykle odnosi się do liczby bitów używanych do reprezentowania jednego składnika koloru, a nie liczby bitów potrzebnych do reprezentowania koloru piksela. Na przykład, zakodowany w RGB kolorowy obraz z 8-bitową głębią wymagałby 8 bitów dla każdego kanału, co daje w sumie 24 bity, podczas gdy ten sam obraz z 12-bitową głębią wymagałby w sumie 36 bitów.\nObrazy binarne (ang. binary images) - to specjalny rodzaj obrazu, w którym piksele mogą przyjmować tylko jedną z dwóch wartości, czarną lub białą. Wartości te są zwykle kodowane przy użyciu pojedynczego bitu (0/1) na piksel. Obrazy binarne są często wykorzystywane do reprezentowania grafiki liniowej, archiwizacji dokumentów, kodowania transmisji faksowych i oczywiście w druku elektronicznym.\nObrazy kolorowe (ang. color images) - większość kolorowych obrazów opiera się na kolorach podstawowych: czerwonym, zielonym i niebieskim (RGB), zwykle wykorzystując 8 bitów dla każdego kanału. W tego rodzaju obrazach kolorowych, każdy piksel wymaga 3×8 = 24 bity do zakodowania wszystkich trzech składowych, a zakres każdej indywidualnej składowej koloru wynosi [0, 255]. Podobnie jak w przypadku obrazów w skali szarości, kolorowe obrazy z 30, 36 i 42 bitami na piksel są powszechnie używane w profesjonalnych aplikacjach. Wreszcie, podczas gdy większość obrazów kolorowych zawiera trzy składowe, obrazy z czterema lub więcej składowymi koloru są powszechne w druku, zwykle oparte na modelu koloru CMYK (Cyan-Magenta-Yellow- Black). Główna różnica między tymi dwiema paletami polega na tym, że RGB ma więcej możliwości kolorów, ponieważ jest w stanie wygenerować więcej odcieni niż CMYK, ale kolory wyświetlane przez RGB nie są takie same jak te, które otrzymujemy przy druku z CMYK. Kolory drukowane z CMYK mogą również różnić się od tych wyświetlanych na ekranie.\nObrazy specjalne - są wymagane, jeżeli żaden z powyższych formatów standardowych nie jest wystarczający do przedstawienia wartości obrazu. Dwa popularne przykłady obrazów specjalnych to obrazy z wartościami ujemnymi oraz obrazy z wartościami zmiennoprzecinkowymi. Obrazy z wartościami ujemnymi powstają podczas etapów przetwarzania obrazu, takich jak filtrowanie w celu wykrywania krawędzi, a obrazy z wartościami zmiennoprzecinkowymi są często spotykane w zastosowaniach medycznych, biologicznych lub astronomicznych, gdzie wymagany jest zwiększony zakres liczbowy i precyzja. Te specjalne formaty są w większości przypadków specyficzne dla danego zastosowania i dlatego mogą być trudne do wykorzystania przez standardowe narzędzia do przetwarzania obrazów."
  },
  {
    "objectID": "digt_img.html#formaty-zapisu",
    "href": "digt_img.html#formaty-zapisu",
    "title": "\n3  Obrazy cyfrowe\n",
    "section": "\n3.2 Formaty zapisu",
    "text": "3.2 Formaty zapisu\n\n3.2.1 TIFF\nTIFF (ang. Tagged Image File Format) jest formatem pliku, który jest używany do przechowywania i wymiany obrazów cyfrowych. Jest to format bezstratny, co oznacza, że po zapisaniu i odczytaniu obrazu jego jakość pozostaje taka sama. TIFF jest obsługiwany przez wiele programów do edycji obrazów i może być używany do przechowywania różnych rodzajów obrazów, w tym obrazów w skali szarości, kolorowych oraz map bitowych. Format TIFF jest często używany przez profesjonalnych fotografów i grafików, ponieważ pozwala na zachowanie wysokiej jakości obrazu i jest kompatybilny z wieloma programami i urządzeniami.\n\n3.2.2 GIF\nGIF (ang. Graphics Interchange Format) jest formatem pliku graficznego, który jest używany do przechowywania i wymiany obrazów w internecie. GIF jest formatem bezstratnym, ale jest kompresowany, co pozwala na zmniejszenie rozmiaru pliku i przyspieszenie jego przesyłania. Co ważne, GIF jest formatem obsługującym animacje, co oznacza, że może on przechowywać kilka klatek jako jeden plik, co pozwala na tworzenie animowanych obrazów, często używanych jako emotikony, ikony lub małe animacje na stronach internetowych. GIF jest również ograniczony do 256 kolorów, co oznacza, że nie jest on dobrym rozwiązaniem do przechowywania zdjęć o wysokiej jakości.\n\n3.2.3 PNG\nPNG (ang. Portable Network Graphics) jest bezstratnym formatem pliku graficznego, który jest używany do przechowywania i wymiany obrazów w internecie. Podobnie jak GIF może być kompresowany w celu zmniejszenia rozmiaru pliku. Co ważne, format PNG jest formatem obsługującym przezroczystość, co oznacza, że może on przechowywać kanał alfa, który jest odpowiedzialny za przezroczystość obrazu, co pozwala na zastosowanie efektu przezroczystości na obrazie bez konieczności dodatkowego tworzenia specjalnego tła. PNG jest również w stanie przechowywać więcej kolorów niż GIF, co oznacza, że jest to lepsze rozwiązanie dla obrazów o wysokiej jakości.\n\n3.2.4 JPEG\nJPEG (ang. Joint Photographic Experts Group) to popularny format zapisu obrazów cyfrowych, który jest szczególnie przydatny do przechowywania zdjęć. Format ten pozwala na kompresję pliku (stratną), dzięki czemu pliki JPEG są mniejsze niż pliki niekompresowane. Format ten jest szczególnie przydatny do przechowywania zdjęć z wysokim poziomem szczegółów, takich jak zdjęcia przyrody czy portrety.\n\n3.2.5 EXIF\nEXIF (ang. Exchangeable Image File Format) to format danych, który jest zapisywany w pliku obrazu cyfrowego, takim jak JPEG lub TIFF. Informacje EXIF zawierają szczegółowe dane dotyczące zdjęcia, takie jak data i godzina utworzenia zdjęcia, parametry aparatu fotograficznego (np. przysłona, czas naświetlania, ISO), dane dotyczące obiektywu, a także współrzędne GPS, jeśli zdjęcie zostało zrobione z użyciem aparatu z GPS. EXIF jest przydatny dla fotografów i programów do obróbki zdjęć, ponieważ pozwala na łatwe odczytanie i wykorzystanie tych danych.\n\n3.2.6 BMP\nBMP (ang. Bitmap) to format pliku obrazu, który jest przeznaczony do przechowywania obrazów rastrowych, takich jak zdjęcia, grafiki i mapy bitowe. BMP jest formatem pliku natywnym dla systemów operacyjnych Windows, co oznacza, że pliki tego formatu są bezpośrednio obsługiwane przez system Windows i nie wymagają dodatkowego oprogramowania do odczytu.\nBMP jest formatem bezstratnym, co oznacza, że po zapisie obrazu w tym formacie, jego jakość pozostaje taka sama jak przed zapisem. Pliki BMP są jednak dość duże, ponieważ nie są skompresowane, co oznacza, że zajmują więcej miejsca na dysku niż pliki skompresowane innymi formatami. BMP jest często używany do przechowywania obrazów w celach archiwizacyjnych, ponieważ zachowuje pełną jakość obrazu.\n\n\nPorównanie formatów\n\n\n\n3.2.7 Operacje na plikach\nIstnieje wiele zewnętrznych (w stosunku do środowiska R) profesjonalnych narzędzi do obróbki zdjęć. Wśród nich z pewnością należy wymienić: Adobe Photoshop, CorelDRAW, Gimp, PIXLR, FIji (ImageJ) i wiele innych. Część z nich jest komercyjna, a część darmowa. Osobiście do przetwarzania obrazów pochodzących z badań biologicznych, medycznych, czy inżynierskich polecam darmowy program Fiji, będący rozszerzeniem swojego pierwowzoru, czyli ImageJ.\nRównież w samym środowisku R istnieje szereg bibliotek do obsługi obrazów:\n\n\nimager - pozwala na szybkie przetwarzanie obrazów w maksymalnie 4 wymiarach (dwa wymiary przestrzenne, jeden wymiar czasowy/głębokościowy, jeden wymiar koloru). Udostępnia większość tradycyjnych narzędzi do przetwarzania obrazów (filtrowanie, morfologia, transformacje, itp.), jak również różne funkcje do łatwej analizy danych obrazowych przy użyciu R.\n\nimagerExtra - poszerzenie zestawu funkcji pakietu imager.\n\nmagick - dostarcza nowoczesnego i prostego zestawu narzędzi do przetwarzania obrazów w R. Obejmuje on ImageMagick STL, który jest najbardziej wszechstronną biblioteką przetwarzania obrazów typu open-source dostępną obecnie.\n\nimageseg - pakiet ogólnego przeznaczenia do segmentacji obrazów z wykorzystaniem modeli TensorFlow opartych na architekturze U-Net autorstwa Ronneberger, Fischer, i Brox (2015) oraz architekturze U-Net++ autorstwa Zhou i in. (2018). Dostarcza wstępnie wytrenowane modele do oceny gęstości łanu i gęstości roślinności podszytu na podstawie zdjęć roślinności. Ponadto pakiet zapewnia workflow do łatwego tworzenia wejściowych modeli i architektur modeli dla segmentacji obrazów ogólnego przeznaczenia na podstawie obrazów w skali szarości lub kolorowych, zarówno dla segmentacji obrazów binarnych, jak i wieloklasowych.\n\npliman - jest pakietem do analizy obrazów, ze szczególnym uwzględnieniem obrazów roślin. Jest użytecznym narzędziem do uzyskania informacji ilościowej dla obiektów docelowych. W kontekście obrazów roślin, ilościowe określanie powierzchni liści, nasilenia chorób, liczby zmian chorobowych, liczenie liczby ziaren, uzyskiwanie statystyk ziaren (np. długość i szerokość) można wykonać stosując pakiet pliman.\n\n\nPrzykład 3.1 W tym przykładzie przedstawiona zostanie procedura importu i eksportu obrazów do różnych formatów. Najpierw wczytamy zdjęcie wykonane telefonem (IPhone 12) zapisane w formacie TIFF, a następnie zapiszemy to zdjęcie w kilku innych formatach, by na końcu wczytać je wszystkie i porównać.\n\nKodlibrary(magick)\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\n\n# wczytywanie obrazu \nimg_orig <- image_read(\"images/IMG_3966.tiff\")\n\n# informacje o obrazie\nimg_orig_info <- image_info(img_orig)\nimg_orig_info\n\n# A tibble: 1 × 7\n  format width height colorspace matte filesize density\n  <chr>  <int>  <int> <chr>      <lgl>    <int> <chr>  \n1 TIFF    3024   4032 sRGB       FALSE 36614010 72x72  \n\n\nJak widać obraz w formacie TIFF zajmuje bardzo dużo miejsca na dysku (36,6MB).\n\nKod# eksport do GIF\nimage_write(img_orig, path = \"images/img.gif\", \n            format = \"gif\")\n\n# eksport do PNG\nimage_write(img_orig, path = \"images/img.png\", \n            format = \"png\")\n\n# eksport do JPEG z jakością 100%\nimage_write(img_orig, path = \"images/img.jpeg\", \n            quality = 100, format = \"jpeg\")\n\n# eksport do JPEG z jakością 75%\nimage_write(img_orig, path = \"images/img2.jpeg\", \n            quality = 75, format = \"jpeg\")\n\n# eksport do JPEG z jakością 50%\nimage_write(img_orig, path = \"images/img3.jpeg\", \n            quality = 50, format = \"jpeg\")\n\n# eksport do JPEG z jakością 25%\nimage_write(img_orig, path = \"images/img4.jpeg\", \n            quality = 25, format = \"jpeg\")\n\n\nPo zapisie do innych formatów pliki znacznie zmniejszyły swoją wielkość.\n\nKodimg_gif <- image_read(\"images/img.gif\")\nimg_gif_info <- image_info(img_gif)\n\nimg_png <- image_read(\"images/img.png\")\nimg_png_info <- image_info(img_png)\n\nimg_jpeg1 <- image_read(\"images/img.jpeg\")\nimg_jpeg1_info <- image_info(img_jpeg1)\n\nimg_jpeg2 <- image_read(\"images/img2.jpeg\")\nimg_jpeg2_info <- image_info(img_jpeg2)\n\nimg_jpeg3 <- image_read(\"images/img3.jpeg\")\nimg_jpeg3_info <- image_info(img_jpeg3)\n\nimg_jpeg4 <- image_read(\"images/img4.jpeg\")\nimg_jpeg4_info <- image_info(img_jpeg4)\n\nbind_rows(img_orig_info, img_gif_info, img_png_info,\n          img_jpeg1_info, img_jpeg2_info, img_jpeg3_info,\n          img_jpeg4_info) |> \n  gt()\n\n\n\n\n\n\nTabela 3.1: Podstawowe informacje o obrazach\n\nformat\nwidth\nheight\ncolorspace\nmatte\nfilesize\ndensity\n\n\n\nTIFF\n3024\n4032\nsRGB\nFALSE\n36614010\n72x72\n\n\nGIF\n3024\n4032\nsRGB\nFALSE\n7422827\n72x72\n\n\nPNG\n3024\n4032\nsRGB\nFALSE\n11994144\n28x28\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n8211226\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n1546461\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n1043337\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n691309\n72x72\n\n\n\n\n\n\n\n\nPomimo znacznej różnicy w wielkości obrazów (w sensie miejsca zajmowanego na dysku) nie różnią się one znacznie jakością (przynajmniej na pierwszy rzut oka)\n\nKodimg_orig\nimg_jpeg1\nimg_gif\nimg_png\nimg_jpeg2\nimg_jpeg3\nimg_jpeg4\n\n\n\n\n\nRysunek 3.3: TIFF\n\n\n\n\n\n\nRysunek 3.4: JPEG 100%\n\n\n\n\n\n\n\n\nRysunek 3.5: GIF\n\n\n\n\n\n\nRysunek 3.6: PNG\n\n\n\n\n\n\n\n\nRysunek 3.7: JPEG 75%\n\n\n\n\n\n\nRysunek 3.8: JPEG 50%\n\n\n\n\n\n\n\n\nRysunek 3.9: JPEG 25%\n\n\n\n\n\n\n\nKodmagick2cimg(img_orig) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_jpeg1) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_gif) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_png) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\n\n\n\n\nRysunek 3.10: Histogramy RGB dla pliku TIFF\n\n\n\n\n\n\nRysunek 3.11: Histogray RGB dla pliku JPEG 100%\n\n\n\n\n\n\n\n\nRysunek 3.12: Histogray RGB dla pliku GIF\n\n\n\n\n\n\nRysunek 3.13: Histogray RGB dla pliku PNG\n\n\n\n\n\n\n\n\n\n\n\nRonneberger, Olaf, Philipp Fischer, i Thomas Brox. 2015. „U-Net: Convolutional Networks for Biomedical Image Segmentation”. arXiv. https://doi.org/10.48550/arXiv.1505.04597.\n\n\nZhou, Zongwei, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, i Jianming Liang. 2018. „UNet++: A Nested U-Net Architecture for Medical Image Segmentation”. arXiv. https://doi.org/10.48550/arXiv.1807.10165."
  },
  {
    "objectID": "transformations.html#przykłady-różnych-transformacji",
    "href": "transformations.html#przykłady-różnych-transformacji",
    "title": "\n4  Transformacje geometryczne\n",
    "section": "\n4.1 Przykłady różnych transformacji",
    "text": "4.1 Przykłady różnych transformacji\n\nKodlibrary(magick)\nfrink <- image_read(\"https://jeroen.github.io/images/frink.png\")\nfrink\n\n\n\nFrink w oryginalnym rozmiarze\n\n\n\n\n\nKodimage_scale(frink, \"250%x250%\")\n\n\n\nFrink powiększony o 150%\n\n\n\n\n\nKodimage_scale(frink, \"300x100!\")\n\n\n\nFrink poszerzony do proporcji 3:1\n\n\n\n\n\nKodimage_rotate(frink, 90)\n\n\n\nRotacja o 90 stopni"
  },
  {
    "objectID": "transformations.html#rotacje-obrazów",
    "href": "transformations.html#rotacje-obrazów",
    "title": "\n4  Transformacje geometryczne\n",
    "section": "\n4.2 Rotacje obrazów",
    "text": "4.2 Rotacje obrazów\nObroty o inne kąty niż pełne wielokrotności 90\\(\\degree\\) powodują pewne problemy, ponieważ po rotacji powstają piksele, które nie pokrywają żadnej wartości z oryginalnego obrazu, a część z nich zawiera kilka wartości (Rysunek 4.1). Podobne zjawisko może powstać w sytuacji zmiany rozmiaru obrazów. W tych sytuacjach konieczna jest interpolacja pikseli zarówno “pustych”, jak i “wypełnionych”.\n\n\nRysunek 4.1: Przykład rotacji obrazu\n\n\n\nKodlibrary(imager)\nlibrary(keras)\n\ndata <- dataset_mnist()\nim <- t(data$train$x[2023,,])\nim <- as.cimg(im)\n\nim_list <- map_il(0:6, ~imrotate(im, # Nearest Neighbour interp.\n                                 angle = 15*.x, \n                                 interpolation = 0)) \nrow1 <- imappend(im_list[1:4], axis = \"x\")  \nrow2 <- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |> \n  plot(interp=F) # antialiasing off\n\nim_list <- map_il(0:6, ~imrotate(im, # linear interpolation\n                                 angle = 15*.x, \n                                 interpolation = 1)) \nrow1 <- imappend(im_list[1:4], axis = \"x\")  \nrow2 <- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |> \n  plot(interp=F)\n\nim_list <- map_il(0:6, ~imrotate(im, # cubic interpolation\n                                 angle = 15*.x, \n                                 interpolation = 2)) \nrow1 <- imappend(im_list[1:4], axis = \"x\")  \nrow2 <- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |> \n  plot(interp=F) \n\n\n\n(a) Nearest Neighbour interpolation\n\n\n\n\n\n\n(b) Linear interpolation\n\n\n\n\n\n\n(c) Cubic interpolation\n\n\n\nRysunek 4.2: Przykład rotacji o wielokrotnośc \\(15\\degree\\)\n\n\nJak widać z powyższych wykresów rotacja wpływa na jakość obrazu, dlatego zaleca się nie stosować kilku występujących po sobie rotacji o kąty niebędące wielokrotnościami \\(90\\degree\\).\n\nKodim2 <- imlist(im)\nfor(i in 1:6){\n  im2[[i+1]] <- imrotate(im2[[i]], angle = 15)\n}\nimappend(im2, axis = \"x\") |> plot(interp=F)\n\n\n\nRysunek 4.3: Kilka iteracji rotacji obrazu. Każdy nastepny powstaje jako rotacja poprzedniego."
  },
  {
    "objectID": "transformations.html#zmiana-rozmiaru-obrazu",
    "href": "transformations.html#zmiana-rozmiaru-obrazu",
    "title": "\n4  Transformacje geometryczne\n",
    "section": "\n4.3 Zmiana rozmiaru obrazu",
    "text": "4.3 Zmiana rozmiaru obrazu\nW przypadku gdy zmieniamy rozmiar obrazu w proporcji 0.5, 2, 3 funkcja imresize korzysta z algorytmu opisanego na stronie http://www.scale2x.it/algorithm.html.\n\nKodim_list <- map_il(c(0.5,1, 2), ~imresize(im, \n                                         scale = .x)) \nimappend(im_list, \"x\") |>\n  plot(interp = F) # antialiasing off\n\n\n\nRysunek 4.4: Zmiana rozmiaru obrazu\n\n\n\n\nW innych przypadkach wykorzystuje jedną z 7 opcji interpolacji:\n\nbrak interpolacji (opcja -1);\nbrak interpolacji ale dodatkowo powstała przestrzeń jest wypełniana zgodnie z warunkiem brzegowym boundary_conditions (opcja 0);\ninterpolacja metodą najbliższego sąsiada (opcja 1);\ninterpolacja metodą średniej kroczącej (opcja 2);\ninterpolacja liniowa (opcja 3);\ninterpolacja na siatce (opcja 4);\ninterpolacja kubiczna (opcja 5);\ninterpolacja Lanczosa (opcja 6).\n\n\nKodim_list <- map_il(c(-1:6), ~imresize(im, \n                                     scale = 1.5,\n                                     interpolation = .x))\nrow1 <- imappend(im_list[1:4], \"x\")\nrow2 <- imappend(im_list[5:7], \"x\")\nimappend(list(row1, row2), \"y\") |>\n  plot(interp = F)\n\n\n\nRysunek 4.5: Zastosowania różnych interpolacji"
  },
  {
    "objectID": "point_trans.html",
    "href": "point_trans.html",
    "title": "\n5  Transformacje punktowe\n",
    "section": "",
    "text": "Transformacje obrazów, które odbywają się na poziomie pojedynczych pikseli nazywane są w literaturze tematu transformacjami punktowymi (ang. point transformation). Należą do nich między innymi:\n\nmodyfikacja kontrastu,\nmodyfikacja jasności,\nzmiana intensywności,\nodwracanie wartości piksela,\nkwantyzacja obrazów (ang. posterizing),\nprogowanie,\nkorekta gammy,\ntransformacje kolorów.\n\nWszystkie można opisać formułą\n\\[\ng(x) = h(f(x)),\n\\]\ngdzie \\(x = (i,j)\\) jest położeniem transformowanego piksela, \\(f\\) jest funkcja oryginalnego obrazu (przed przekształceniem)1, natomiast \\(h\\) jest zastosowaną transformacją. Wówczas \\(g\\) opisuje transformację jako funkcję lokalizacji.1 informuje o nasyceniu barw w danej lokalizacji - pikselu\nNa potrzeby zmian w kontraście, czy jasności stosuje się przekształcenia postaci:\n\\[\ng(x) = a(x)\\cdot f(x)+b(x),\n\\]\ngdzie \\(a(x)\\) jest parametrem zmiany kontrastu, a \\(b(x)\\) jest parametrem zmiany jasności2. Należy jednak pamiętać, że wartości \\(g(x)\\) tak określonej transformacji mogą znaleźć się poza przedziałem [0,255]. Oczywiście powoduje to problem, z którym można sobie radzić poprzez progowanie wartości, tak aby znalazły się w przedziale [0,255]. Obrazy zapisane w formacie cimg wartości poszczególnych kanałów mają znormalizowane do przedziału [0,1]. Dodatkowo należy pamiętać, że funkcja plot ma domyślnie włączoną flagę rescale=TRUE co oznacza, że wartości kanałów zostaną znormalizowane do przedziału [0,1] automatycznie. W rezultacie oznacza to, że transformacja liniowa \\(g(x)\\) nie odniesie żadnego skutku.2 jeśli zmieniamy te parametry globalnie (dla całego obrazu) wówczas funkcje te są stałe\n\nKodlibrary(imager)\nlibrary(imagerExtra)\nlibrary(tidyverse)\n\nadjust <- function(x, contrast, brithness){\n  \n  # transformacji dokonujemy od razu na wszystkich kanałach\n  x_trans <- x |> \n    as.data.frame() |> \n    mutate(value = ifelse(value*(contrast+1)+brithness>1, \n                          1, \n                          ifelse(value*(contrast+1)+brithness<0, 0,\n                                 value*(contrast+1)+brithness)), \n           .by  = cc) |> \n    as.cimg(dims = dim(x))\n  \n  return(x_trans)\n}\n\nlayout(t(1:2))\nplot(boats, rescale = F, interpolate = F)\nadjust(boats, 0.7, 0) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.1: Zmiana kontrastu obrazu (podbicie o 70%)\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats, rescale = F, interpolate = F)\nadjust(boats, -0.3, 0) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.2: Zmiana kontrastu obrazu (redukcja o 30%)\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nadjust(boats, 0, 0.2) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.3: Zmiana jasności obrazu (podbicie o 20% pełnej skali)\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nadjust(boats, 0, -0.4) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.4: Zmiana jasności obrazu (redukcja o 40% pełnej skali)\n\n\n\n\nAby uniknąć przekraczania wartości dla poszczególnych kanałów wprowadza się często funkcję, która podnosi kontrast (tzw. autokontrast) przez poszerzenie spektrum wartości z obserwowanych \\([x_{\\min},x_{\\max}]\\) do przedziału \\([lower, upper]\\) (często przyjmowane jako [0,1]):\n\\[\ng(x) = x_{lower}+(x-x_{\\min})\\cdot\\frac{x_{\\max}-x_{\\min}}{x_{upper}-x_{lower}}.\n\\] Funkcja EqualizeDP pozwala na autokontrast. Dodatkowo umożliwia ustawić wartości skrajne dla spektrum kanału inne niż min i max.\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~EqualizeDP(.x, t_down = min(.x),t_up = max(.x), range = c(0,1))) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.5: Autokontrast w zakresie min, max\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~EqualizeDP(.x, t_down = 50,t_up = 170, range = c(0,1))) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.6: Autokontrast w zakresie 50, 170\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~{\n    max(.x)-.x\n  }) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.7: Przykład odwrócenia wartości pikseli\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~round(.x/0.2, digits = 0)) |> \n  imappend(\"c\") |> \n  plot(interpolate = F)\n\n\n\nRysunek 5.8: Przykład kwantyzacji obrazu\n\n\n\n\n\nKodlist(\"10%\",\"25%\", \"50%\", \"75%\", \"auto\") |> \n  map_il(~threshold(im = boats, thr = .x)) |> \n  imappend(\"x\") |> \n  plot()\n\n\n\nRysunek 5.9: Przykład progowania obrazu z różnymi poziomami progowania\n\n\n\n\n\nKod2^(-3:3) |> \n  map_il(~{\n    boats^.x\n  }) |> \n  imappend(\"x\") |> \n  plot()\n\n\n\nRysunek 5.10: Przykład korekty gamma dla różnych potęg\n\n\n\n\nTransformację kolorów możemy wykonywać na dwa sposoby:\n\nprzeprowadzając transformację oddzielnie na każdym kanale,\nzachowując barwę (ang. hue) obrazu, przetwarzać składową intensywności, a następnie obliczać wartości RGB z nowej składowej intensywności.\n\n\nKodlayout(t(1:2))\nplot(boats)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~BalanceSimplest(.x, 2, 2, range = c(0,1))) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.11: Transformacja kolorów niezależnie modyfikując każdy kanał\n\n\n\n\nAby wykonać transformację drugą metodą zapiszemy plik w skali szarości, czyli dokonamy faktycznie agregacji postaci:\n\\[\ng_{grayscale}(x)=0.3R+0.59G+0.11B.\n\\]\nPonadto będziemy potrzebowali intensywności barw. Do zapisu nasycenia barw użyjemy funkcji GetHue pakietu imagerExtra.\n\nKod# zapisujemy obraz w skali szarości\ng <- grayscale(boats)\n# zapisujemy nasycenia barw\nhueim <- GetHue(boats)\n# transformujemy obraz w skali szarości\ng <- BalanceSimplest(g, 2, 2, range = c(0,1))\n# oddtwarzamy kolory\ny <- RestoreHue(g, hueim)\n\nlayout(t(1:2))\nplot(boats)\nplot(y)\n\n\n\nRysunek 5.12: Transformacja kolorów modyfikując kanały łącznie"
  },
  {
    "objectID": "filters.html#filtry-liniowe",
    "href": "filters.html#filtry-liniowe",
    "title": "\n6  Filtry\n",
    "section": "\n6.1 Filtry liniowe",
    "text": "6.1 Filtry liniowe\nFiltry liniowe są nazywane w ten sposób, ponieważ łączą wartości pikseli w otoczeniu w sposób liniowy, czyli jako suma ważona. Szczególnym przykładem jest omówiony na początku proces uśredniania lokalnego (Równanie 6.1), gdzie wszystkie dziewięć pikseli w lokalnym otoczeniu 3 × 3 jest dodawanych z identycznymi wagami (1/9). Dzięki temu samemu mechanizmowi można zdefiniować mnóstwo filtrów o różnych właściwościach, modyfikując po prostu rozkład poszczególnych wag.\n\\[\nI'(u,v) = \\frac19\\sum_{j = -1}^1\\sum_{i = -1}^1I(u+i,v+j),\n\\tag{6.1}\\]\nDla dowolnego filtra liniowego rozmiar i kształt regionu wsparcia (ang. support region), jak również wagi poszczególnych pikseli, są określone przez jądro filtra (ang. kernel) \\(H(i,j)\\). Rozmiar jądra \\(H\\) równa się rozmiarowi regionu filtrującego, a każdy element \\((i, j)\\) określa wagę odpowiedniego piksela w sumowaniu. Dla filtra wygładzającego 3x3 w równaniu (Równanie 6.1), jądro filtra to\n\\[\nH = \\begin{bmatrix}\n  1/9,&1/9,&1/9\\\\\n  1/9,&1/9,&1/9\\\\\n  1/9,&1/9,&1/9\n\\end{bmatrix}=\n\\frac19\\begin{bmatrix}\n  1,&1,&1\\\\\n  1,&1,&1\\\\\n  1,&1,&1\n\\end{bmatrix}\n\\]\nponieważ każda z wartości filtra wnosi 1/9 do piksela wynikowego.\nW istocie, jądro filtra \\(H(i, j)\\) jest, podobnie jak sam obraz, dyskretną, dwuwymiarową funkcją o rzeczywistą, \\(H : \\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{R}\\). Filtr ma swój własny układ współrzędnych z początkiem - często określanym jako hot spot - przeważnie (ale niekoniecznie) znajdującym się w środku. Tak więc współrzędne filtra są na ogół dodatnie i ujemne (Rysunek 6.2). Funkcja filtra ma nieskończony zakres i jest uważana za zerową poza obszarem zdefiniowanym przez macierz \\(H\\).\n\n\nRysunek 6.2: Schemat działania filtru\n\n\nDla filtru liniowego wynik jest jednoznacznie i całkowicie określony przez współczynniki jądra filtru. Zastosowanie filtru do obrazu jest prostym procesem, który został zilustrowany na Rysunek 6.2. W każdej pozycji obrazu \\((u, v)\\) wykonywane są następujące kroki:\n\nJądro filtra \\(H\\) jest przesuwane nad oryginalnym obrazem \\(I\\) tak, że jego początek \\(H(0, 0)\\) pokrywa się z aktualną pozycją obrazu \\((u, v)\\).\nWszystkie współczynniki filtra \\(H(i, j)\\) są mnożone z odpowiadającym im elementem obrazu \\(I(u+i,v+j)\\), a wyniki są sumowane.\nNa koniec otrzymana suma jest zapisywana w aktualnej pozycji w nowym obrazie \\(I'(u, v)\\).\n\nOpisując formalnie, wartości pikseli nowego obrazu \\(I'(u,v)\\) są obliczane przez operację\n\\[\nI'(u,v) = \\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(i,j),\n\\tag{6.2}\\]\ngdzie \\(R_H\\) oznacza zbiór współrzędnych pokrytych przez filtr \\(H\\). Nie całkiem dla wszystkich współrzędnych, aby być dokładnym. Istnieje oczywisty problem na granicach obrazu, gdzie filtr sięga poza obraz i nie znajduje odpowiadających mu wartości pikseli, które mógłby wykorzystać do obliczenia wyniku. Na razie ignorujemy ten problem granic, ale w dalszej części tego wykładu się tym zajmiemy.\n\n\nRysunek 6.3: Ilustracja działania filtru\n\n\nSkoro rozumiemy już zasadnicze działanie filtrów i wiemy, że granice wymagają szczególnej uwagi, możemy pójść dalej i zaprogramować prosty filtr liniowy. Zanim jednak to zrobimy, możemy chcieć rozważyć jeszcze jeden szczegół. W operacji punktowej każda nowa wartość piksela zależy tylko od odpowiadającej jej wartości piksela w oryginalnym obrazie, dlatego nie było problemu z zapisaniem wyników z powrotem do tego samego obrazu - obliczenia są wykonywane “w locie” bez potrzeby pośredniego przechowywania. Obliczenia w miejscu nie są generalnie możliwe dla filtra, ponieważ każdy oryginalny piksel przyczynia się do zmiany więcej niż jednego piksela wynikowego i dlatego nie może być zmodyfikowany przed zakończeniem wszystkich operacji.\nPotrzebujemy zatem dodatkowego miejsca na przechowywanie obrazu wynikowego, który następnie może być ponownie skopiowany do obrazu źródłowego (jeśli jest to pożądane). Tak więc kompletna operacja filtrowania może być zaimplementowana na dwa różne sposoby (Rysunek 6.4):\n\nWynik obliczeń filtra jest początkowo zapisywany w nowym obrazie, którego zawartość jest ostatecznie zapisywana z powrotem do obrazu oryginalnego.\nOryginalny obraz jest najpierw kopiowany do obrazu pośredniego, który służy jako źródło dla właściwej operacji filtrowania. Wynik zastępuje piksele w oryginalnym obrazie.\n\n\n\nRysunek 6.4: Dwa schematy implementacji filtrów do obrazów\n\n\nDla obu wersji wymagana jest taka sama ilość pamięci masowej, a więc żadna z nich nie oferuje szczególnej przewagi. W poniższych przykładach używamy na ogół wersji B.\nW filtrze prezentowanym powyżej wagi nie muszą być wszystkie takie same. Przykładowo filtr \\(H(u,v)\\) określony następująco\n\\[\nH(u,v)=\\begin{bmatrix}\n  0.075,&0.125,&0.075\\\\\n  0.125,&0.200,&0.125\\\\\n  0.075,&0.125,&0.075\n\\end{bmatrix}\n\\tag{6.3}\\]\nrównież uśrednia wartości w regionie wsparcia filtru ale nadając największe wagi wartościom w środku.\nZauważmy, że wagi filtra \\(H\\) są tak dobrane aby się sumowały do 1. Oznacza to, że filtr ten jest znormalizowany. Normalizacji filtrów używa się po to aby uniknąć sytuacji, w której wartość wyjściowa z filtra byłaby większa niż 255.\n\n\n\n\n\n\nWskazówka\n\n\n\nPonieważ funkcja plot pakietu imager ma włączoną opcję rescale = TRUE co oznacza, że wartości wynikowe i tak będą przekształcone do przedziału [0,1], to nie unormowane filtry i tak będą wyświetlać poprawnie przefiltrowane obrazy.\n\n\n\nKodlibrary(MASS)\nfilter1 <- matrix(c(1,1,1,\n                   1,1,1,\n                   1,1,1), \n                  ncol = 3)\nfilter1\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\nKodfilter2 <- filter1/9\nfilter2 |> \n  fractions() # aby ładnie wyświetlić ułamki\n\n     [,1] [,2] [,3]\n[1,] 1/9  1/9  1/9 \n[2,] 1/9  1/9  1/9 \n[3,] 1/9  1/9  1/9 \n\nKod# filtry w funkcji convolve muszą być zapisane jako obraz (as.cimg)\nlayout(t(1:2))\nconvolve(boats, as.cimg(filter1)) |> plot()\nconvolve(boats, as.cimg(filter2)) |> plot()\n\n\n\nRysunek 6.5: Przykład użycia filtra przed i po normalizacji\n\n\n\n\n\nKodtry(\n  convolve(boats, as.cimg(filter1)) |> \n  plot(rescale = FALSE)\n)\n\nError in colourscale(v[[1]], v[[2]], v[[3]]) : \n  color intensity 1.57913, not in [0,1]\n\n\nZatem możemy w konstruowaniu filtrów stosować wartości całkowite i wspólnej wartości normalizacyjnej.\n\\[\nH(u,v)=\\begin{bmatrix}\n  0.075,&0.125,&0.075\\\\\n  0.125,&0.200,&0.125\\\\\n  0.075,&0.125,&0.075\n\\end{bmatrix}=\n\\frac1{40}\\cdot\n\\begin{bmatrix}\n  3,&5,&3\\\\\n  5,&8,&5\\\\\n  3,&5,&3\n\\end{bmatrix}.\n\\]\nAby uniknąć wartości ujemnych, które mogłyby się pojawić w przypadku gdy jądro filtra zawierałoby wartości ujemne, stosuje się stałą przesunięcia (ang. offset). Wówczas Równanie 6.2 przyjmuje postać\n\\[\nI'(u,v) = Offset+\\frac{1}{Scale}\\cdot \\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(i,j),\n\\]\na \\(H(i,j)\\) jest zdefiniowany na \\(\\mathbb{Z}\\times\\mathbb{Z}\\). Chociaż najczęściej używa się filtrów kwadratowych, to nie ma przeszkód aby stosować również filtry prostokątne. Powyższa formuła ma zastosowanie do filtrów o dowolnym rozmiarze i kształcie.\n\n6.1.1 Przykładowe filtry wygładzające (ang. smoothing)\n\nFiltr pudełkowy (ang box), którego zasada działania została już przedstawiona w Równanie 6.1 dla rozmiaru filtra 3x3 jest jednym z filtrów wygładzających. Im większy jest rozmiar filtra, tym większy stopień wygładzenia obrazu wyjściowego. Ten najprostszy ze wszystkich filtrów wygładzających, którego kształt 3D przypomina pudełko (Rysunek 6.6 (a)), jest dobrze znany. Niestety, filtr pudełkowy jest daleki od optymalnego filtra wygładzającego ze względu na swoje dziwne zachowanie w przestrzeni częstotliwości, które jest spowodowane ostrym odcięciem wokół jego boków. Opisane w kategoriach częstotliwościowych wygładzanie odpowiada tzw. filtracji dolnoprzepustowej, czyli efektywnemu tłumieniu wszystkich składowych sygnału powyżej danej częstotliwości odcięcia. Filtr pudełkowy wytwarza jednak silne “dzwonienie” w przestrzeni częstotliwości i dlatego nie jest uważany za wysokiej jakości filtr wygładzający. Przypisanie tej samej wagi wszystkim pikselom obrazu w regionie filtru może też wydawać się dość doraźne. Zamiast tego należałoby prawdopodobnie oczekiwać, że silniejszy nacisk zostanie położony na piksele znajdujące się w pobliżu centrum filtra niż na te bardziej odległe. Ponadto filtry wygładzające powinny ewentualnie działać “izotropowo” (tzn. jednolicie w każdym kierunku), co z pewnością nie ma miejsca w przypadku filtra prostokątnego.\nFiltr gaussowski - z pewnością lepszy w tym kontekście wygładzania z względu na brak ostrych krawędzi jądra. Definiuje się go następująco \\[\nH^{G,\\sigma}(x,y) = e^{-\\frac{x^2+y^2}{2\\sigma^2}},\n\\] gdzie \\(\\sigma\\) oznacza odchylenie standardowe rozkładu.\n\n6.1.2 Przykład filtru różnicującego\nJeśli niektóre współczynniki filtra są ujemne, to obliczenie filtra można zinterpretować jako różnicę dwóch sum: suma ważona wszystkich pikseli z przypisanymi współczynnikami dodatnimi minus suma ważona pikseli z ujemnymi współczynnikami w regionie filtra RH , czyli\n\\[\n\\begin{align}\n  I'(u,v)=&\\sum_{(i,j)\\in R^+}I(u+i, v+j)\\cdot \\vert H(i,j)\\vert -\\\\\n-&\\sum_{(i,j)\\in R^-}I(u+i, v+j)\\cdot \\vert H(i,j)\\vert,\n\\end{align}\n\\tag{6.4}\\]\ngdzie \\(R^-, R^+\\) oznaczają podział filtra na współczynniki ujemne \\(H(i,j)<0\\) i dodatnie \\(H(i,j)>0\\) odpowiednio. Na przykład filtr Laplace’a 5x5 na Rysunek 6.6 (c) oblicza różnicę między pikselem środkowym (o wadze 16) a sumą ważoną 12 otaczających go pikseli (o wagach -1 lub -2). Pozostałe 12 pikseli ma przypisane zerowe współczynniki i dlatego są one ignorowane w obliczeniach. Podczas gdy lokalne zmiany intensywności są wygładzane przez uśrednianie, możemy oczekiwać, że w przypadku różnic stanie się dokładnie odwrotnie - lokalne zmiany intensywności zostaną wzmocnione.\n\n\nRysunek 6.6: Przykłady różnych filtrów o rozmiarze 5x5. (a) filtr pudełkowy, (b) gaussowski, (c) Laplace’a (zwany także Mexican Hut)\n\n\n\n6.1.3 Formalny zapis operatorów filtracji\nWspomniany zapis w równaniu Równanie 6.2 nazywany jest w literaturze operatorem korelacyjnym (ang. correlation operator). Ma on jedną poważną wadę, ponieważ filtr zastosowany do obrazu z pojedynczym wtrąceniem (jednym pikselem świecącym) w rezultacie daje w obrazie wynikowym wartości filtra zrotowane o \\(180\\degree\\) (patrz Rysunek 6.7).\n\n\nRysunek 6.7: Zastosowanie operatora korelacyjnego na obrazie z jednym wtrąceniem\n\n\nRozwiązaniem tej niedogodności jest wprowadzenie operatora konwolucyjnego (ang. convolution operator). Definiuje się go w następujący sposób:\n\\[\nI'(u,v) = \\sum_{i,j\\in R_H}I(u-i, v-j)\\cdot H(i,j),\n\\tag{6.5}\\]\nzapisywany również w bardziej zwartej formie\n\\[\nI'=I*H.\n\\tag{6.6}\\]\nAby pokazać związek pomiędzy oboma sposobami filtracji przekształćmy wzór Równanie 6.5\n\\[\n\\begin{align}\n  I'(u,v) =& \\sum_{i,j\\in R_H}I(u-i, v-j)\\cdot H(i,j)=\\\\\n  =&\\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(-i,-j)=\\\\\n  =&\\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H^*(i,j),\n\\end{align}\n\\tag{6.7}\\]\ngdzie \\(H^*(i,j)=H(-i,-j)\\). Zmiana parametryzacji powoduje obrócenie wyniku o \\(180\\degree\\).\n\n\nRysunek 6.8: Zastosowanie operatora konwolucyjnego do obrazu z jednym wtrąceniem\n\n\n\n6.1.4 Własności filtrów liniowych\nKonwolucja liniowa jest odpowiednim modelem dla wielu rodzajów zjawisk naturalnych, w tym układów mechanicznych, akustycznych i optycznych. W szczególności istnieją silne formalne powiązania z reprezentacją Fouriera sygnałów w dziedzinie częstotliwości, które są niezwykle cenne dla zrozumienia złożonych zjawisk, takich jak próbkowanie i aliasing. Poniżej przedstawione zostaną własności konwolucji liniowej.\n\n6.1.4.1 Przemienność\nKonwolucja liniowa jest przemienna, czyli dla dowolnego obrazu \\(I\\) i jądra filtru \\(H\\), zachodzi\n\\[\nI ∗ H = H ∗ I.\n\\tag{6.8}\\]\nWynik jest więc taki sam, jeśli obraz i jądro filtra są wzajemnie zamienione, i nie ma różnicy, czy składamy obraz \\(I\\) z jądrem \\(H\\), czy odwrotnie.\n\n6.1.4.2 Liniowość\nFiltry liniowe nazywane są tak ze względu na właściwości liniowości operacji konwolucji, która przejawia się w różnych aspektach. Na przykład, jeśli obraz jest mnożony przez skalar \\(s\\in\\mathbb{R},\\) to wynik konwolucji mnoży się o ten sam czynnik, czyli\n\\[\n(s\\cdot I)∗H = I ∗(s\\cdot H) = s\\cdot(I ∗H).\n\\tag{6.9}\\]\nPodobnie, jeśli dodamy dwa obrazy \\(I_1\\), \\(I_2\\) piksel po pikselu i spleciemy wynikowy obraz za pomocą pewnego jądra \\(H\\), to taki sam wynik uzyskamy splatając każdy obraz osobno i dodając potem oba wyniki, czyli\n\\[\n(I_1 +I_2)∗H = (I_1 ∗H)+(I_2 ∗H).\n\\tag{6.10}\\]\nZaskakujące może być jednak to, że samo dodanie do obrazu stałej wartości \\(b\\) nie powiększa wyniku splotu o taką samą ilość,\n\\[\n(b+I)∗H\\neq b+(I∗H),\n\\]\na więc nie jest częścią własności liniowości. Chociaż liniowość jest ważną własnością teoretyczną, należy zauważyć, że w praktyce filtry “liniowe” są często tylko częściowo liniowe z powodu błędów zaokrąglenia lub ograniczonego zakresu wartości wyjściowych.\n\n6.1.4.3 Łączność\nKonwolucja liniowa jest łączna, co oznacza, że kolejność operacji na filtrze nie ma znaczenia, czyli,\n\\[\n(I∗H_1)∗H_2 =I∗(H_1 ∗H_2).\n\\tag{6.11}\\]\nTak więc wiele filtrów może być zastosowanych w dowolnej kolejności, jak również wiele filtrów może być dowolnie łączonych w nowe filtry.\nBezpośrednią konsekwencją łączności jest rozdzielność filtrów liniowych. Jeżeli jądro konwolucyjne \\(H\\) można wyrazić jako złożenie wielu jąder \\(H_i\\) w postaci\n\\[\nH = H1 ∗ H2 ∗ . . . ∗ Hn,\n\\tag{6.12}\\]\nwówczas (jako konsekwencja Równanie 6.11) operacja filtru \\(I ∗ H\\) może być wykonana jako ciąg konwolucji z jądrami składowymi \\(H_i,\\)\n\\[\nI ∗ H = I ∗ (H_1 ∗ H_2 ∗ . . . ∗ H_n)\n= (\\ldots((I ∗H_1)∗H_2)∗\\ldots∗H_n).\n\\tag{6.13}\\]\nW zależności od rodzaju dekompozycji może to przynieść znaczne oszczędności obliczeniowe. O rozdzielczości filtrów możemy myśleć również nieco inaczej\n\\[\nI'=(I*h_x)*h_y,\n\\] gdzie \\(h_x, h_y\\) są filtrami 1D, które po wymnożeniu tworzą filtr o wymiarze \\(k\\times m\\). Przykładowo\n\\[\n\\begin{bmatrix}\n  1,&1,&1,&1,&1\\\\\n  1,&1,&1,&1,&1\\\\\n  1,&1,&1,&1,&1\n\\end{bmatrix}\n= H = h_x*h_y=\n\\begin{bmatrix}\n  1\\\\\n  1\\\\\n  1\n\\end{bmatrix}\n\\cdot\n    \\begin{bmatrix}\n        1,&1,&1,&1,&1\n       \\end{bmatrix}\n\\tag{6.14}\\]\nStosując tą własność rozłączności możemy również przedstawić filtr o jądrze gassuowskim 2D, za pomocą mnożenia filtrów gaussowskich 1D.\n\\[\nH^{G,\\sigma}=h^{G,\\sigma}_x*h^{G,\\sigma}_y,\n\\tag{6.15}\\]\ngdzie \\(h^{G,\\sigma}_x,h^{G,\\sigma}_y\\) są filtrami gaussowskimi 1D. Kolejność użytych filtrów ponownie nie ma znaczenia. Jeśli dla poszczególnych składowych odchylenia standardowe nie są równe, to filtr gaussowski 2D ma charakter eliptyczny.\nW algebrze filtrów liniowych istniej coś na kształt elementu neutralnego dla operacji splotu\n\\[\nI = \\delta*I,\n\\tag{6.16}\\]\ngdzie\n\\[\n\\delta(u,v)=\n\\begin{cases}\n  1, &\\text{ jeśli }u=v=0\\\\\n  0, &\\text{ w przeciwnym przypadku}\n\\end{cases}\n\\tag{6.17}\\]\nCo ciekawe filtr ten nie zmienia jedynie samego obrazu ale również innych filtrów\n\\[\nH=\\delta*H=H*\\delta.\n\\tag{6.18}\\]"
  },
  {
    "objectID": "filters.html#filtry-nieliniowe",
    "href": "filters.html#filtry-nieliniowe",
    "title": "\n6  Filtry\n",
    "section": "\n6.2 Filtry nieliniowe",
    "text": "6.2 Filtry nieliniowe\n\n6.2.1 Filtry minimum i maksimum\nJak wszystkie inne filtry, filtry nieliniowe obliczają wynik w danej pozycji obrazu \\((u,v)\\) z pikseli znajdujących się wewnątrz ruchomego regionu \\(R_{u,v}\\) oryginalnego obrazu. Filtry te nazywane są “nieliniowymi”, ponieważ wartości pikseli źródłowych są łączone przez jakąś funkcję nieliniową. Najprostszymi ze wszystkich filtrów nieliniowych są filtry minimum i maksimum, zdefiniowane jako\n\\[\n\\begin{align}\n  I'(u,v)=&\\min_{(i,j)\\in R}\\{I(u+i, v+j)\\}\\\\\n  I'(u,v)=&\\max_{(i,j)\\in R}\\{I(u+i, v+j)\\}\n\\end{align}\n\\tag{6.19}\\]\ngdzie \\(R\\) oznacza region filtra (zbiór współrzędnych filtra, zwykle kwadrat o rozmiarach 3x3 pikseli). Rysunek 6.10 ilustruje wpływ minimalnego filtra 1D na różne lokalne struktury sygnału.\n\n\nRysunek 6.9: Zastosowanie filtru minimum 1D do różnych sygnałów wejściowych. Górny rząd przedstawia oryginalny sygnał, a dolny spleciony z filtrem minimum\n\n\n\nKodkwiat <- load.image(file = \"~/kwiat.jpg\") |> grayscale()\n\nmin_filter <- function(im, radius) {\n  stencil <- expand.grid(dx = -radius:radius, dy = -radius:radius)\n  filtered <- matrix(0, nrow = height(im), ncol = width(im))\n  range_x <- (1 + radius):(width(im) - radius)\n  range_y <- (1 + radius):(height(im) - radius)\n  dt <- expand.grid(range_x, range_y)\n  dt$min <- apply(dt, 1, function(row) min(get.stencil(im, \n                                                       stencil, \n                                                       x = row[1],\n                                                       y = row[2])))\n\n  filtered <- dt$min |> \n    matrix(ncol = height(im)-2*radius,\n           nrow = width(im)-2*radius) |> \n    as.cimg()\n    \n  return(filtered)\n}\n\nkwiat_filtered <- min_filter(kwiat, radius = 2)\n\nlayout(t(1:2))\nkwiat |> plot(main = 'oryginał')\nkwiat_filtered |> plot(main = \"filtr minimum\")\n\n\n\nRysunek 6.10: Zastosowanie filtru minimum\n\n\n\n\nW przypadku obrazów z wtrąceniami w kolorze białym lub czarnym efekt występowania tych wtrąceń jest potęgowany (patrz Rysunek 6.11). Filtr minimum wyciąga kolor czarny, natomiast filtr maksimum kolor biały.\n\nKodkwiat_noisy <- load.image(file = \"~/kwiat_salt_pepper.jpg\") |> grayscale()\n\nkwiat_noisy |> plot()\nkwiat_noisy |> \n  erode_square(size = 5) |> \n  plot()\nkwiat_noisy |> \n  dilate_square(size = 5) |> \n  plot()\n\n\n\n\n\n(a) Oryginał\n\n\n\n\n\n\n(b) Filtr minimum\n\n\n\n\n\n\n(c) Filtr maximum\n\n\n\n\nRysunek 6.11: Porównanie trzech obrazów\n\n\n\n\n6.2.2 Filtr medianowy\nNie da się oczywiście zaprojektować filtra, który usunie każdy szum i zachowa wszystkie ważne struktury obrazu, ponieważ żaden filtr nie jest w stanie rozróżnić, która zawartość obrazu jest ważna dla widza, a która nie. Popularny filtr medianowy jest z pewnością dobrym krokiem w tym kierunku.\nFiltr medianowy zastępuje każdy piksel obrazu medianą pikseli w bieżącym regionie filtra \\(R\\), czyli\n\\[\nI'(u,v)=\\operatorname{Median}_{(i,j)\\in R}\\{I(u+i, v+j)\\}.\n\\tag{6.20}\\]\n\n\nRysunek 6.12: Zasada działania filtra medianowego\n\n\nRównanie (Równanie 6.20) definiuje medianę zbioru wartości o nieparzystej liczebności. Jeśli długość boku filtrów prostokątnych jest nieparzysta (co zwykle ma miejsce), to liczba elementów w regionie filtrów również jest nieparzysta. W takim przypadku filtr medianowy nie tworzy żadnych nowych wartości pikseli. Jeśli jednak liczba elementów jest parzysta, to mediana posortowanego ciągu \\(A = (a_0,\\ldots,a_{2n-1})\\) jest definiowana jako średnia arytmetyczna dwóch sąsiednich wartości środkowych \\(a_{n-1}\\) i \\(a_n\\). W ten sposób mogą być wprowadzone nowe wartości do obrazu.\n\nKodbox_flt <- kwiat_noisy |> \n  boxblur(boxsize = 3)\nmedian_flt <- kwiat_noisy |> \n  medianblur(n = 3)\nlist(kwiat_noisy, box_flt, median_flt) |> \n  imappend(\"x\") |> \n  plot()\n\n\n\nRysunek 6.13: Porównanie metod usuwania szumu. Na obrazie po lewej stronie naniesiony jest szum (znany jako salt and papper). Na środkowym szum jest usuwany filtrem pudełkowym (ang. box filter). Na obrazie po prawej szum jest usuwany filtrem medianowym.\n\n\n\n\n\nKodbox_flt_big <- imsub(box_flt, x %inr% c(200, 450), y %inr% c(300, 550)) \nmedian_flt_big <- imsub(median_flt, x %inr% c(200, 450), y %inr% c(300, 550)) \nimappend(list(box_flt_big, median_flt_big), \"x\") |> \n  plot()\n\n\n\nRysunek 6.14: W powiększeniu ten efekt jest jeszcze lepiej widoczny\n\n\n\n\nMediana jest statystyką porządkową i w pewnym sensie “większość” uwzględnianych wartości pikseli określa wynik. Pojedyncza wyjątkowo wysoka lub niska wartość (“odstająca”) nie może wpłynąć na wynik, a jedynie przesunąć go w górę lub w dół do następnej wartości. Dlatego mediana (w przeciwieństwie do średniej) jest uważana za miarę “odporną”. W zwykłym filtrze medianowym każdy piksel w regionie filtra ma taki sam wpływ, niezależnie od jego odległości od środka."
  },
  {
    "objectID": "filters.html#obramowania-obrazów",
    "href": "filters.html#obramowania-obrazów",
    "title": "\n6  Filtry\n",
    "section": "\n6.3 Obramowania obrazów",
    "text": "6.3 Obramowania obrazów\nJak to zostało zaznaczone wcześniej zastosowanie jakiegokolwiek filtru (liniowego lub nieliniowego) wiąże się z pewna niedogodnością. Mianowicie wszystkie filtry kwadratowe czy prostokątne są kłopotliwe w zastosowaniu jeśli centrum (hot spot) filtra leży blisko brzegu obrazu. Teoretycznie filtry nie mogą być stosowane w miejscach, gdzie macierz filtrów nie jest w pełni zawarta w macierzy obrazu. Zatem każda operacja filtrująca zmniejszyłaby rozmiar obrazu wynikowego, co w większości zastosowań jest nie do przyjęcia. Choć nie istnieje formalnie poprawne remedium, istnieje kilka mniej lub bardziej praktycznych metod obsługi regionów granicznych:\n\nUstaw nieprzetworzone piksele na granicach na jakąś stałą wartość (np. “czarny”). Jest to z pewnością najprostsza metoda, ale w wielu sytuacjach nie do przyjęcia, ponieważ rozmiar obrazu jest stopniowo zmniejszany przez każdą operację filtra.\nUstaw nieprzetworzone piksele na oryginalne (niefiltrowane) wartości obrazu. Zazwyczaj wyniki są również nie do przyjęcia, ze względu na zauważalną różnicę między przefiltrowanymi i nieprzetworzonymi fragmentami obrazu.\nRozwiń obraz, “wypełniając” (ang. padding) dodatkowe piksele wokół niego i zastosuj filtr również do regionów granicznych. Na Rysunek 6.15 pokazano różne opcje wypełniania obrazów (obraz (a) jest oryginałem).\n\nPiksele poza obrazem mają stałą wartość (np. “czarny” lub “szary”, patrz Rysunek 6.15 (b)). Może to powodować silne artefakty na granicach obrazu, szczególnie w przypadku stosowania dużych filtrów.\nPiksele graniczne wykraczają poza granice obrazu (Rysunek 6.15 (c)). W miejscach występowania granic można spodziewać się jedynie niewielkich artefaktów. Metoda ta jest również prosta obliczeniowo i dlatego często jest uważana za najlepszy wybór.\nObraz jest odbijany na każdej ze swoich czterech granic (Rysunek 6.15 (d)). Wyniki będą podobne jak w przypadku poprzedniej metody, o ile nie zostaną użyte bardzo duże filtry.\nObraz powtarza się cyklicznie w kierunku poziomym i pionowym (Rysunek 6.15 (e)). Może się to początkowo wydawać dziwne, a wyniki na ogół nie są zadowalające. Jednak w dyskretnej analizie spektralnej obraz jest pośrednio traktowany również jako funkcja okresowa. Jeśli więc obraz jest filtrowany w dziedzinie częstotliwości, to wyniki będą równe filtracji w dziedzinie przestrzeni w ramach tego powtarzalnego modelu.\n\n\n\n\n\nRysunek 6.15: Różne sposoby obsługi obramowania\n\n\nŻadna z tych metod nie jest doskonała i zwykle, właściwy wybór zależy od rodzaju obrazu i zastosowanego filtra."
  },
  {
    "objectID": "edge.html#wykrywanie-krawędzi-na-podstawie-gradientu",
    "href": "edge.html#wykrywanie-krawędzi-na-podstawie-gradientu",
    "title": "\n7  Wykrywanie krawędzi i konturów\n",
    "section": "\n7.1 Wykrywanie krawędzi na podstawie gradientu",
    "text": "7.1 Wykrywanie krawędzi na podstawie gradientu\nDla uproszczenia, najpierw zbadamy sytuację tylko w jednym wymiarze, zakładając, że obraz zawiera pojedynczy jasny obszar w centrum otoczony ciemnym tłem (Rysunek 7.2 (a)). W tym przypadku profil natężenia wzdłuż jednej linii obrazu wyglądałby jak funkcja 1D \\(f(x)\\), jak pokazano na Rysunek 7.2 (b). Biorąc pierwszą pochodną funkcji \\(f\\)\n\\[\nf'(x)=\\frac{df}{dx}(x),\n\\tag{7.1}\\]\npowoduje dodatnie wahnięcie w tych miejscach, gdzie intensywność wzrasta i ujemne wahnięcie tam, gdzie wartość funkcji spada (Rysunek 7.2 (c)).\n\n\nRysunek 7.2: Przykładowy obraz z wyraźną zmianą jasności\n\n\nW przeciwieństwie do przypadku ciągłego, pierwsza pochodna jest nieokreślona dla funkcji dyskretnej \\(f(u)\\) i potrzebna jest jakaś metoda, by ją oszacować. Rysunek 7.3 pokazuje podstawową ideę, ponownie dla przypadku 1D: pierwsza pochodna funkcji ciągłej w pozycji \\(x\\) może być interpretowana jako nachylenie jej stycznej w tej pozycji. Jedną z prostych metod przybliżonego określenia nachylenia stycznej dla funkcji dyskretnej \\(f(u)\\) jest dopasowanie linii prostej przechodzącej przez wartości funkcji \\(f(u-1)\\) i \\(f(u+1)\\) w sąsiedztwie \\(u\\)\n\\[\n\\frac{df}{dx}(u)\\approx\\frac{f(u+1)-f(u-1)}{2}.\n\\tag{7.2}\\]\nOczywiście tę samą metodę można zastosować w kierunku pionowym, aby oszacować pierwszą pochodną wzdłuż osi y, czyli wzdłuż kolumn obrazu.\n\n\nRysunek 7.3: Oszacowanie gradientu dla funkcji dyskretnej\n\n\nW przypadku funkcji wielowymiarowej obliczamy pochodne cząstkowe \\(I_u = \\frac{\\partial I(u,v)}{\\partial u}\\) i \\(I_v = \\frac{\\partial I(u,v)}{\\partial v}\\) funkcji obrazu 2D \\(I(u, v)\\) odpowiednio wzdłuż osi \\(u\\) i \\(v\\). Wektor\n\\[\n\\nabla I(u,v)= \\begin{bmatrix}\n  I_u(u,v)\\\\\n  I_v(u,v)\n\\end{bmatrix}\n\\tag{7.3}\\]\nnazywamy gradientem funkcji \\(I\\) w punkcie \\((u, v)\\). Długość gradientu\n\\[\n\\vert \\nabla I\\vert = \\sqrt{I_u^2+I_v^2}\n\\tag{7.4}\\]\njest niezmienna ze względu na obroty obrazu, a więc niezależna od orientacji leżących u jego podstaw struktur. Własność ta jest istotna dla izotropowej1 lokalizacji krawędzi, a zatem \\(\\vert\\nabla I\\vert\\) jest podstawą wielu praktycznych metod detekcji krawędzi.1 we wszystkich kierunkach wykazują one te same właściwości fizyczne\nSkładowe funkcji gradientu (Równanie 7.3) są po prostu pierwszymi pochodnymi wierszy i kolumn obrazu odpowiednio wzdłuż osi poziomej i pionowej. Aproksymacja pierwszych pochodnych poziomych (Równanie 7.2) może być łatwo zrealizowana przez filtr liniowy z jądrem 1D\n\\[\nH_u^D = [-0.5,0,0.5]\n\\tag{7.5}\\]\ngdzie współczynniki -0.5 i 0.5 stosujemy do elementów obrazu \\(I(u-1, v)\\) i \\(I(u+1, v)\\) odpowiednio. Zauważmy, że sam piksel środkowy \\(I(u, v)\\) jest z wagą zerową, a więc jest ignorowany. Analogicznie, pionowa składowa gradientu jest uzyskiwana za pomocą filtru liniowego\n\\[\nH^D_v= \\begin{bmatrix}\n  -0.5\\\\\n  0\\\\\n  0.5\n\\end{bmatrix}\n\\tag{7.6}\\]\nRysunek 6.4 przedstawia wynik zastosowania filtrów gradientowych zdefiniowanych w Równanie 7.5 i Równanie 7.6 do syntetycznego obrazu testowego. Widać wyraźnie zależność odpowiedzi filtrów od orientacji. Filtr gradientu poziomego \\(H_u^D\\) reaguje najsilniej na szybkie zmiany wzdłuż kierunku poziomego, (czyli na krawędzie pionowe); analogicznie filtr gradientu pionowego \\(H_v^D\\) reaguje najsilniej na krawędzie poziome. W płaskich obszarach obrazu (przedstawionych jako szare na Rysunek 7.4 (b, c)) reakcja filtra jest zerowa.\n\n\nRysunek 7.4: Zastosowanie metody gradientowej do obrazu 2D\n\n\nLokalne gradienty funkcji obrazu są podstawą wielu klasycznych operatorów detekcji krawędzi. Praktycznie różnią się one jedynie rodzajem filtra stosowanego do estymacji składowych gradientu oraz sposobem łączenia tych składowych. W wielu sytuacjach człowiek jest zainteresowany nie tylko natężeniem punktów krawędziowych, ale także lokalnym kierunkiem krawędzi. Oba rodzaje informacji zawarte są w funkcji gradientu i mogą być łatwo obliczone ze składowych kierunkowych. Poniższy niewielki zbiór opisuje kilka często używanych, prostych operatorów krawędziowych, które istnieją od wielu lat, a więc są interesujące również z historycznego punktu widzenia.\n\n7.1.1 Przegląd filtrów do detekcji krawędzi\n\n7.1.1.1 Filtry Prewitta i Sobela\nOperatory krawędziowe autorstwa Prewitta („Picture Processing and Psychopictorics - 1st Edition”, b.d.) i Sobela (Davis 1975) to dwie klasyczne metody, które różnią się tylko nieznacznie stosowanymi przez nie filtrami pochodnymi.\nOba operatory wykorzystują filtry liniowe, które rozciągają się odpowiednio na trzy sąsiednie wiersze i kolumny, aby przeciwdziałać wrażliwości na szumy prostych (pojedynczy wiersz/kolumna) operatorów gradientowych. Operator Prewitta wykorzystuje jądra postaci\n\\[\nH^P_u= \\begin{bmatrix}\n  -1& 0& 1\\\\\n  -1& 0& 1\\\\\n  -1& 0& 1\n\\end{bmatrix}\n\\quad\\text{i}\\quad\nH^P_v=\\begin{bmatrix}\n  -1&-1&-1\\\\\n  0&0&0\\\\\n  1&1&1\n\\end{bmatrix}\n\\tag{7.7}\\]\nktóre obliczają średnie gradientu odpowiednio w trzech sąsiednich wierszach lub kolumnach. Podczas gdy filtry Sobela definiuje się w postaci\n\\[\nH^S_u= \\begin{bmatrix}\n  -1&0&1\\\\\n  -2&0&2\\\\\n  -1&0&1\n\\end{bmatrix}\n\\quad \\text{i}\\quad\nH^S_v=\\begin{bmatrix}\n  -1&-2&-1\\\\\n  0&0&0\\\\\n  1&2&1\n\\end{bmatrix}\n\\tag{7.8}\\]\nczyli jak widać są podobne do filtrów Prewitta z tą różnicą, że część wygładzająca przypisuje większą wagę do aktualnej linii środkowej i kolumny odpowiednio. W przypadku obu filtrów długość wektora gradientu krawędzi jest określona przez\n\\[\nE(u,v)=\\sqrt{I_u^2(u,v)+I_v^2(u,v)},\n\\tag{7.9}\\]\ngdzie \\(I_u\\) i \\(I_v\\) powstały przez mnożenie \\(I*H\\) (\\(H\\) jest filtrem Prewitta lub Sobela). Natomiast lokalny kąt orientacji krawędzi jest równy\n\\[\n\\Phi(u,v)=\\tan^{-1}\\left(\\frac{I_u(u,v)}{I_v(u,v)}\\right)\n\\tag{7.10}\\]\n\n\nRysunek 7.5: Ilustracja dla długości wektora gradientu i orientacji krawędzi\n\n\nCały proces ekstrakcji wielkości i orientacji krawędzi jest podsumowany na Rysunek 7.6. Najpierw oryginalny obraz \\(I\\) jest niezależnie splatany z dwoma filtrami gradientowymi \\(H_u\\) i \\(H_v\\), a następnie z filtrów obliczana jest długość wektora gradientu krawędzi \\(E\\) i orientacja \\(\\Phi\\).\n\n\nRysunek 7.6: Przykładowy proces ekstrakcji krawędzi\n\n\n\n7.1.1.2 Filtry Roberts’a\nJako jeden z najprostszych i najstarszych detektorów krawędzi, operator Robertsa („Optical and Electro-Optical Information Processing”, b.d.) ma dziś głównie znaczenie historyczne. Wykorzystuje on dwa niezwykle małe filtry o rozmiarach 2 × 2 do estymacji gradientu kierunkowego wzdłuż przekątnych obrazu. Zdefiniowane są one za pomocą jądra\n\\[\nH^R_1= \\begin{bmatrix}\n  0&1\\\\\n  -1&0\n\\end{bmatrix}\\quad\\text{i}\\quad\nH^R_2= \\begin{bmatrix}\n  -1&0\\\\\n  0&1\n\\end{bmatrix}\n\\tag{7.11}\\]\nProjektowanie liniowych filtrów krawędziowych wiąże się z pewnym kompromisem: im silniej filtr reaguje na struktury podobne do krawędzi, tym bardziej wrażliwy jest na orientację. Innymi słowy, filtry niewrażliwe na orientację mają tendencję do reagowania na struktury nie będące krawędziami, podczas gdy najbardziej dyskryminujące filtry krawędziowe reagują tylko na krawędzie w wąskim zakresie orientacji. Jednym z rozwiązań jest zastosowanie nie tylko pojedynczej pary stosunkowo “szerokich” filtrów dla dwóch kierunków (takich jak Prewitta i Sobela), ale większego zestawu filtrów o ciasno rozłożonych orientacjach.\n\n7.1.1.3 Rozszerzony filtr Sobela\nTym razem zastosujemy 8 filtrów zmieniając orientację o \\(45\\degree\\).\n\n\n\n\n\n7.1.1.4 Filtr Kirscha\nInnym klasycznym operatorem kompasu jest operator zaproponowany przez Kirscha (Kirsch 1971), który również wykorzystuje osiem zorientowanych filtrów o następujących kernelach\n\n\n\n\nJednym z problemów z operatorami do wykrywania krawędzi opartymi na pierwszych pochodnych jest to, że każda wynikowa krawędź jest tak szeroka jak przedział zmiany intensywności, a zatem krawędzie mogą być trudne do precyzyjnego zlokalizowania. Alternatywna klasa operatorów krawędzi wykorzystuje drugie pochodne funkcji obrazu.\n\n\nRysunek 7.7: Zastosowanie drugiej pochodnej w detekcji krawędzi\n\n\nDruga pochodna funkcji mierzy jej lokalną krzywiznę. Idea jest taka, że krawędzie można znaleźć w miejscach zerowych lub - jeszcze lepiej - w miejscach zerowych drugich pochodnych funkcji obrazu, jak pokazano na Rysunek 7.7 dla przypadku 1D. Ponieważ drugie pochodne mają tendencję do wzmacniania szumu obrazu, zwykle stosuje się pewien rodzaj wygładzania wstępnego za pomocą odpowiednich filtrów dolnoprzepustowych.\nPopularnym przykładem jest operator “Laplacian-of-Gaussian” (LoG) (Marr i Hildreth 1980), który łączy wygładzanie gussowskie i obliczanie drugich pochodnych w jeden filtr liniowy. Przykład na Rysunek 7.8 pokazuje, że krawędzie uzyskane za pomocą operatora LoG są dokładniej zlokalizowane niż te dostarczone przez operatory Prewitta i Sobela.\nNiestety, wyniki działania prostych operatorów krawędziowych, o których mówiliśmy do tej pory, często odbiegają od tego, co my jako ludzie postrzegamy jako ważne krawędzie. Dwie główne przyczyny tego stanu rzeczy to:\n\nPo pierwsze, operatory krawędziowe reagują jedynie na lokalne różnice intensywności, podczas gdy nasz system wzrokowy jest w stanie rozpoznać krawędzie na obszarach o minimalnym lub zanikającym kontraście.\nPo drugie, krawędzie istnieją nie w jednej stałej rozdzielczości czy w pewnej skali, ale w całym szeregu różnych skal.\n\nTypowe małe operatory krawędzi, takie jak operator Sobela, mogą reagować tylko na różnice intensywności, które występują w obrębie ich regionów filtrów 3x3 piksele. Aby rozpoznać wtrącenia podobne do krawędzi w większym zakresie, potrzebowalibyśmy albo większych operatorów krawędzi (z odpowiednio dużymi filtrami), albo użyć oryginalnych (małych) operatorów na zredukowanych (tj. przeskalowanych) obrazach. Jest to główna idea technik “multiresolution” (zwanych również “hierarchicznymi” lub “piramidowymi”), które tradycyjnie są wykorzystywane w wielu zastosowaniach przetwarzania obrazów. W kontekście wykrywania krawędzi, sprowadza się to zazwyczaj do wykrywania najpierw krawędzi na różnych poziomach skali, a następnie decydowania, która krawędź (jeśli w ogóle) na danym poziomie skali jest dominująca w każdej pozycji obrazu.\nW wielu sytuacjach, kolejnym krokiem po wzmocnieniu krawędzi (przez jakiś operator krawędziowy) jest wybór punktów krawędziowych, czyli binarna decyzja o tym, czy piksel obrazu jest punktem krawędziowym czy nie. Najprostszą metodą jest zastosowanie operacji progowania do długości wektora gradientu krawędzi dostarczonej przez operator krawędzi, przy użyciu stałej lub adaptacyjnej wartości progowej, co daje binarny obraz krawędzi lub “mapę krawędzi”.\nW praktyce mapy krawędzi rzadko zawierają idealne kontury, ale zamiast tego wiele małych, niepołączonych fragmentów konturów, przerwanych w miejscach o niewystarczającej sile krawędzi2. Po progowaniu puste miejsca nie zawierają oczywiście żadnej informacji o krawędziach, która mogłaby zostać wykorzystana w kolejnym kroku, np. do łączenia sąsiadujących segmentów krawędzi. Pomimo tej słabości, globalne progowanie jest często stosowane w tym momencie ze względu na swoją prostotę.2 wyrażonej długością wektora gradientu krawędzi\nIdea sekwencyjnego śledzenia konturów wzdłuż odkrytych punktów krawędziowych nie jest rzadkością i wydaje się dość prosta w założeniu. Rozpoczynając od punktu obrazu o dużej sile krawędzi, podąża się iteracyjnie w obu kierunkach, aż do momentu, gdy oba ślady spotkają się i powstanie zamknięty kontur. Niestety, istnieje kilka przeszkód, które sprawiają, że zadanie to jest trudniejsze niż się początkowo wydaje, m.in:\n\nkrawędzie mogą kończyć się w regionach zanikającego gradientu intensywności,\nprzecinające się krawędzie prowadzą do niejednoznaczności,\nkontury mogą się rozgałęziać w kilku kierunkach.\n\nZe względu na te problemy, śledzenie konturów zazwyczaj nie jest stosowane do obrazów oryginalnych lub obrazów o ciągłej wartości krawędzi, z wyjątkiem bardzo prostych sytuacji, takich jak wyraźne oddzielenie obiektów (pierwszego planu) od tła. Śledzenie konturów w segmentowanych obrazach binarnych jest oczywiście znacznie prostsze.\n\n7.1.1.5 Filtry Canny’ego\nOperator zaproponowany przez Canny’ego jest szeroko stosowany i nadal uważany za “state of the art”3 w detekcji krawędzi. Metoda ta stara się osiągnąć trzy główne cele:3 najwyższe osiągnięcie w danej dziedzinie\n\nzminimalizować liczbę fałszywych punktów krawędziowych,\nosiągnąć dobrą lokalizację krawędzi\ndostarczyć tylko pojedynczy znak na każdej krawędzi.\n\nWłaściwości te nie są zwykle osiągane przez proste operatory krawędziowe (najczęściej oparte na pierwszych pochodnych i późniejszym progowaniu). W swej istocie filtr Canny’ego jest metodą gradientową (opartą na pierwszych pochodnych), ale do precyzyjnej lokalizacji krawędzi wykorzystuje miejsca zerowe drugich pochodnych. Pod tym względem metoda ta jest podobna do detektorów krawędzi, które bazują na drugich pochodnych funkcji obrazu (Canny 1986).\n\n\nRysunek 7.8: Przykłady zastosowania różnych detektorów krawędzi\n\n\nW pełni zaimplementowany detektor Canny’ego wykorzystuje zestaw stosunkowo dużych, zorientowanych filtrów przy wielu rozdzielczościach obrazu i łączy poszczególne wyniki we wspólną mapę krawędzi. Często jednak stosuje się tylko jednoskalową implementację algorytmu z regulowanym promieniem filtra (parametr wygładzania \\(\\sigma\\)), która jednak przewyższa większość prostych operatorów krawędziowych. Ponadto algorytm ten daje nie tylko binarną mapę krawędzi, ale także połączone łańcuchy pikseli krawędziowych, co znacznie upraszcza kolejne etapy przetwarzania. Dlatego nawet w swojej podstawowej (jednoskalowej) postaci operator Canny’ego jest często preferowany w stosunku do innych metod detekcji krawędzi. W swojej podstawowej (jednoskalowej) postaci operator Canny’ego wykonuje następujące kroki:\n\nObróbka wstępna - Oryginalny obraz intensywności \\(I\\) jest najpierw wygładzany jądrem filtra gaussowskiego \\(H^{G,\\sigma}\\); jego szerokość \\(\\sigma\\) określa skalę przestrzenną, w której mają być wykrywane krawędzie. Następnie na wygładzony obraz \\(\\bar{I}\\) nakładane są filtry różnicowe pierwszego rzędu w celu obliczenia składowych \\(\\bar{I}_u,\\bar{I}_v\\) lokalnych wektorów gradientu. Następnie obliczana jest lokalna wielkość \\(E_{mag}\\) jako norma odpowiadającego wektora gradientu. Ze względu na późniejsze progowanie pomocne może być znormalizowanie wartości siły krawędzi do standardowego zakresu (np. do [0, 100]).\nLokalizacja krawędzi - Kandydackie piksele krawędziowe są izolowane poprzez lokalne “non-maximum suppression” siły krawędzi \\(E_{mag}\\). W tym kroku zachowywane są tylko te piksele, które reprezentują lokalne maksimum wzdłuż profilu 1D w kierunku gradientu, czyli prostopadle do stycznej krawędzi (patrz Rysunek 7.9). Chociaż gradient może być skierowany w dowolnym kierunku, to dla ułatwienia efektywnego przetwarzania stosuje się zwykle tylko cztery dyskretne kierunki. Piksel w pozycji \\((u,v)\\) jest traktowany jako kandydat na krawędź tylko wtedy, gdy wielkość jego gradientu jest większa niż obu jego bezpośrednich sąsiadów w kierunku określonym przez wektor gradientu \\((dx,dy)\\) w pozycji \\((u,v)\\). Jeśli piksel nie jest lokalnym maksimum, jego wartość siły krawędzi jest ustawiona na zero (tj. “stłumiona”).\nŚledzenie krawędzi i progowanie z histerezą - W ostatnim kroku zbiory połączonych punktów brzegowych są zbierane z wartości, które pozostały nietłumione w poprzedniej operacji. Dokonuje się tego za pomocą techniki zwanej “progowaniem z histerezą” z wykorzystaniem dwóch różnych wartości progowych , \\(t_{lo}\\) (przy czym \\(t_{hi} > t_{lo}\\)). Obraz jest skanowany w poszukiwaniu pikseli o wielkości krawędzi \\(E_{nms}(u, v) \\geq t_{hi}\\). W każdym przypadku znalezienia takiego (wcześniej nie odwiedzanego) miejsca, uruchamiany jest nowy ślad krawędziowy i dodawane są do niego wszystkie połączone piksele krawędziowe \\((u′,v′)\\), dopóki \\(E_nms(u′,v′) \\geq t_{lo}\\). Pozostają tylko te ślady krawędzi, które zawierają co najmniej jeden piksel o wielkości krawędzi większej niż \\(t_{hi}\\) i żadnych pikseli o wielkości krawędzi mniejszej niż \\(t_{lo}\\). Typowe wartości progów dla 8-bitowych obrazów w skali szarości to \\(t_{hi} = 5.0\\) i \\(t_{lo} = 2.5\\).\n\n\n\nRysunek 7.9: Zasada działania filtru Canny’ego"
  },
  {
    "objectID": "edge.html#wyostrzanie-obrazu",
    "href": "edge.html#wyostrzanie-obrazu",
    "title": "\n7  Wykrywanie krawędzi i konturów\n",
    "section": "\n7.2 Wyostrzanie obrazu",
    "text": "7.2 Wyostrzanie obrazu\nWyostrzanie obrazów (ang. sharpening) jest częstym zadaniem, np. w celu poprawy ostrości po zeskanowaniu lub przeskalowaniu obrazu lub w celu wstępnej kompensacji późniejszej utraty ostrości w trakcie drukowania lub wyświetlania obrazu. Powszechnym podejściem do wyostrzania obrazu jest wzmacnianie wysokoczęstotliwościowych (ang. hight-frequency amplifying) składowych obrazu, które są głównie odpowiedzialne za postrzeganą ostrość obrazu i dla których najsilniejsze występują przy szybkich zmianach intensywności. W dalszej części opisujemy dwie metody sztucznego wyostrzania obrazu, które opierają się na technikach podobnych do detekcji krawędzi.\n\n7.2.1 Filtr Laplace’a\n\n\nRysunek 7.10: Wyostrzanie obrazu za pomocą drugich pochodnych\n\n\nPopularną metodą lokalizacji szybkich zmian intensywności są filtry oparte na drugich pochodnych funkcji obrazu. Rysunek 7.10 ilustruje tę ideę dla obrazu 1D i ciągłej funkcji \\(f(x)\\). Druga pochodna \\(f''(x)\\) funkcji schodkowej pokazuje dodatni impuls na dolnym końcu przejścia i ujemny impuls na górnym końcu. Krawędź wyostrza się przez odjęcie pewnego ułamka w drugiej pochodnej \\(f''(x)\\) od oryginalnej funkcji \\(f(x)\\)\n\\[\n\\hat{f}(x)=f(x)-w\\cdot f''(x).\n\\tag{7.12}\\]\nW zależności od współczynnika wagowego \\(w\\geq0\\), wyrażenie w Równanie 7.12 powoduje, że funkcja intensywności przeskakuje po obu stronach krawędzi, co powoduje wyolbrzymienie krawędzi i zwiększenie postrzeganej ostrości.\nWyostrzenie funkcji 2D można uzyskać za pomocą drugich pochodnych w kierunku poziomym i pionowym połączonych tzw. operatorem Laplace’a. Operator Laplace’a \\(\\nabla^2\\) funkcji 2D \\(f(x, y)\\) jest zdefiniowany jako suma drugich pochodnych cząstkowych wzdłuż kierunków \\(x\\) i \\(y\\):\n\\[\n(\\nabla^2f)(x,y)=\\frac{\\partial^2 f}{\\partial^2 x}(x,y)+\\frac{\\partial^2 f}{\\partial^2 y}(x,y).\n\\tag{7.13}\\]\nPodobnie jak w przypadku pierwszych pochodnych, również drugie pochodne funkcji obrazu dyskretnego mogą być estymowane za pomocą zestawu prostych filtrów liniowych. Również w tym przypadku zaproponowano kilka wersji. Na przykład, dwa filtry 1D:\n\\[\n\\frac{\\partial^2 f}{\\partial^2 x}\\approx H_x^L= \\begin{bmatrix}\n  1 &-2 &1\n\\end{bmatrix}\\quad\\text{i}\\quad \\frac{\\partial^2 f}{\\partial^2 y}\\approx H_y^L=\n\\begin{bmatrix}\n  1\\\\\n  -2\\\\\n  1\n\\end{bmatrix}.\n\\tag{7.14}\\]\ndo szacowania drugich pochodnych odpowiednio wzdłuż kierunku \\(x\\) i \\(y\\), łączą się w filtr Laplace’a 2D:\n\\[\nH^L= \\begin{bmatrix}\n  0&1&0\\\\\n  1&-4&1\\\\\n  0&1&0\n\\end{bmatrix}.\n\\tag{7.15}\\]\nWyostrzanie odbywa się poprzez odjęcie przekształconego obrazu przez filtr Laplace’a od oryginalnego z odpowiednią wagą \\(w\\)\n\\[\nI'\\leftarrow I-w\\cdot (H^L*I).\n\\tag{7.16}\\]\nRysunek 7.11 przedstawia przykład zastosowania filtru Laplace’a \\(H^L\\) do obrazu w skali szarości, gdzie wyraźnie widoczne są pary dodatnio i ujemnych szczytów po obu stronach każdej krawędzi. Filtr wydaje się niemal izotropowy pomimo grubego przybliżenia za pomocą małych filtrów.\n\n\nRysunek 7.11: Porównanie filtru Laplace’a z filtrami drugich pochodnych\n\n\n\n7.2.2 Metoda nieostrej maski\nMetoda nieostrej maski (ang. unsharp masking) (USM) to technika wyostrzania krawędzi, która jest szczególnie popularna w astronomii, druku cyfrowym i wielu innych dziedzinach przetwarzania obrazów. Termin ten wywodzi się z klasycznej fotografii, gdzie ostrość obrazu była optycznie zwiększana poprzez połączenie go z wygładzoną (“nieostrą”) kopią. Proces ten jest w zasadzie taki sam dla obrazów cyfrowych.\nPierwszym krokiem w filtrze USM jest odjęcie od oryginału wygładzonej wersji obrazu, która uwydatnia krawędzie. Wynik ten nazywany jest “maską”. W fotografii analogowej wymagane wygładzenie uzyskiwano przez zwykłe rozogniskowanie obiektywu4. Następnie maska jest ponownie dodawana do oryginału, w taki sposób, że krawędzie w obrazie są wyostrzone. Podsumowując, kroki zaangażowane w filtrowanie USM to:4 utratę ostrości\n\nObraz maski \\(M\\) jest generowany przez odjęcie od obrazu oryginalnego \\(I\\) jego wygładzonej wersji, uzyskanej przez filtrowanie za pomocą filtru \\(\\tilde{H}\\)\n\n\n\\[\nM\\leftarrow I-(I*\\tilde{H})=I-\\tilde{I}.\n\\tag{7.17}\\]\nPrzyjmuje się, że jądro \\(\\tilde{H}\\) filtra wygładzającego jest znormalizowane. 2. Aby uzyskać wyostrzony obraz \\(\\check{I}\\), maska \\(M\\) jest dodawana do oryginalnego obrazu \\(I\\) z odpowiednią wagą \\(a\\), za pomocą której kontrolujemy stopień wyostrzenia\n\\[\n\\check{I}\\leftarrow I+a\\cdot M,\n\\tag{7.18}\\] zatem podstawiając Równanie 7.17 otrzymujemy\n\\[\n\\check{I}\\leftarrow I+a\\cdot (I-\\tilde{I})=(1+a)\\cdot I-a\\cdot \\tilde{I}.\n\\] Jako filtr wygładzający \\(\\tilde{H}\\) można właściwie zastosować dowolny filtr wygładzający, choć najczęściej spotyka się w tym miejscu filtry gassowskie \\(H^{G,\\sigma}\\) o \\(\\sigma \\in [1,20]\\). Natomiast najczęściej przyjmowane wartości dla \\(a\\) mieszczą się w przedziale [0.2,4].\n\nKodlena <- Rvision::image(\"images/lena.png\")\n\nlayout(matrix(1:6, 2, 3, byrow = TRUE))\n\nplot(lena)\nlena_blr <- Rvision::gaussianBlur(lena, 10, 10, 5, 5)\nplot(lena_blr)\nmaska <- lena-lena_blr\nplot(maska)\nlena_sharp <- lena+maska\nplot(lena_sharp)\n\nlena_flt <- Rvision::filter2D(lena, matrix(c(0,1,0,\n                                    1,-4,1,\n                                    0,1,0), byrow = T, ncol = 3))\nplot(lena_flt)\n(lena-lena_flt) |>  plot()\n\n\n\nRysunek 7.12: Wyostrzenie obrazu Leny. Pierwszy obraz w górnym rzędzie to oryginał, środkowy w górnym rzędzie to rozmyty obraz Leny filtrem gaussowskim, obraz po prawej w górnym rzędzie to maska. Obraz po lewej stronie w dolnym rzędzie to wyostrzony obraz Leny metodą USM. Środkowy obraz w dolnym rzędzie to filtr Laplace’a nałożony na Lenę, a obraz po prawej w dolnym rzędzie to wyostrzony obraz Leny metodą Laplace’a\n\n\n\n\n\n\n\n\nCanny, John. 1986. „A Computational Approach to Edge Detection”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\nDavis, Larry S. 1975. „A Survey of Edge Detection Techniques”. Computer Graphics and Image Processing 4 (3): 248–70. https://doi.org/10.1016/0146-664X(75)90012-X.\n\n\nKirsch, Russell A. 1971. „Computer Determination of the Constituent Structure of Biological Images”. Computers and Biomedical Research 4 (3): 315–28. https://doi.org/10.1016/0010-4809(71)90034-6.\n\n\nMarr, D., i E. Hildreth. 1980. „Theory of Edge Detection”. Proceedings of the Royal Society of London. Series B, Biological Sciences 207 (1167): 187–217. https://doi.org/10.1098/rspb.1980.0020.\n\n\n„Optical and Electro-Optical Information Processing”. b.d. MIT Press.\n\n\n„Picture Processing and Psychopictorics - 1st Edition”. b.d. https://www.elsevier.com/books/picture-processing-and-psychopictorics/lipkin/978-0-12-451550-5."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Barnard, Stephen T., and Martin A. Fischler. 1982. “Computational\nStereo.” ACM Computing Surveys 14 (4):\n553–72. https://doi.org/10.1145/356893.356896.\n\n\nBarrow, H. G., and J. M. Tenenbaum. 1981. “Computational\nVision.” Proceedings of the IEEE 69 (5): 572–95. https://doi.org/10.1109/PROC.1981.12026.\n\n\nBlake, Andrew, and Michael Isard. 2012. Active\nContours: The Application of\nTechniques from Graphics, Vision,\nControl Theory and Statistics to Visual\nTracking of Shapes in Motion.\nSpringer Science & Business Media.\n\n\nBlake, Andrew, Andrew Zisserman, and Greg Knowles. 1985. “Surface\nDescriptions from Stereo and Shading.” Image and Vision\nComputing, Papers from the 1985 Alvey Computer Vision\nand Image Interpretation Meeting, 3 (4): 183–91. https://doi.org/10.1016/0262-8856(85)90006-X.\n\n\nBurger, Wilhelm, and Mark J. Burge. 2016. Digital Image\nProcessing: An Algorithmic Introduction Using\nJava. Texts in Computer Science.\nLondon: Springer. https://doi.org/10.1007/978-1-4471-6684-9.\n\n\nCanny, John. 1986. “A Computational Approach to\nEdge Detection.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\nChollet, Francois, and J. J. Allaire. 2018. Deep\nLearning with R. Manning\nPublications.\n\n\n“Cooperative Computation of Stereo\nDisparity | Science.” n.d.\nhttps://www.science.org/doi/10.1126/science.968482.\n\n\nDavis, Larry S. 1975. “A Survey of Edge Detection\nTechniques.” Computer Graphics and Image Processing 4\n(3): 248–70. https://doi.org/10.1016/0146-664X(75)90012-X.\n\n\n“Deep Learning with Python, Second\nEdition.” n.d. Manning Publications.\nhttps://www.manning.com/books/deep-learning-with-python-second-edition.\n\n\nDev, Parvati. 1975. “Perception of Depth Surfaces in Random-Dot\nStereograms : A Neural Model.” International Journal of\nMan-Machine Studies 7 (4): 511–28. https://doi.org/10.1016/S0020-7373(75)80030-7.\n\n\nFelzenszwalb, Pedro F., and Daniel P. Huttenlocher. 2005.\n“Pictorial Structures for Object\nRecognition.” International Journal of Computer\nVision 61 (1): 55–79. https://doi.org/10.1023/B:VISI.0000042934.15159.49.\n\n\nFergus, R., P. Perona, and A. Zisserman. 2007. “Weakly\nSupervised Scale-Invariant Learning of Models\nfor Visual Recognition.” International Journal\nof Computer Vision 71 (3): 273–303. https://doi.org/10.1007/s11263-006-8707-x.\n\n\nFischler, M., and O. Firschein. 1987. “Readings in Computer\nVision: Issues, Problems, Principles, and Paradigms.” In.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. Illustrated edition. Cambridge,\nMassachusetts: The MIT Press.\n\n\nHanson, Allen. 1978. Computer Vision Systems.\nElsevier.\n\n\nHorn, Berthold K P. n.d. “Obtaining Shape from\nShading Information.”\n\n\nKass, Michael, Andrew Witkin, and Demetri Terzopoulos. 1988.\n“Snakes: Active Contour Models.”\nInternational Journal of Computer Vision 1 (4): 321–31. https://doi.org/10.1007/BF00133570.\n\n\nKetkar, Nikhil, and Eder Santana. 2017. Deep Learning with\nPython. Vol. 1. Springer.\n\n\nKirsch, Russell A. 1971. “Computer Determination of the\nConstituent Structure of Biological Images.” Computers and\nBiomedical Research 4 (3): 315–28. https://doi.org/10.1016/0010-4809(71)90034-6.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep\nLearning.” Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nMalladi, R., J. A. Sethian, and B. C. Vemuri. 1995. “Shape\nModeling with Front Propagation: A Level Set Approach.” IEEE\nTransactions on Pattern Analysis and Machine Intelligence 17 (2):\n158–75. https://doi.org/10.1109/34.368173.\n\n\nMarr, D., and E. Hildreth. 1980. “Theory of Edge\nDetection.” Proceedings of the Royal Society of London.\nSeries B, Biological Sciences 207 (1167): 187–217. https://doi.org/10.1098/rspb.1980.0020.\n\n\n“Mind as Machine: A History of Cognitive Science.” 2007.\nChoice Reviews Online 44 (11). https://doi.org/10.5860/choice.44-6202.\n\n\nMundy, Joseph L., and Andrew Zisserman, eds. 1992. Geometric\nInvariance in Computer Vision. Cambridge, MA, USA:\nMIT Press.\n\n\nNalwa, Vishvjit S., and Thomas O. Binford. 1986. “On\nDetecting Edges.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence PAMI-8 (6): 699–714. https://doi.org/10.1109/TPAMI.1986.4767852.\n\n\n“Optical and Electro-Optical Information\nProcessing.” n.d. MIT Press.\n\n\n“Picture Processing and Psychopictorics\n- 1st Edition.” n.d.\nhttps://www.elsevier.com/books/picture-processing-and-psychopictorics/lipkin/978-0-12-451550-5.\n\n\nPonce, Jean, Martial Hebert, Cordelia Schmid, and Andrew Zisserman.\n2007. Toward Category-Level Object Recognition.\nSpringer.\n\n\nRoberts, Lawrence G. 1980. Machine Perception of Three-dimensional Solids. Garland\nPub.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015.\n“U-Net: Convolutional Networks for\nBiomedical Image Segmentation.” arXiv.\nhttps://doi.org/10.48550/arXiv.1505.04597.\n\n\nSzeliski, Richard. 2022. Computer Vision:\nAlgorithms and Applications. Texts in\nComputer Science. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-030-34372-9.\n\n\nWinston, Patrick Henry. 1976. “The Psychology of Computer\nVision.” Pattern Recognition 8 (3): 193. https://doi.org/10.1016/0031-3203(76)90020-0.\n\n\nWitkin, Andrew P. 1981. “Recovering Surface Shape and Orientation\nfrom Texture.” Artificial Intelligence 17 (1): 17–45. https://doi.org/10.1016/0004-3702(81)90019-9.\n\n\nWoodham, Robert J. 1981. “Analysing Images of Curved\nSurfaces.” Artificial Intelligence 17 (1): 117–40. https://doi.org/10.1016/0004-3702(81)90022-9.\n\n\nZhou, Zongwei, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and\nJianming Liang. 2018. “UNet++: A Nested U-Net\nArchitecture for Medical Image Segmentation.”\narXiv. https://doi.org/10.48550/arXiv.1807.10165."
  }
]