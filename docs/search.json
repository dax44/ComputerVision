[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automatyczna analiza obrazu",
    "section": "",
    "text": "Wstęp\nNiniejsza książka powstała na potrzeby prowadzenia wykładu z Automatycznej analizy obrazu. Jest wynikiem moich doświadczeń z automatyczną analizą obrazu. W pisaniu tego kompendium wiedzy na temat Computer Vision bardzo pomocne były dwie pozycje literaturowe (Burger i Burge 2016; Szeliski 2022). Oprócz wspomnianych książek poświęconych wizji komputerowej, ważne są również pozycje objaśniające arkana deep learning. Wśród nich należy wymienić (Goodfellow, Bengio, i Courville 2016; Ketkar i Santana 2017; „Deep Learning with Python, Second Edition”, b.d.; Chollet i Allaire 2018). Ponadto zostaną wykorzystane nieprzebrane zasoby internetu - na stronach takich jak https://stackoverflow.com czy https://github.com można znaleźć rozwiązania do niemal każdego zadania.\nNa potrzeby zajęć laboratoryjnych będą dodatkowo potrzebne pewne programy komputerowe i biblioteki:\n\nFiji - darmowy program będący nakładką na program ImageJ. Służy on do operacji na zdjęciach. Do pobrania ze strony https://fiji.sc/;\nPython - język programowania, w którym można wykonać niemal dowolne zadanie z zakresu automatycznej analizy obrazu. Przez instalacje Python-a rozumiem zainstalowanie odpowiedniej dystrybucji tego programu (np. dla Windows zaleca się instalację dystrybucji Anconda lub Miniconda). Po szczegóły dotyczące instalacji Pythona na Windows odsyłam na stronę https://support.posit.co/hc/en-us/articles/1500007929061-Using-Python-with-the-RStudio-IDE;\nPo zainstalowaniu Pythona, trzeba też zainstalować dwie bardzo ważne biblioteki pythonowe do budowania i uczenia sieci głębokiego uczenia:\n\ntensorflow - jest end-to-end platformą typu open-source do uczenia maszynowego. Jest to kompleksowy i elastyczny ekosystem narzędzi, bibliotek i innych zasobów, które zapewniają przepływy pracy z wysokopoziomowymi interfejsami API. Ramy oferują różne poziomy koncepcji, abyś mógł wybrać ten, którego potrzebujesz do budowania i wdrażania modeli uczenia maszynowego;\nkeras - jest wysokopoziomową biblioteką do budowy sieci neuronowych, która działa na bazie TensorFlow, CNTK i Theano. Wykorzystanie Keras w deep learningu pozwala na łatwe i szybkie prototypowanie, a także płynne działanie na CPU i GPU. Aby zainstalować zarówno tensorflow, jak i keras z obsługo CPU lub GPU polecam instrukcję w filmie https://youtu.be/PnK1jO2kXOQ;\n\nOpenCV - jest biblioteką (ale nie programu R) funkcji programistycznych skierowanych głównie do wizji komputerowej czasu rzeczywistego. Instalację w Windows można znaleźć pod adresem https://docs.opencv.org/4.x/d3/d52/tutorial_windows_install.html;\nBiblioteki R-owe potrzebne do budowy modeli i obsługi obrazów, to:\n\nreticulate - biblioteka pozwalająca na wykorzystanie bibliotek i funkcji Python-a w R;\nmagick - biblioteka potrzebna do różnego rodzaju transformacji obrazów;\ntensorflow - biblioteka R-owa pozwalająca na wykorzystanie funkcji tensorflow Pythona;\nkeras - biblioteka R-owa pozwalająca na korzystanie z funkcji pakietu keras Pythonowego.\n\n\n\n\n\n\nBurger, Wilhelm, i Mark J. Burge. 2016. Digital Image Processing: An Algorithmic Introduction Using Java. Texts w Computer Science. London: Springer. https://doi.org/10.1007/978-1-4471-6684-9.\n\n\nChollet, Francois, i J. J. Allaire. 2018. Deep Learning with R. Manning Publications.\n\n\n„Deep Learning with Python, Second Edition”. b.d. Manning Publications. https://www.manning.com/books/deep-learning-with-python-second-edition.\n\n\nGoodfellow, Ian, Yoshua Bengio, i Aaron Courville. 2016. Deep Learning. Illustrated edition. Cambridge, Massachusetts: The MIT Press.\n\n\nKetkar, Nikhil, i Eder Santana. 2017. Deep Learning with Python. T. 1. Springer.\n\n\nSzeliski, Richard. 2022. Computer Vision: Algorithms and Applications. Texts w Computer Science. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-34372-9."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Automatyczna analiza obrazu jest znana również pod inną nazwą wizja komputerowa (ang. Computer Vision). Można powiedzieć też, że AAO1 jest częścią jeszcze szerszej dziedziny automatycznej analizy sygnałów. Ponieważ różnice pomiędzy analizą obrazu i dźwięku w niektórych zadaniach będą się zacierać, to poznane metody w toku tego wykładu będzie można śmiało przenieść na inne dziedziny. Oczywiście uwzględniając szereg podobieństw pomiędzy analizą obrazu i analizą dźwięku, istniej wiele dedykowanych modeli stosowanych tylko w domenie fal dźwiękowych.1 skrót od Automatyczna Analiza Obrazu\n\n\n\n\n\nW ramach zadań realizowanych przez wizję komputerową można wymienić:\n\npozyskiwanie obrazów (opis procesu “robienia zdjęcia” cyfrowego);\nprzetwarzanie obrazów w celu zmiany ich parametrów (np. poprawy ostrości, usuwania szumów, itp);\nanalizowania zdjęć w celu poszukiwania wzorców:\n\nzastosowanie ML2 do klasyfikacji obiektów na zdjęciach;\nzastosowanie ML do zadań regresyjnych (np. wyznaczanie poziomu wylania na podstawie zdjęć satelitarnych);\nzastosowanie ML w lokalizacji obiektów na obrazie (np. wskazanie położenia samolotu na zdjęciu w postaci ujęcia go w prostokątną ramkę);\nautoidentyfikacja (np. rozpoznawanie twarzy czy odcisku palca);\ntworzenie obrazów na podstawie fraz (istnieją sieci np. GAN, które są w stanie wygenerować całkowicie fikcyjny obraz na podstawie zdania opisującego co ma się na nim znaleźć);\nśledzenie ruchów na podstawie obrazu wideo (np. automatyczne kadrowanie obrazu wideo na podstawie położenia twarzy podczas rozmowy przez komunikator);\nsegmentacja obrazu;\ni wiele innych\n\n\n2 Machine LearningŚmiało można stwierdzić, że AAO towarzyszy nam codziennie i na każdym kroku. Czasami nie jesteśmy nawet tego świadomi.\n\nPoniżej przedstawiam listę wybranych zastosowań AAO:\n\nTransport - Rosnące wymagania sektora transportowego napędzają rozwój technologiczny w tej branży, w którego centrum znajduje się wizja komputerowa. Od pojazdów autonomicznych po wykrywanie zajętości miejsc parkingowych, Inteligentny System Transportowy (ITS) stał się krytycznym obszarem promowania wydajności, efektywności i bezpieczeństwa transportu.\n\n\n\n\n\nMonitorowanie zajętości miejsc na parkingu\n\n\n\nMedycyna. Dane z obrazowania medycznego są jednym z najbogatszych źródeł informacji. Bez odpowiedniej technologii lekarze są zmuszeni spędzać godziny na ręcznym analizowaniu danych pacjentów i wykonywaniu prac administracyjnych. Na szczęście, wraz z upływem lat i rozwojem technologii, branża opieki zdrowotnej stała się jedną z najszybciej przyjmujących nowe rozwiązania automatyzacji, w tym wizję komputerową.\n\n\n\n\n\nSegmentacja obrazu MRI\n\n\n\nProdukcja. Przemysł produkcyjny przyjął już szeroką gamę rozwiązań automatyzacji z wizją komputerową w centrum. Pomaga ona zautomatyzować kontrolę jakości, zminimalizować zagrożenia bezpieczeństwa i zwiększyć wydajność produkcji. Oto niektóre z najczęstszych zastosowań wizji komputerowej w przemyśle produkcyjnym.\n\n\n\n\nAAO w kontroli jakości\n\n\n\n\n\nAAO w procesie magazynowania\n\n\n\nBudowa. Sektor budowlany szybko przyjmuje technologię wizji komputerowej i wykorzystuje ją do wykrywania sprzętu ochrony osobistej, kontroli aktywów infrastruktury, wykrywania zagrożeń w miejscu pracy lub konserwacji.\n\n\n\n\nWykrywanie zużytych części\n\n\n\n\n\nNaruszenia procedur bezpieczeństwa\n\n\n\nRolnictwo. Sektor rolniczy był świadkiem kilku przypadków zastosowania modeli sztucznej inteligencji (w tym wizji komputerowej) w takich dziedzinach, jak monitorowanie upraw i plonów, zautomatyzowane zbiory, analiza warunków pogodowych, monitorowanie zdrowia zwierząt gospodarskich czy wykrywanie chorób roślin. Technologia ta zdobyła już silną pozycję dzięki możliwościom automatyzacji i wykrywania, a jej zastosowania będą się tylko rozszerzać.\n\n\n\n\nWyrywanie nietypowych zachowań zwierząt\n\n\n\n\n\nChoroby roślin\n\n\n\nSprzedaż detaliczna. Kamery zainstalowane w sklepach detalicznych pozwalają sprzedawcom zbierać duże ilości danych wizualnych pomocnych w projektowaniu lepszych doświadczeń klientów i pracowników. Rozwój systemów wizji komputerowej do przetwarzania tych danych sprawia, że cyfrowa transformacja branży realnej staje się znacznie bardziej osiągalna.\n\n\n\n\nWykrywanie braków towaru\n\n\n\n\n\nWykrywanie nietypowych zachowań klientów lub badanie zatłoczenia\n\n\nPrzestrzeni do zastosowań AAO jest jeszcze dużo więcej ale nie sposób ich wszystkich opisać. Oto kilka przykładów z różnych kategorii.\n\n\n\nBadanie ruchu zawodnika\n\n\n\n\n\nWykrywanie naruszeń prawa\n\n\n\n\n\nWykrywanie twarzy\n\n\nMoże też służyć do zabawy\n\n\n\nKilka obrazów wygenerowanych jako wariacje na temat mojego zdjęcia z wakacji\n\n\n\n\n\nKilka przykładów obrazów wygenerowanych przez sieć DALL E2 jako odpowiedź na zdanie “narysuj bez odrywania ręki jedna linią misia na zakupach”\n\n\n\n\n\nTym razem sieć DALL E2 została poproszona o namalowanie kobiet w stylu Van Gogha"
  },
  {
    "objectID": "history.html#lata-70",
    "href": "history.html#lata-70",
    "title": "2  Historia wizji komputerowej",
    "section": "2.1 Lata ’70",
    "text": "2.1 Lata ’70\nKiedy wizja komputerowa po raz pierwszy pojawiła się na początku lat siedemdziesiątych, była postrzegana jako wizualny komponent percepcji ambitnego programu naśladowania ludzkiej inteligencji i obdarzenia robotów inteligentnym zachowaniem. W tym czasie niektórzy z pionierów sztucznej inteligencji i robotyki (w miejscach takich jak MIT, Stanford) wierzyli, że rozwiązanie problemu “wejścia wizualnego” będzie łatwym krokiem na drodze do rozwiązania trudniejszych problemów, takich jak rozumowanie na wyższym poziomie i planowanie. Według jednej ze znanych historii, w 1966 roku Marvin Minsky z MIT poprosił swojego studenta Geralda Jay Sussmana o “spędzenie lata na podłączeniu kamery do komputera i nakłonieniu komputera do opisania tego, co widział”. Obecnie wiemy, że problem jest nieco trudniejszy niż wówczas się wydawało.\nTym, co odróżniało widzenie komputerowe od istniejącej już dziedziny cyfrowej obróbki obrazów, była chęć odzyskania trójwymiarowej struktury świata z obrazów i wykorzystania tego jako kroku w kierunku pełnego zrozumienia prezentowanej sceny. Winston (1976) oraz Hanson (1978) dostarczają dwóch ładnych zbiorów klasycznych prac z tego wczesnego okresu. Wczesne próby zrozumienia sceny polegały na wyodrębnieniu krawędzi, a następnie wnioskowaniu o strukturze 3D obiektu lub “świata bloków” z topologicznej struktury linii 2D Roberts (1980).\nJakościowe podejście do rozumienia intensywności i zmienności cieniowania oraz wyjaśniania ich przez efekty zjawisk formowania się obrazu, takich jak orientacja powierzchni i cienie, zostało spopularyzowane przez Barrow i Tenenbaum (1981) w ich pracy na temat obrazów wewnętrznych. W tym czasie opracowano również bardziej ilościowe podejścia do wizji komputerowej, w tym pierwszy z wielu opartych na cechach algorytmów korespondencji stereo (Dev 1975; „Cooperative Computation of Stereo Disparity | Science”, b.d.; Barnard i Fischler 1982)."
  },
  {
    "objectID": "history.html#lata-80",
    "href": "history.html#lata-80",
    "title": "2  Historia wizji komputerowej",
    "section": "2.2 Lata ’80",
    "text": "2.2 Lata ’80\nW latach ’80 ubiegłego wieku wiele uwagi poświęcono bardziej wyrafinowanym technikom matematycznym służącym do przeprowadzania ilościowej analizy obrazów i scen. Piramidy obrazów zaczęły być powszechnie stosowane do wykonywania zadań takich jak mieszanie obrazów i wyszukiwanie korespondencji coarse-to-fine. Wykorzystanie stereo jako ilościowej wskazówki kształtu zostało rozszerzone o szeroką gamę technik shape-from-X, w tym shape from shading (Horn, b.d.; Blake, Zisserman, i Knowles 1985).\n\n\n\nRysunek 2.2: Przykład wykorzystania techniki piramid blending\n\n\nW tym okresie prowadzono również badania nad lepszym wykrywaniem krawędzi i konturów (Canny 1986; Nalwa i Binford 1986), stereografii fotometrycznej (Woodham 1981) oraz kształty z tekstur (Witkin 1981). W tym okresie prowadzono również badania nad lepszym wykrywaniem krawędzi i konturów, w tym wprowadzono dynamicznie ewoluujące trackery konturów, takie jak węże, a także trójwymiarowe modele oparte na fizyce. Naukowcy zauważyli, że wiele algorytmów detekcji stereoskopowej, przepływu, shape-from-X i krawędzi może być zunifikowanych lub przynajmniej opisanych przy użyciu tych samych ram matematycznych, jeśli zostaną one postawione jako problemy optymalizacji wariacyjnej i uodpornione (dobrze postawione) przy użyciu regularyzacji.\nNieco później wprowadzono warianty on-line algorytmów MRF (ang. Markov Random Field), które modelowały i aktualizowały niepewności za pomocą filtru Kalmana. Podjęto również próby odwzorowania zarówno algorytmów regularyzowanych jak i MRF na sprzęt zrównoleglony (ang. parallel). Książka (Fischler i Firschein 1987) zawiera zbiór artykułów skupiających się na wszystkich tych tematach (stereo, przepływ, regularność, MRF, a nawet widzenie wyższego poziomu)."
  },
  {
    "objectID": "history.html#lata-90",
    "href": "history.html#lata-90",
    "title": "2  Historia wizji komputerowej",
    "section": "2.3 Lata ’90",
    "text": "2.3 Lata ’90\nPodczas gdy wiele z wcześniej wymienionych tematów było nadal eksplorowanych, kilka z nich stało się znacznie bardziej aktywnych. Nagły wzrost aktywności w zakresie wykorzystania niezmienników projekcyjnych do celów rozpoznania (Mundy i Zisserman 1992) przerodził się w skoordynowane wysiłki zmierzające do rozwiązania problemu structure from motion. Wiele początkowych działań skierowanych było na rekonstrukcje rzutowe, które nie wymagają znajomości kalibracji kamery. Równolegle, techniki faktoryzacji zostały opracowane w celu efektywnego rozwiązywania problemów, dla których miały zastosowanie przybliżenia kamery ortograficznej, a następnie rozszerzone na przypadek perspektywiczny.\nW końcu zaczęto stosować pełną optymalizację globalną która później została uznana za tożsama z technikami dopasowania wiązki, tradycyjnie stosowanymi w fotogrametrii. W pełni zautomatyzowane systemy modelowania 3D zostały zbudowane przy użyciu tych technik.\nPrace rozpoczęte w latach 80-tych nad wykorzystaniem szczegółowych pomiarów barwy i natężenia światła w połączeniu z dokładnymi modelami fizycznymi transportu promieniowania i tworzenia kolorowych obrazów stworzyły własną dziedzinę znaną jako widzenie oparte na fizyce. Algorytmy stereo na podstawie wielu obrazów, które tworzą kompletne powierzchnie 3D były również aktywnym tematem badań, który jest aktualny do dziś.\nAlgorytmy śledzenia również uległy dużej poprawie, w tym śledzenie konturów z wykorzystaniem aktywnych konturów, takich jak węże (Kass, Witkin, i Terzopoulos 1988), filtry cząsteczkowe (Blake i Isard 2012) i zbiorów poziomnicowych (ang. level set) (Malladi, Sethian, i Vemuri 1995), a także techniki oparte na intensywności (bezpośrednie), często stosowane do śledzenia twarzy.\n\n\n\nRysunek 2.3: Przykład śledzenia twarzy przez algorytm\n\n\nSegmentacja obrazów, temat, który jest aktywny od początku wizji komputerowej, był również aktywnym tematem badań, produkując techniki oparte na minimalnej energii i minimalnej długości opisu, znormalizowanych cięciach i średnim przesunięciu.\nZaczęły pojawiać się techniki uczenia statystycznego, najpierw w zastosowaniu analizy składowych głównych, eigenface do rozpoznawania twarzy oraz liniowych systemów dynamicznych do śledzenia krzywych.\nByć może najbardziej zauważalnym rozwojem w dziedzinie widzenia komputerowego w tej dekadzie była zwiększona interakcja z grafiką komputerową, zwłaszcza w interdyscyplinarnym obszarze modelowania i renderowania opartego na obrazach. Pomysł manipulowania obrazami świata rzeczywistego bezpośrednio w celu tworzenia nowych animacji po raz pierwszy stał się znany dzięki technikom morfingu obrazu."
  },
  {
    "objectID": "history.html#lata-00",
    "href": "history.html#lata-00",
    "title": "2  Historia wizji komputerowej",
    "section": "2.4 Lata ’00",
    "text": "2.4 Lata ’00\nTa dekada kontynuowała pogłębianie interakcji pomiędzy dziedzinami wizji i grafiki, ale co ważniejsze, przyjęła podejścia oparte na danych i uczeniu się jako kluczowe komponenty wizji. Wiele z tematów wprowadzonych w rubryce renderingu opartego na obrazie, takich jak zszywanie obrazów, przechwytywanie i renderowanie pola świetlnego oraz przechwytywanie obrazów o wysokim zakresie dynamicznym (HDR) poprzez bracketing ekspozycji, zostało ponownie ochrzczonych mianem fotografii obliczeniowej, aby potwierdzić zwiększone wykorzystanie takich technik w codziennej fotografii cyfrowej. Na przykład, szybkie przyjęcie bracketingu ekspozycji do tworzenia obrazów o wysokim zakresie dynamicznym wymagało opracowania algorytmów kompresji dynamiki, aby przekształcić takie obrazy z powrotem do wyników możliwych do wyświetlenia. Oprócz łączenia wielu ekspozycji, opracowano techniki łączenia obrazów z lampą błyskową z ich odpowiednikami bez lampy błyskowej.\n\n\n\nRysunek 2.4: Przykład rozpoznawania obiektów\n\n\nDrugim wartym uwagi trendem w tej dekadzie było pojawienie się technik opartych na cechach (połączonych z uczeniem) do rozpoznawania obiektów. Niektóre z godnych uwagi prac w tej dziedzinie obejmują model konstelacji (Ponce i in. 2007; Fergus, Perona, i Zisserman 2007) oraz struktury obrazowe (Felzenszwalb i Huttenlocher 2005). Techniki oparte na cechach dominują również w innych zadaniach rozpoznawania, takich jak rozpoznawanie scen, panoram i lokalizacji. I chociaż cechy oparte na punktach zainteresowania (patch-based) dominują w obecnych badaniach, niektóre grupy zajmują się rozpoznawaniem na podstawie konturów i segmentacji regionów.\nInnym istotnym trendem tej dekady było opracowanie bardziej wydajnych algorytmów dla złożonych problemów optymalizacji globalnej. Chociaż trend ten rozpoczął się od prac nad cięciami grafów, duży postęp dokonał się również w algorytmach przekazywania wiadomości, takich jak loopy belief propagation (LBP).\nNajbardziej zauważalnym trendem tej dekady, który do tej pory całkowicie opanował rozpoznawanie obrazu i większość innych aspektów widzenia komputerowego, było zastosowanie zaawansowanych technik uczenia maszynowego do problemów widzenia komputerowego. Trend ten zbiegł się w czasie ze zwiększoną dostępnością ogromnych ilości częściowo oznakowanych danych w Internecie, a także ze znacznym wzrostem mocy obliczeniowej, co sprawiło, że uczenie się kategorii obiektów bez użycia starannego nadzoru człowieka stało się bardziej realne."
  },
  {
    "objectID": "history.html#lata-10",
    "href": "history.html#lata-10",
    "title": "2  Historia wizji komputerowej",
    "section": "2.5 Lata ’10",
    "text": "2.5 Lata ’10\nTrend do wykorzystywania dużych etykietowanych zbiorów danych do rozwoju algorytmów uczenia maszynowego stał się falą, która całkowicie zrewolucjonizowała rozwój algorytmów rozpoznawania obrazów, a także innych aplikacji, takich jak denoising i przepływ optyczny, które wcześniej wykorzystywały techniki Bayesa i optymalizacji globalnej. Tendencję tę umożliwił rozwój wysokiej jakości wielkoskalowych anotowanych zbiorów danych, takich jak ImageNet, Microsoft COCO i LVIS. Te zbiory danych dostarczyły nie tylko wiarygodnych metryk do śledzenia postępów algorytmów rozpoznawania i segmentacji semantycznej, ale co ważniejsze, wystarczającej ilości etykietowanych danych do opracowania kompletnych rozwiązań opartych na uczeniu maszynowym.\nInnym ważnym trendem był dramatyczny wzrost mocy obliczeniowej dostępny dzięki rozwojowi algorytmów ogólnego przeznaczenia (data-parallel) na jednostkach przetwarzania graficznego (GPGPU). Przełomowa głęboka sieć neuronowa SuperVision (“AlexNet”), która jako pierwsza wygrała coroczne zawody w rozpoznawaniu obrazów na dużą skalę ImageNet, opierała się na treningu na GPU, a także na szeregu usprawnień technicznych, które przyczyniły się dramatycznie do wzrostu jej wydajności. Po opublikowaniu tej pracy postęp w wykorzystaniu głębokich architektur konwolucyjnych gwałtownie przyspieszył, do tego stopnia, że obecnie są one jedyną architekturą braną pod uwagę w zadaniach rozpoznawania i segmentacji semantycznej, a także preferowaną architekturą w wielu innych zadaniach wizyjnych, w tym w zadaniach przepływu optycznego, denoisingu i wnioskowania o głębi monokularnej (LeCun, Bengio, i Hinton 2015).\nDuże zbiory danych i architektury GPU, w połączeniu z szybkim upowszechnianiem pomysłów poprzez pojawiające się w odpowiednim czasie publikacje na arXiv, a także rozwój języków głębokiego uczenia i otwarty dostęp do modeli sieci neuronowych, przyczyniły się do gwałtownego rozwoju tej dziedziny, zarówno pod względem szybkich postępów i możliwości, jak i samej liczby publikacji i badaczy zajmujących się obecnie tymi tematami. Umożliwiły one również rozszerzenie podejść do rozpoznawania obrazów na zadania związane z rozumieniem wideo, takie jak rozpoznawanie akcji, a także zadania regresji strukturalnej, takie jak estymacja w czasie rzeczywistym wieloosobowej pozy ciała.\nSpecjalistyczne czujniki i sprzęt do zadań związanych z widzeniem komputerowym również stale się rozwijały. Wprowadzona w 2010 r. kamera głębi Microsoft Kinect szybko stała się podstawowym elementem wielu systemów modelowania 3D i śledzenia osób. W ciągu dekady systemy modelowania i śledzenia kształtu ciała 3D nadal się rozwijały, do tego stopnia, że obecnie możliwe jest wnioskowanie o modelu 3D osoby wraz z gestami i ekspresją na podstawie jednego obrazu.\n\n\n\n\nBarnard, Stephen T., i Martin A. Fischler. 1982. „Computational Stereo”. ACM Computing Surveys 14 (4): 553–72. https://doi.org/10.1145/356893.356896.\n\n\nBarrow, H. G., i J. M. Tenenbaum. 1981. „Computational Vision”. Proceedings of the IEEE 69 (5): 572–95. https://doi.org/10.1109/PROC.1981.12026.\n\n\nBlake, Andrew, i Michael Isard. 2012. Active Contours: The Application of Techniques from Graphics, Vision, Control Theory and Statistics to Visual Tracking of Shapes in Motion. Springer Science & Business Media.\n\n\nBlake, Andrew, Andrew Zisserman, i Greg Knowles. 1985. „Surface Descriptions from Stereo and Shading”. Image and Vision Computing, Papers from the 1985 Alvey Computer Vision i Image Interpretation Meeting, 3 (4): 183–91. https://doi.org/10.1016/0262-8856(85)90006-X.\n\n\nCanny, John. 1986. „A Computational Approach to Edge Detection”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\n„Cooperative Computation of Stereo Disparity | Science”. b.d. https://www.science.org/doi/10.1126/science.968482.\n\n\nDev, Parvati. 1975. „Perception of Depth Surfaces in Random-Dot Stereograms : A Neural Model”. International Journal of Man-Machine Studies 7 (4): 511–28. https://doi.org/10.1016/S0020-7373(75)80030-7.\n\n\nFelzenszwalb, Pedro F., i Daniel P. Huttenlocher. 2005. „Pictorial Structures for Object Recognition”. International Journal of Computer Vision 61 (1): 55–79. https://doi.org/10.1023/B:VISI.0000042934.15159.49.\n\n\nFergus, R., P. Perona, i A. Zisserman. 2007. „Weakly Supervised Scale-Invariant Learning of Models for Visual Recognition”. International Journal of Computer Vision 71 (3): 273–303. https://doi.org/10.1007/s11263-006-8707-x.\n\n\nFischler, M., i O. Firschein. 1987. „Readings in Computer Vision: Issues, Problems, Principles, and Paradigms”. W.\n\n\nHanson, Allen. 1978. Computer Vision Systems. Elsevier.\n\n\nHorn, Berthold K P. b.d. „Obtaining Shape from Shading Information”.\n\n\nKass, Michael, Andrew Witkin, i Demetri Terzopoulos. 1988. „Snakes: Active Contour Models”. International Journal of Computer Vision 1 (4): 321–31. https://doi.org/10.1007/BF00133570.\n\n\nLeCun, Yann, Yoshua Bengio, i Geoffrey Hinton. 2015. „Deep Learning”. Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nMalladi, R., J. A. Sethian, i B. C. Vemuri. 1995. „Shape Modeling with Front Propagation: A Level Set Approach”. IEEE Transactions on Pattern Analysis and Machine Intelligence 17 (2): 158–75. https://doi.org/10.1109/34.368173.\n\n\n„Mind as Machine: A History of Cognitive Science”. 2007. Choice Reviews Online 44 (11). https://doi.org/10.5860/choice.44-6202.\n\n\nMundy, Joseph L., i Andrew Zisserman, red. 1992. Geometric Invariance in Computer Vision. Cambridge, MA, USA: MIT Press.\n\n\nNalwa, Vishvjit S., i Thomas O. Binford. 1986. „On Detecting Edges”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 699–714. https://doi.org/10.1109/TPAMI.1986.4767852.\n\n\nPonce, Jean, Martial Hebert, Cordelia Schmid, i Andrew Zisserman. 2007. Toward Category-Level Object Recognition. Springer.\n\n\nRoberts, Lawrence G. 1980. Machine Perception of Three-Dimensional Solids. Garland Pub.\n\n\nWinston, Patrick Henry. 1976. „The Psychology of Computer Vision”. Pattern Recognition 8 (3): 193. https://doi.org/10.1016/0031-3203(76)90020-0.\n\n\nWitkin, Andrew P. 1981. „Recovering Surface Shape and Orientation from Texture”. Artificial Intelligence 17 (1): 17–45. https://doi.org/10.1016/0004-3702(81)90019-9.\n\n\nWoodham, Robert J. 1981. „Analysing Images of Curved Surfaces”. Artificial Intelligence 17 (1): 117–40. https://doi.org/10.1016/0004-3702(81)90022-9."
  },
  {
    "objectID": "digt_img.html#skale",
    "href": "digt_img.html#skale",
    "title": "\n3  Obrazy cyfrowe\n",
    "section": "\n3.1 Skale",
    "text": "3.1 Skale\n\nSkala szarości - dane obrazu w skali szarości składają się z pojedynczego kanału (ang. channel), który reprezentuje intensywność, jasność lub gęstość obrazu. W większości przypadków sens mają tylko wartości dodatnie, ponieważ liczby reprezentują natężenie energii świetlnej lub gęstość filmu, a więc nie mogą być ujemne, więc zwykle używa się całych liczb całkowitych z zakresu \\(0, \\ldots , 2^{k - 1}\\) są używane. Na przykład typowy obraz w skali szarości wykorzystuje \\(k = 8\\) bitów (1 bajt) na piksel i wartości intensywności z zakresu \\(0,\\ldots,255\\), gdzie wartość 0 oznacza minimalną jasność (czerń), a 255 maksymalną jasność (biel). W wielu zastosowaniach profesjonalnej fotografii i druku, a także w medycynie i astronomii, 8 bitów na piksel nie jest wystarczające. W tych dziedzinach często spotyka się głębię obrazu 12, 14, a nawet 16 bitów. Zauważ, że głębia bitowa zwykle odnosi się do liczby bitów używanych do reprezentowania jednego składnika koloru, a nie liczby bitów potrzebnych do reprezentowania koloru piksela. Na przykład, zakodowany w RGB kolorowy obraz z 8-bitową głębią wymagałby 8 bitów dla każdego kanału, co daje w sumie 24 bity, podczas gdy ten sam obraz z 12-bitową głębią wymagałby w sumie 36 bitów.\nObrazy binarne (ang. binary images) - to specjalny rodzaj obrazu, w którym piksele mogą przyjmować tylko jedną z dwóch wartości, czarną lub białą. Wartości te są zwykle kodowane przy użyciu pojedynczego bitu (0/1) na piksel. Obrazy binarne są często wykorzystywane do reprezentowania grafiki liniowej, archiwizacji dokumentów, kodowania transmisji faksowych i oczywiście w druku elektronicznym.\nObrazy kolorowe (ang. color images) - większość kolorowych obrazów opiera się na kolorach podstawowych: czerwonym, zielonym i niebieskim (RGB), zwykle wykorzystując 8 bitów dla każdego kanału. W tego rodzaju obrazach kolorowych, każdy piksel wymaga 3×8 = 24 bity do zakodowania wszystkich trzech składowych, a zakres każdej indywidualnej składowej koloru wynosi [0, 255]. Podobnie jak w przypadku obrazów w skali szarości, kolorowe obrazy z 30, 36 i 42 bitami na piksel są powszechnie używane w profesjonalnych aplikacjach. Wreszcie, podczas gdy większość obrazów kolorowych zawiera trzy składowe, obrazy z czterema lub więcej składowymi koloru są powszechne w druku, zwykle oparte na modelu koloru CMYK (Cyan-Magenta-Yellow- Black). Główna różnica między tymi dwiema paletami polega na tym, że RGB ma więcej możliwości kolorów, ponieważ jest w stanie wygenerować więcej odcieni niż CMYK, ale kolory wyświetlane przez RGB nie są takie same jak te, które otrzymujemy przy druku z CMYK. Kolory drukowane z CMYK mogą również różnić się od tych wyświetlanych na ekranie.\nObrazy specjalne - są wymagane, jeżeli żaden z powyższych formatów standardowych nie jest wystarczający do przedstawienia wartości obrazu. Dwa popularne przykłady obrazów specjalnych to obrazy z wartościami ujemnymi oraz obrazy z wartościami zmiennoprzecinkowymi. Obrazy z wartościami ujemnymi powstają podczas etapów przetwarzania obrazu, takich jak filtrowanie w celu wykrywania krawędzi, a obrazy z wartościami zmiennoprzecinkowymi są często spotykane w zastosowaniach medycznych, biologicznych lub astronomicznych, gdzie wymagany jest zwiększony zakres liczbowy i precyzja. Te specjalne formaty są w większości przypadków specyficzne dla danego zastosowania i dlatego mogą być trudne do wykorzystania przez standardowe narzędzia do przetwarzania obrazów."
  },
  {
    "objectID": "digt_img.html#formaty-zapisu",
    "href": "digt_img.html#formaty-zapisu",
    "title": "\n3  Obrazy cyfrowe\n",
    "section": "\n3.2 Formaty zapisu",
    "text": "3.2 Formaty zapisu\n\n3.2.1 TIFF\nTIFF (ang. Tagged Image File Format) jest formatem pliku, który jest używany do przechowywania i wymiany obrazów cyfrowych. Jest to format bezstratny, co oznacza, że po zapisaniu i odczytaniu obrazu jego jakość pozostaje taka sama. TIFF jest obsługiwany przez wiele programów do edycji obrazów i może być używany do przechowywania różnych rodzajów obrazów, w tym obrazów w skali szarości, kolorowych oraz map bitowych. Format TIFF jest często używany przez profesjonalnych fotografów i grafików, ponieważ pozwala na zachowanie wysokiej jakości obrazu i jest kompatybilny z wieloma programami i urządzeniami.\n\n3.2.2 GIF\nGIF (ang. Graphics Interchange Format) jest formatem pliku graficznego, który jest używany do przechowywania i wymiany obrazów w internecie. GIF jest formatem bezstratnym, ale jest kompresowany, co pozwala na zmniejszenie rozmiaru pliku i przyspieszenie jego przesyłania. Co ważne, GIF jest formatem obsługującym animacje, co oznacza, że może on przechowywać kilka klatek jako jeden plik, co pozwala na tworzenie animowanych obrazów, często używanych jako emotikony, ikony lub małe animacje na stronach internetowych. GIF jest również ograniczony do 256 kolorów, co oznacza, że nie jest on dobrym rozwiązaniem do przechowywania zdjęć o wysokiej jakości.\n\n3.2.3 PNG\nPNG (ang. Portable Network Graphics) jest bezstratnym formatem pliku graficznego, który jest używany do przechowywania i wymiany obrazów w internecie. Podobnie jak GIF może być kompresowany w celu zmniejszenia rozmiaru pliku. Co ważne, format PNG jest formatem obsługującym przezroczystość, co oznacza, że może on przechowywać kanał alfa, który jest odpowiedzialny za przezroczystość obrazu, co pozwala na zastosowanie efektu przezroczystości na obrazie bez konieczności dodatkowego tworzenia specjalnego tła. PNG jest również w stanie przechowywać więcej kolorów niż GIF, co oznacza, że jest to lepsze rozwiązanie dla obrazów o wysokiej jakości.\n\n3.2.4 JPEG\nJPEG (ang. Joint Photographic Experts Group) to popularny format zapisu obrazów cyfrowych, który jest szczególnie przydatny do przechowywania zdjęć. Format ten pozwala na kompresję pliku (stratną), dzięki czemu pliki JPEG są mniejsze niż pliki niekompresowane. Format ten jest szczególnie przydatny do przechowywania zdjęć z wysokim poziomem szczegółów, takich jak zdjęcia przyrody czy portrety.\n\n3.2.5 EXIF\nEXIF (ang. Exchangeable Image File Format) to format danych, który jest zapisywany w pliku obrazu cyfrowego, takim jak JPEG lub TIFF. Informacje EXIF zawierają szczegółowe dane dotyczące zdjęcia, takie jak data i godzina utworzenia zdjęcia, parametry aparatu fotograficznego (np. przysłona, czas naświetlania, ISO), dane dotyczące obiektywu, a także współrzędne GPS, jeśli zdjęcie zostało zrobione z użyciem aparatu z GPS. EXIF jest przydatny dla fotografów i programów do obróbki zdjęć, ponieważ pozwala na łatwe odczytanie i wykorzystanie tych danych.\n\n3.2.6 BMP\nBMP (ang. Bitmap) to format pliku obrazu, który jest przeznaczony do przechowywania obrazów rastrowych, takich jak zdjęcia, grafiki i mapy bitowe. BMP jest formatem pliku natywnym dla systemów operacyjnych Windows, co oznacza, że pliki tego formatu są bezpośrednio obsługiwane przez system Windows i nie wymagają dodatkowego oprogramowania do odczytu.\nBMP jest formatem bezstratnym, co oznacza, że po zapisie obrazu w tym formacie, jego jakość pozostaje taka sama jak przed zapisem. Pliki BMP są jednak dość duże, ponieważ nie są skompresowane, co oznacza, że zajmują więcej miejsca na dysku niż pliki skompresowane innymi formatami. BMP jest często używany do przechowywania obrazów w celach archiwizacyjnych, ponieważ zachowuje pełną jakość obrazu.\n\n\nPorównanie formatów\n\n\n\n3.2.7 Operacje na plikach\nIstnieje wiele zewnętrznych (w stosunku do środowiska R) profesjonalnych narzędzi do obróbki zdjęć. Wśród nich z pewnością należy wymienić: Adobe Photoshop, CorelDRAW, Gimp, PIXLR, FIji (ImageJ) i wiele innych. Część z nich jest komercyjna, a część darmowa. Osobiście do przetwarzania obrazów pochodzących z badań biologicznych, medycznych, czy inżynierskich polecam darmowy program Fiji, będący rozszerzeniem swojego pierwowzoru, czyli ImageJ.\nRównież w samym środowisku R istnieje szereg bibliotek do obsługi obrazów:\n\n\nimager - pozwala na szybkie przetwarzanie obrazów w maksymalnie 4 wymiarach (dwa wymiary przestrzenne, jeden wymiar czasowy/głębokościowy, jeden wymiar koloru). Udostępnia większość tradycyjnych narzędzi do przetwarzania obrazów (filtrowanie, morfologia, transformacje, itp.), jak również różne funkcje do łatwej analizy danych obrazowych przy użyciu R.\n\nimagerExtra - poszerzenie zestawu funkcji pakietu imager.\n\nmagick - dostarcza nowoczesnego i prostego zestawu narzędzi do przetwarzania obrazów w R. Obejmuje on ImageMagick STL, który jest najbardziej wszechstronną biblioteką przetwarzania obrazów typu open-source dostępną obecnie.\n\nimageseg - pakiet ogólnego przeznaczenia do segmentacji obrazów z wykorzystaniem modeli TensorFlow opartych na architekturze U-Net autorstwa Ronneberger, Fischer, i Brox (2015) oraz architekturze U-Net++ autorstwa Zhou i in. (2018). Dostarcza wstępnie wytrenowane modele do oceny gęstości łanu i gęstości roślinności podszytu na podstawie zdjęć roślinności. Ponadto pakiet zapewnia workflow do łatwego tworzenia wejściowych modeli i architektur modeli dla segmentacji obrazów ogólnego przeznaczenia na podstawie obrazów w skali szarości lub kolorowych, zarówno dla segmentacji obrazów binarnych, jak i wieloklasowych.\n\npliman - jest pakietem do analizy obrazów, ze szczególnym uwzględnieniem obrazów roślin. Jest użytecznym narzędziem do uzyskania informacji ilościowej dla obiektów docelowych. W kontekście obrazów roślin, ilościowe określanie powierzchni liści, nasilenia chorób, liczby zmian chorobowych, liczenie liczby ziaren, uzyskiwanie statystyk ziaren (np. długość i szerokość) można wykonać stosując pakiet pliman.\n\n\nPrzykład 3.1 W tym przykładzie przedstawiona zostanie procedura importu i eksportu obrazów do różnych formatów. Najpierw wczytamy zdjęcie wykonane telefonem (IPhone 12) zapisane w formacie TIFF, a następnie zapiszemy to zdjęcie w kilku innych formatach, by na końcu wczytać je wszystkie i porównać.\n\nKodlibrary(magick)\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\n\n# wczytywanie obrazu \nimg_orig <- image_read(\"images/IMG_3966.tiff\")\n\n# informacje o obrazie\nimg_orig_info <- image_info(img_orig)\nimg_orig_info\n\n# A tibble: 1 × 7\n  format width height colorspace matte filesize density\n  <chr>  <int>  <int> <chr>      <lgl>    <int> <chr>  \n1 TIFF    3024   4032 sRGB       FALSE 36614010 72x72  \n\n\nJak widać obraz w formacie TIFF zajmuje bardzo dużo miejsca na dysku (36,6MB).\n\nKod# eksport do GIF\nimage_write(img_orig, path = \"images/img.gif\", \n            format = \"gif\")\n\n# eksport do PNG\nimage_write(img_orig, path = \"images/img.png\", \n            format = \"png\")\n\n# eksport do JPEG z jakością 100%\nimage_write(img_orig, path = \"images/img.jpeg\", \n            quality = 100, format = \"jpeg\")\n\n# eksport do JPEG z jakością 75%\nimage_write(img_orig, path = \"images/img2.jpeg\", \n            quality = 75, format = \"jpeg\")\n\n# eksport do JPEG z jakością 50%\nimage_write(img_orig, path = \"images/img3.jpeg\", \n            quality = 50, format = \"jpeg\")\n\n# eksport do JPEG z jakością 25%\nimage_write(img_orig, path = \"images/img4.jpeg\", \n            quality = 25, format = \"jpeg\")\n\n\nPo zapisie do innych formatów pliki znacznie zmniejszyły swoją wielkość.\n\nKodimg_gif <- image_read(\"images/img.gif\")\nimg_gif_info <- image_info(img_gif)\n\nimg_png <- image_read(\"images/img.png\")\nimg_png_info <- image_info(img_png)\n\nimg_jpeg1 <- image_read(\"images/img.jpeg\")\nimg_jpeg1_info <- image_info(img_jpeg1)\n\nimg_jpeg2 <- image_read(\"images/img2.jpeg\")\nimg_jpeg2_info <- image_info(img_jpeg2)\n\nimg_jpeg3 <- image_read(\"images/img3.jpeg\")\nimg_jpeg3_info <- image_info(img_jpeg3)\n\nimg_jpeg4 <- image_read(\"images/img4.jpeg\")\nimg_jpeg4_info <- image_info(img_jpeg4)\n\nbind_rows(img_orig_info, img_gif_info, img_png_info,\n          img_jpeg1_info, img_jpeg2_info, img_jpeg3_info,\n          img_jpeg4_info) |> \n  gt()\n\n\n\n\n\n\nTabela 3.1:  Podstawowe informacje o obrazach \n  \n  \nformat\n      width\n      height\n      colorspace\n      matte\n      filesize\n      density\n    \n\n\nTIFF\n3024\n4032\nsRGB\nFALSE\n36614010\n72x72\n\n\nGIF\n3024\n4032\nsRGB\nFALSE\n7422827\n72x72\n\n\nPNG\n3024\n4032\nsRGB\nFALSE\n11994144\n28x28\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n8211226\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n1546461\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n1043337\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n691309\n72x72\n\n\n\n\n\n\n\nPomimo znacznej różnicy w wielkości obrazów (w sensie miejsca zajmowanego na dysku) nie różnią się one znacznie jakością (przynajmniej na pierwszy rzut oka)\n\nKodimg_orig\nimg_jpeg1\nimg_gif\nimg_png\nimg_jpeg2\nimg_jpeg3\nimg_jpeg4\n\n\n\n\n\nRysunek 3.3: TIFF\n\n\n\n\n\n\nRysunek 3.4: JPEG 100%\n\n\n\n\n\n\n\n\nRysunek 3.5: GIF\n\n\n\n\n\n\nRysunek 3.6: PNG\n\n\n\n\n\n\n\n\nRysunek 3.7: JPEG 75%\n\n\n\n\n\n\nRysunek 3.8: JPEG 50%\n\n\n\n\n\n\n\n\nRysunek 3.9: JPEG 25%\n\n\n\n\n\n\n\nKodmagick2cimg(img_orig) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_jpeg1) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_gif) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_png) |> \n  as.data.frame() |> \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |>\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\n\n\n\n\nRysunek 3.10: Histogramy RGB dla pliku TIFF\n\n\n\n\n\n\nRysunek 3.11: Histogray RGB dla pliku JPEG 100%\n\n\n\n\n\n\n\n\nRysunek 3.12: Histogray RGB dla pliku GIF\n\n\n\n\n\n\nRysunek 3.13: Histogray RGB dla pliku PNG\n\n\n\n\n\n\n\n\n\n\n\nRonneberger, Olaf, Philipp Fischer, i Thomas Brox. 2015. „U-Net: Convolutional Networks for Biomedical Image Segmentation”. arXiv. https://doi.org/10.48550/arXiv.1505.04597.\n\n\nZhou, Zongwei, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, i Jianming Liang. 2018. „UNet++: A Nested U-Net Architecture for Medical Image Segmentation”. arXiv. https://doi.org/10.48550/arXiv.1807.10165."
  },
  {
    "objectID": "transformations.html#przykłady-różnych-transformacji",
    "href": "transformations.html#przykłady-różnych-transformacji",
    "title": "\n4  Transformacje geometryczne\n",
    "section": "\n4.1 Przykłady różnych transformacji",
    "text": "4.1 Przykłady różnych transformacji\n\nKodlibrary(magick)\nfrink <- image_read(\"https://jeroen.github.io/images/frink.png\")\nfrink\n\n\n\nFrink w oryginalnym rozmiarze\n\n\n\n\n\nKodimage_scale(frink, \"250%x250%\")\n\n\n\nFrink powiększony o 150%\n\n\n\n\n\nKodimage_scale(frink, \"300x100!\")\n\n\n\nFrink poszerzony do proporcji 3:1\n\n\n\n\n\nKodimage_rotate(frink, 90)\n\n\n\nRotacja o 90 stopni"
  },
  {
    "objectID": "transformations.html#rotacje-obrazów",
    "href": "transformations.html#rotacje-obrazów",
    "title": "\n4  Transformacje geometryczne\n",
    "section": "\n4.2 Rotacje obrazów",
    "text": "4.2 Rotacje obrazów\nObroty o inne kąty niż pełne wielokrotności 90\\(\\degree\\) powodują pewne problemy, ponieważ po rotacji powstają piksele, które nie pokrywają żadnej wartości z oryginalnego obrazu, a część z nich zawiera kilka wartości (Rysunek 4.1). Podobne zjawisko może powstać w sytuacji zmiany rozmiaru obrazów. W tych sytuacjach konieczna jest interpolacja pikseli zarówno “pustych”, jak i “wypełnionych”.\n\n\nRysunek 4.1: Przykład rotacji obrazu\n\n\n\nKodlibrary(imager)\nlibrary(keras)\n\ndata <- dataset_mnist()\nim <- t(data$train$x[2023,,])\nim <- as.cimg(im)\n\nim_list <- map_il(0:6, ~imrotate(im, # Nearest Neighbour interp.\n                                 angle = 15*.x, \n                                 interpolation = 0)) \nrow1 <- imappend(im_list[1:4], axis = \"x\")  \nrow2 <- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |> \n  plot(interp=F) # antialiasing off\n\nim_list <- map_il(0:6, ~imrotate(im, # linear interpolation\n                                 angle = 15*.x, \n                                 interpolation = 1)) \nrow1 <- imappend(im_list[1:4], axis = \"x\")  \nrow2 <- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |> \n  plot(interp=F)\n\nim_list <- map_il(0:6, ~imrotate(im, # cubic interpolation\n                                 angle = 15*.x, \n                                 interpolation = 2)) \nrow1 <- imappend(im_list[1:4], axis = \"x\")  \nrow2 <- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |> \n  plot(interp=F) \n\n\n\n(a) Nearest Neighbour interpolation\n\n\n\n\n\n\n(b) Linear interpolation\n\n\n\n\n\n\n(c) Cubic interpolation\n\n\n\nRysunek 4.2: Przykład rotacji o wielokrotnośc \\(15\\degree\\)\n\n\nJak widać z powyższych wykresów rotacja wpływa na jakość obrazu, dlatego zaleca się nie stosować kilku występujących po sobie rotacji o kąty niebędące wielokrotnościami \\(90\\degree\\).\n\nKodim2 <- imlist(im)\nfor(i in 1:6){\n  im2[[i+1]] <- imrotate(im2[[i]], angle = 15)\n}\nimappend(im2, axis = \"x\") |> plot(interp=F)\n\n\n\nRysunek 4.3: Kilka iteracji rotacji obrazu. Każdy nastepny powstaje jako rotacja poprzedniego."
  },
  {
    "objectID": "transformations.html#zmiana-rozmiaru-obrazu",
    "href": "transformations.html#zmiana-rozmiaru-obrazu",
    "title": "\n4  Transformacje geometryczne\n",
    "section": "\n4.3 Zmiana rozmiaru obrazu",
    "text": "4.3 Zmiana rozmiaru obrazu\nW przypadku gdy zmieniamy rozmiar obrazu w proporcji 0.5, 2, 3 funkcja imresize korzysta z algorytmu opisanego na stronie http://www.scale2x.it/algorithm.html.\n\nKodim_list <- map_il(c(0.5,1, 2), ~imresize(im, \n                                         scale = .x)) \nimappend(im_list, \"x\") |>\n  plot(interp = F) # antialiasing off\n\n\n\nRysunek 4.4: Zmiana rozmiaru obrazu\n\n\n\n\nW innych przypadkach wykorzystuje jedną z 7 opcji interpolacji:\n\nbrak interpolacji (opcja -1);\nbrak interpolacji ale dodatkowo powstała przestrzeń jest wypełniana zgodnie z warunkiem brzegowym boundary_conditions (opcja 0);\ninterpolacja metodą najbliższego sąsiada (opcja 1);\ninterpolacja metodą średniej kroczącej (opcja 2);\ninterpolacja liniowa (opcja 3);\ninterpolacja na siatce (opcja 4);\ninterpolacja kubiczna (opcja 5);\ninterpolacja Lanczosa (opcja 6).\n\n\nKodim_list <- map_il(c(-1:6), ~imresize(im, \n                                     scale = 1.5,\n                                     interpolation = .x))\nrow1 <- imappend(im_list[1:4], \"x\")\nrow2 <- imappend(im_list[5:7], \"x\")\nimappend(list(row1, row2), \"y\") |>\n  plot(interp = F)\n\n\n\nRysunek 4.5: Zastosowania różnych interpolacji"
  },
  {
    "objectID": "point_trans.html",
    "href": "point_trans.html",
    "title": "\n5  Transformacje punktowe\n",
    "section": "",
    "text": "Transformacje obrazów, które odbywają się na poziomie pojedynczych pikseli nazywane są w literaturze tematu transformacjami punktowymi (ang. point transformation). Należą do nich między innymi:\n\nmodyfikacja kontrastu,\nmodyfikacja jasności,\nzmiana intensywności,\nodwracanie wartości piksela,\nkwantyzacja obrazów (ang. posterizing),\nprogowanie,\nkorekta gammy,\ntransformacje kolorów.\n\nWszystkie można opisać formułą\n\\[\ng(x) = h(f(x)),\n\\]\ngdzie \\(x = (i,j)\\) jest położeniem transformowanego piksela, \\(f\\) jest funkcja oryginalnego obrazu (przed przekształceniem)1, natomiast \\(h\\) jest zastosowaną transformacją. Wówczas \\(g\\) opisuje transformację jako funkcję lokalizacji.1 informuje o nasyceniu barw w danej lokalizacji - pikselu\nNa potrzeby zmian w kontraście, czy jasności stosuje się przekształcenia postaci:\n\\[\ng(x) = a(x)\\cdot f(x)+b(x),\n\\]\ngdzie \\(a(x)\\) jest parametrem zmiany kontrastu, a \\(b(x)\\) jest parametrem zmiany jasności2. Należy jednak pamiętać, że wartości \\(g(x)\\) tak określonej transformacji mogą znaleźć się poza przedziałem [0,255]. Oczywiście powoduje to problem, z którym można sobie radzić poprzez progowanie wartości, tak aby znalazły się w przedziale [0,255]. Obrazy zapisane w formacie cimg wartości poszczególnych kanałów mają znormalizowane do przedziału [0,1]. Dodatkowo należy pamiętać, że funkcja plot ma domyślnie włączoną flagę rescale=TRUE co oznacza, że wartości kanałów zostaną znormalizowane do przedziału [0,1] automatycznie. W rezultacie oznacza to, że transformacja liniowa \\(g(x)\\) nie odniesie żadnego skutku.2 jeśli zmieniamy te parametry globalnie (dla całego obrazu) wówczas funkcje te są stałe\n\nKodlibrary(imager)\nlibrary(imagerExtra)\nlibrary(tidyverse)\n\nadjust <- function(x, contrast, brithness){\n  \n  # transformacji dokonujemy od razu na wszystkich kanałach\n  x_trans <- x |> \n    as.data.frame() |> \n    mutate(value = ifelse(value*(contrast+1)+brithness>1, \n                          1, \n                          ifelse(value*(contrast+1)+brithness<0, 0,\n                                 value*(contrast+1)+brithness)), \n           .by  = cc) |> \n    as.cimg(dims = dim(x))\n  \n  return(x_trans)\n}\n\nlayout(t(1:2))\nplot(boats, rescale = F, interpolate = F)\nadjust(boats, 0.7, 0) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.1: Zmiana kontrastu obrazu (podbicie o 70%)\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats, rescale = F, interpolate = F)\nadjust(boats, -0.3, 0) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.2: Zmiana kontrastu obrazu (redukcja o 30%)\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nadjust(boats, 0, 0.2) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.3: Zmiana jasności obrazu (podbicie o 20% pełnej skali)\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nadjust(boats, 0, -0.4) |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.4: Zmiana jasności obrazu (redukcja o 40% pełnej skali)\n\n\n\n\nAby uniknąć przekraczania wartości dla poszczególnych kanałów wprowadza się często funkcję, która podnosi kontrast (tzw. autokontrast) przez poszerzenie spektrum wartości z obserwowanych \\([x_{\\min},x_{\\max}]\\) do przedziału \\([lower, upper]\\) (często przyjmowane jako [0,1]):\n\\[\ng(x) = x_{lower}+(x-x_{\\min})\\cdot\\frac{x_{\\max}-x_{\\min}}{x_{upper}-x_{lower}}.\n\\] Funkcja EqualizeDP pozwala na autokontrast. Dodatkowo umożliwia ustawić wartości skrajne dla spektrum kanału inne niż min i max.\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~EqualizeDP(.x, t_down = min(.x),t_up = max(.x), range = c(0,1))) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.5: Autokontrast w zakresie min, max\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~EqualizeDP(.x, t_down = 50,t_up = 170, range = c(0,1))) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.6: Autokontrast w zakresie 50, 170\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~{\n    max(.x)-.x\n  }) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.7: Przykład odwrócenia wartości pikseli\n\n\n\n\n\nKodlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~round(.x/0.2, digits = 0)) |> \n  imappend(\"c\") |> \n  plot(interpolate = F)\n\n\n\nRysunek 5.8: Przykład kwantyzacji obrazu\n\n\n\n\n\nKodlist(\"10%\",\"25%\", \"50%\", \"75%\", \"auto\") |> \n  map_il(~threshold(im = boats, thr = .x)) |> \n  imappend(\"x\") |> \n  plot()\n\n\n\nRysunek 5.9: Przykład progowania obrazu z różnymi poziomami progowania\n\n\n\n\n\nKod2^(-3:3) |> \n  map_il(~{\n    boats^.x\n  }) |> \n  imappend(\"x\") |> \n  plot()\n\n\n\nRysunek 5.10: Przykład korekty gamma dla różnych potęg\n\n\n\n\nTransformację kolorów możemy wykonywać na dwa sposoby:\n\nprzeprowadzając transformację oddzielnie na każdym kanale,\nzachowując barwę (ang. hue) obrazu, przetwarzać składową intensywności, a następnie obliczać wartości RGB z nowej składowej intensywności.\n\n\nKodlayout(t(1:2))\nplot(boats)\nboats |> \n  imsplit(\"c\") |> \n  map_il(~BalanceSimplest(.x, 2, 2, range = c(0,1))) |> \n  imappend(\"c\") |> \n  plot(rescale = F, interpolate = F)\n\n\n\nRysunek 5.11: Transformacja kolorów niezależnie modyfikując każdy kanał\n\n\n\n\nAby wykonać transformację drugą metodą zapiszemy plik w skali szarości, czyli dokonamy faktycznie agregacji postaci:\n\\[\ng_{grayscale}(x)=0.3R+0.59G+0.11B.\n\\]\nPonadto będziemy potrzebowali intensywności barw. Do zapisu nasycenia barw użyjemy funkcji GetHue pakietu imagerExtra.\n\nKod# zapisujemy obraz w skali szarości\ng <- grayscale(boats)\n# zapisujemy nasycenia barw\nhueim <- GetHue(boats)\n# transformujemy obraz w skali szarości\ng <- BalanceSimplest(g, 2, 2, range = c(0,1))\n# oddtwarzamy kolory\ny <- RestoreHue(g, hueim)\n\nlayout(t(1:2))\nplot(boats)\nplot(y)\n\n\n\nRysunek 5.12: Transformacja kolorów modyfikując kanały łącznie"
  },
  {
    "objectID": "filters.html#filtry-liniowe",
    "href": "filters.html#filtry-liniowe",
    "title": "\n6  Filtry\n",
    "section": "\n6.1 Filtry liniowe",
    "text": "6.1 Filtry liniowe\nFiltry liniowe są nazywane w ten sposób, ponieważ łączą wartości pikseli w otoczeniu w sposób liniowy, czyli jako suma ważona. Szczególnym przykładem jest omówiony na początku proces uśredniania lokalnego (Równanie 6.1), gdzie wszystkie dziewięć pikseli w lokalnym otoczeniu 3 × 3 jest dodawanych z identycznymi wagami (1/9). Dzięki temu samemu mechanizmowi można zdefiniować mnóstwo filtrów o różnych właściwościach, modyfikując po prostu rozkład poszczególnych wag.\n\\[\nI'(u,v) = \\frac19\\sum_{j = -1}^1\\sum_{i = -1}^1I(u+i,v+j),\n\\tag{6.1}\\]\nDla dowolnego filtra liniowego rozmiar i kształt regionu wsparcia (ang. support region), jak również wagi poszczególnych pikseli, są określone przez jądro filtra (ang. kernel) \\(H(i,j)\\). Rozmiar jądra \\(H\\) równa się rozmiarowi regionu filtrującego, a każdy element \\((i, j)\\) określa wagę odpowiedniego piksela w sumowaniu. Dla filtra wygładzającego 3x3 w równaniu (Równanie 6.1), jądro filtra to\n\\[\nH = \\begin{bmatrix}\n  1/9,&1/9,&1/9\\\\\n  1/9,&1/9,&1/9\\\\\n  1/9,&1/9,&1/9\n\\end{bmatrix}=\n\\frac19\\begin{bmatrix}\n  1,&1,&1\\\\\n  1,&1,&1\\\\\n  1,&1,&1\n\\end{bmatrix}\n\\]\nponieważ każda z wartości filtra wnosi 1/9 do piksela wynikowego.\nW istocie, jądro filtra \\(H(i, j)\\) jest, podobnie jak sam obraz, dyskretną, dwuwymiarową funkcją o rzeczywistą, \\(H : \\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{R}\\). Filtr ma swój własny układ współrzędnych z początkiem - często określanym jako hot spot - przeważnie (ale niekoniecznie) znajdującym się w środku. Tak więc współrzędne filtra są na ogół dodatnie i ujemne (Rysunek 6.2). Funkcja filtra ma nieskończony zakres i jest uważana za zerową poza obszarem zdefiniowanym przez macierz \\(H\\).\n\n\nRysunek 6.2: Schemat działania filtru\n\n\nDla filtru liniowego wynik jest jednoznacznie i całkowicie określony przez współczynniki jądra filtru. Zastosowanie filtru do obrazu jest prostym procesem, który został zilustrowany na Rysunek 6.2. W każdej pozycji obrazu \\((u, v)\\) wykonywane są następujące kroki:\n\nJądro filtra \\(H\\) jest przesuwane nad oryginalnym obrazem \\(I\\) tak, że jego początek \\(H(0, 0)\\) pokrywa się z aktualną pozycją obrazu \\((u, v)\\).\nWszystkie współczynniki filtra \\(H(i, j)\\) są mnożone z odpowiadającym im elementem obrazu \\(I(u+i,v+j)\\), a wyniki są sumowane.\nNa koniec otrzymana suma jest zapisywana w aktualnej pozycji w nowym obrazie \\(I'(u, v)\\).\n\nOpisując formalnie, wartości pikseli nowego obrazu \\(I'(u,v)\\) są obliczane przez operację\n\\[\nI'(u,v) = \\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(i,j),\n\\tag{6.2}\\]\ngdzie \\(R_H\\) oznacza zbiór współrzędnych pokrytych przez filtr \\(H\\). Nie całkiem dla wszystkich współrzędnych, aby być dokładnym. Istnieje oczywisty problem na granicach obrazu, gdzie filtr sięga poza obraz i nie znajduje odpowiadających mu wartości pikseli, które mógłby wykorzystać do obliczenia wyniku. Na razie ignorujemy ten problem granic, ale w dalszej części tego wykładu się tym zajmiemy.\n\n\nRysunek 6.3: Ilustracja działania filtru\n\n\nSkoro rozumiemy już zasadnicze działanie filtrów i wiemy, że granice wymagają szczególnej uwagi, możemy pójść dalej i zaprogramować prosty filtr liniowy. Zanim jednak to zrobimy, możemy chcieć rozważyć jeszcze jeden szczegół. W operacji punktowej każda nowa wartość piksela zależy tylko od odpowiadającej jej wartości piksela w oryginalnym obrazie, dlatego nie było problemu z zapisaniem wyników z powrotem do tego samego obrazu - obliczenia są wykonywane “w locie” bez potrzeby pośredniego przechowywania. Obliczenia w miejscu nie są generalnie możliwe dla filtra, ponieważ każdy oryginalny piksel przyczynia się do zmiany więcej niż jednego piksela wynikowego i dlatego nie może być zmodyfikowany przed zakończeniem wszystkich operacji.\nPotrzebujemy zatem dodatkowego miejsca na przechowywanie obrazu wynikowego, który następnie może być ponownie skopiowany do obrazu źródłowego (jeśli jest to pożądane). Tak więc kompletna operacja filtrowania może być zaimplementowana na dwa różne sposoby (Rysunek 6.4):\n\nWynik obliczeń filtra jest początkowo zapisywany w nowym obrazie, którego zawartość jest ostatecznie zapisywana z powrotem do obrazu oryginalnego.\nOryginalny obraz jest najpierw kopiowany do obrazu pośredniego, który służy jako źródło dla właściwej operacji filtrowania. Wynik zastępuje piksele w oryginalnym obrazie.\n\n\n\nRysunek 6.4: Dwa schematy implementacji filtrów do obrazów\n\n\nDla obu wersji wymagana jest taka sama ilość pamięci masowej, a więc żadna z nich nie oferuje szczególnej przewagi. W poniższych przykładach używamy na ogół wersji B.\nW filtrze prezentowanym powyżej wagi nie muszą być wszystkie takie same. Przykładowo filtr \\(H(u,v)\\) określony następująco\n\\[\nH(u,v)=\\begin{bmatrix}\n  0.075,&0.125,&0.075\\\\\n  0.125,&0.200,&0.125\\\\\n  0.075,&0.125,&0.075\n\\end{bmatrix}\n\\tag{6.3}\\]\nrównież uśrednia wartości w regionie wsparcia filtru ale nadając największe wagi wartościom w środku.\nZauważmy, że wagi filtra \\(H\\) są tak dobrane aby się sumowały do 1. Oznacza to, że filtr ten jest znormalizowany. Normalizacji filtrów używa się po to aby uniknąć sytuacji, w której wartość wyjściowa z filtra byłaby większa niż 255.\n\n\n\n\n\n\nWskazówka\n\n\n\nPonieważ funkcja plot pakietu imager ma włączoną opcję rescale = TRUE co oznacza, że wartości wynikowe i tak będą przekształcone do przedziału [0,1], to nie unormowane filtry i tak będą wyświetlać poprawnie przefiltrowane obrazy.\n\n\n\nKodlibrary(MASS)\nfilter1 <- matrix(c(1,1,1,\n                   1,1,1,\n                   1,1,1), \n                  ncol = 3)\nfilter1\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\nKodfilter2 <- filter1/9\nfilter2 |> \n  fractions() # aby ładnie wyświetlić ułamki\n\n     [,1] [,2] [,3]\n[1,] 1/9  1/9  1/9 \n[2,] 1/9  1/9  1/9 \n[3,] 1/9  1/9  1/9 \n\nKod# filtry w funkcji convolve muszą być zapisane jako obraz (as.cimg)\nlayout(t(1:2))\nconvolve(boats, as.cimg(filter1)) |> plot()\nconvolve(boats, as.cimg(filter2)) |> plot()\n\n\n\nRysunek 6.5: Przykład użycia filtra przed i po normalizacji\n\n\n\n\n\nKodtry(\n  convolve(boats, as.cimg(filter1)) |> \n  plot(rescale = FALSE)\n)\n\nError in colourscale(v[[1]], v[[2]], v[[3]]) : \n  color intensity 1.57913, not in [0,1]\n\n\nZatem możemy w konstruowaniu filtrów stosować wartości całkowite i wspólnej wartości normalizacyjnej.\n\\[\nH(u,v)=\\begin{bmatrix}\n  0.075,&0.125,&0.075\\\\\n  0.125,&0.200,&0.125\\\\\n  0.075,&0.125,&0.075\n\\end{bmatrix}=\n\\frac1{40}\\cdot\n\\begin{bmatrix}\n  3,&5,&3\\\\\n  5,&8,&5\\\\\n  3,&5,&3\n\\end{bmatrix}.\n\\] Aby uniknąć wartości ujemnych, które mogłyby się pojawić w przypadku gdy jądro filtra zawierałoby wartości ujemne, stosuje się stałą przesunięcia (ang. offset). Wówczas Równanie 6.2 przyjmuje postać\n\\[\nI'(u,v) = Offset+\\frac{1}{Scale}\\cdot \\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(i,j),\n\\] a \\(H(i,j)\\) jest zdefiniowany na \\(\\mathbb{Z}\\times\\mathbb{Z}\\). Chociaż najczęściej używa się filtrów kwadratowych, to nie ma przeszkód aby stosować również filtry prostokątne. Powyższa formuła ma zastosowanie do filtrów o dowolnym rozmiarze i kształcie.\n\n6.1.1 Przykładowe filtry wygładzające (ang. smoothing)\n\nFiltr pudełkowy (ang box), którego zasada działania została już przedstawiona w Równanie 6.1 dla rozmiaru filtra 3x3 jest jednym z filtrów wygładzających. Im większy jest rozmiar filtra, tym większy stopień wygładzenia obrazu wyjściowego. Ten najprostszy ze wszystkich filtrów wygładzających, którego kształt 3D przypomina pudełko (Rysunek 6.6 (a)), jest dobrze znany. Niestety, filtr pudełkowy jest daleki od optymalnego filtra wygładzającego ze względu na swoje dziwne zachowanie w przestrzeni częstotliwości, które jest spowodowane ostrym odcięciem wokół jego boków. Opisane w kategoriach częstotliwościowych wygładzanie odpowiada tzw. filtracji dolnoprzepustowej, czyli efektywnemu tłumieniu wszystkich składowych sygnału powyżej danej częstotliwości odcięcia. Filtr pudełkowy wytwarza jednak silne “dzwonienie” w przestrzeni częstotliwości i dlatego nie jest uważany za wysokiej jakości filtr wygładzający. Przypisanie tej samej wagi wszystkim pikselom obrazu w regionie filtru może też wydawać się dość doraźne. Zamiast tego należałoby prawdopodobnie oczekiwać, że silniejszy nacisk zostanie położony na piksele znajdujące się w pobliżu centrum filtra niż na te bardziej odległe. Ponadto filtry wygładzające powinny ewentualnie działać “izotropowo” (tzn. jednolicie w każdym kierunku), co z pewnością nie ma miejsca w przypadku filtra prostokątnego.\nFiltr gaussowski - z pewnością lepszy w tym kontekście wygładzania z względu na brak ostrych krawędzi jądra. Definiuje się go następująco \\[\nH^{G,\\sigma}(x,y) = e^{-\\frac{x^2+y^2}{2\\sigma^2}},\n\\] gdzie \\(\\sigma\\) oznacza odchylenie standardowe rozkładu.\n\n6.1.2 Przykład filtru różnicującego\nJeśli niektóre współczynniki filtra są ujemne, to obliczenie filtra można zinterpretować jako różnicę dwóch sum: suma ważona wszystkich pikseli z przypisanymi współczynnikami dodatnimi minus suma ważona pikseli z ujemnymi współczynnikami w regionie filtra RH , czyli\n\\[\n\\begin{align}\n  I'(u,v)=&\\sum_{(i,j)\\in R^+}I(u+i, v+j)\\cdot \\vert H(i,j)\\vert -\\\\\n-&\\sum_{(i,j)\\in R^-}I(u+i, v+j)\\cdot \\vert H(i,j)\\vert,\n\\end{align}\n\\tag{6.4}\\]\ngdzie \\(R^-, R^+\\) oznaczają podział filtra na współczynniki ujemne \\(H(i,j)<0\\) i dodatnie \\(H(i,j)>0\\) odpowiednio. Na przykład filtr Laplace’a 5x5 na Rysunek 6.6 (c) oblicza różnicę między pikselem środkowym (o wadze 16) a sumą ważoną 12 otaczających go pikseli (o wagach -1 lub -2). Pozostałe 12 pikseli ma przypisane zerowe współczynniki i dlatego są one ignorowane w obliczeniach. Podczas gdy lokalne zmiany intensywności są wygładzane przez uśrednianie, możemy oczekiwać, że w przypadku różnic stanie się dokładnie odwrotnie - lokalne zmiany intensywności zostaną wzmocnione.\n\n\nRysunek 6.6: Przykłady różnych filtrów o rozmiarze 5x5. (a) filtr pudełkowy, (b) gaussowski, (c) Laplace’a (zwany także Mexican Hut)\n\n\n\n6.1.3 Formalny zapis operatorów filtracji\nWspomniany zapis w równaniu Równanie 6.2 nazywany jest w literaturze operatorem korelacyjnym (ang. correlation operator). Ma on jedną poważną wadę, ponieważ filtr zastosowany do obrazu z pojedynczym wtrąceniem (jednym pikselem świecącym) w rezultacie daje w obrazie wynikowym wartości filtra zrotowane o \\(180\\degree\\) (patrz Rysunek 6.7).\n\n\nRysunek 6.7: Zastosowanie operatora korelacyjnego na obrazie z jednym wtrąceniem\n\n\nRozwiązaniem tej niedogodności jest wprowadzenie operatora konwolucyjnego (ang. convolution operator). Definiuje się go w następujący sposób:\n\\[\nI'(u,v) = \\sum_{i,j\\in R_H}I(u-i, v-j)\\cdot H(i,j),\n\\tag{6.5}\\]\nzapisywany również w bardziej zwartej formie\n\\[\nI'=I*H.\n\\tag{6.6}\\]\nAby pokazać związek pomiędzy oboma sposobami filtracji przekształćmy wzór Równanie 6.5\n\\[\n\\begin{align}\n  I'(u,v) =& \\sum_{i,j\\in R_H}I(u-i, v-j)\\cdot H(i,j)=\\\\\n  =&\\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(-i,-j)=\\\\\n  =&\\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H^*(i,j),\n\\end{align}\n\\tag{6.7}\\]\ngdzie \\(H^*(i,j)=H(-i,-j)\\). Zmiana parametryzacji powoduje obrócenie wyniku o \\(180\\degree\\).\n\n\nRysunek 6.8: Zastosowanie operatora konwolucyjnego do obrazu z jednym wtrąceniem\n\n\n\n6.1.4 Własności filtrów liniowych\nKonwolucja liniowa jest odpowiednim modelem dla wielu rodzajów zjawisk naturalnych, w tym układów mechanicznych, akustycznych i optycznych. W szczególności istnieją silne formalne powiązania z reprezentacją Fouriera sygnałów w dziedzinie częstotliwości, które są niezwykle cenne dla zrozumienia złożonych zjawisk, takich jak próbkowanie i aliasing. Poniżej przedstawione zostaną własności konwolucji liniowej.\n\n6.1.4.1 Przemienność\nKonwolucja liniowa jest przemienna, czyli dla dowolnego obrazu \\(I\\) i jądra filtru \\(H\\), zachodzi\n\\[\nI ∗ H = H ∗ I.\n\\tag{6.8}\\]\nWynik jest więc taki sam, jeśli obraz i jądro filtra są wzajemnie zamienione, i nie ma różnicy, czy składamy obraz \\(I\\) z jądrem \\(H\\), czy odwrotnie.\n\n6.1.4.2 Liniowość\nFiltry liniowe nazywane są tak ze względu na właściwości liniowości operacji konwolucji, która przejawia się w różnych aspektach. Na przykład, jeśli obraz jest mnożony przez skalar \\(s\\in\\mathbb{R},\\) to wynik konwolucji mnoży się o ten sam czynnik, czyli\n\\[\n(s\\cdot I)∗H = I ∗(s\\cdot H) = s\\cdot(I ∗H).\n\\tag{6.9}\\]\nPodobnie, jeśli dodamy dwa obrazy \\(I_1\\), \\(I_2\\) piksel po pikselu i spleciemy wynikowy obraz za pomocą pewnego jądra \\(H\\), to taki sam wynik uzyskamy splatając każdy obraz osobno i dodając potem oba wyniki, czyli\n\\[\n(I_1 +I_2)∗H = (I_1 ∗H)+(I_2 ∗H).\n\\tag{6.10}\\]\nZaskakujące może być jednak to, że samo dodanie do obrazu stałej wartości \\(b\\) nie powiększa wyniku splotu o taką samą ilość,\n\\[\n(b+I)∗H\\neq b+(I∗H),\n\\]\na więc nie jest częścią własności liniowości. Chociaż liniowość jest ważną własnością teoretyczną, należy zauważyć, że w praktyce filtry “liniowe” są często tylko częściowo liniowe z powodu błędów zaokrąglenia lub ograniczonego zakresu wartości wyjściowych.\n\n6.1.4.3 Łączność\nKonwolucja liniowa jest łączna, co oznacza, że kolejność operacji na filtrze nie ma znaczenia, czyli,\n\\[\n(I∗H_1)∗H_2 =I∗(H_1 ∗H_2).\n\\tag{6.11}\\]\nTak więc wiele filtrów może być zastosowanych w dowolnej kolejności, jak również wiele filtrów może być dowolnie łączonych w nowe filtry.\nBezpośrednią konsekwencją łączności jest rozdzielność filtrów liniowych. Jeżeli jądro konwolucyjne \\(H\\) można wyrazić jako złożenie wielu jąder \\(H_i\\) w postaci\n\\[\nH = H1 ∗ H2 ∗ . . . ∗ Hn,\n\\tag{6.12}\\]\nwówczas (jako konsekwencja Równanie 6.11) operacja filtru \\(I ∗ H\\) może być wykonana jako ciąg konwolucji z jądrami składowymi \\(H_i,\\)\n\\[\nI ∗ H = I ∗ (H_1 ∗ H_2 ∗ . . . ∗ H_n)\n= (\\ldots((I ∗H_1)∗H_2)∗\\ldots∗H_n).\n\\tag{6.13}\\]\nW zależności od rodzaju dekompozycji może to przynieść znaczne oszczędności obliczeniowe. O rozdzielczości filtrów możemy myśleć również nieco inaczej\n\\[\nI'=(I*h_x)*h_y,\n\\] gdzie \\(h_x, h_y\\) są filtrami 1D, które po wymnożeniu tworzą filtr o wymiarze \\(k\\times m\\). Przykładowo\n$$\n\\[\\begin{bmatrix}\n  1,&1,&1,&1,&1\\\\\n  1,&1,&1,&1,&1\\\\\n  1,&1,&1,&1,&1\n\\end{bmatrix}\\]\n= H = h_x*h_y=\n\\[\\begin{bmatrix}\n  1\\\\\n  1\\\\\n  1\n\\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n1,&1,&1,&1,&1\n\\end{bmatrix}\\]\n\n$${#eq-mnozenie}\nStosując tą własność rozłączności możemy również przedstawić filtr o jądrze gassuowskim 2D, za pomocą mnożenia filtrów gaussowskich 1D.\n\\[\nH^{G,\\sigma}=h^{G,\\sigma}_x*h^{G,\\sigma}_y,\n\\tag{6.14}\\]\ngdzie \\(h^{G,\\sigma}_x,h^{G,\\sigma}_y\\) są filtrami gaussowskimi 1D. Kolejność użytych filtrów ponownie nie ma znaczenia. Jeśli dla poszczególnych składowych odchylenia standardowe nie są równe, to filtr gaussowski 2D ma charakter eliptyczny.\nW algebrze filtrów liniowych istniej coś na kształt elementu neutralnego dla operacji splotu\n\\[\nI = \\delta*I,\n\\tag{6.15}\\]\ngdzie\n\\[\n\\delta(u,v)=\n\\begin{cases}\n  1, &\\text{ jeśli }u=v=0\\\\\n  0, &\\text{ w przeciwnym przypadku}\n\\end{cases}\n\\tag{6.16}\\]\nCo ciekawe filtr ten nie zmienia jedynie samego obrazu ale również innych filtrów\n\\[\nH=\\delta*H=H*\\delta.\n\\tag{6.17}\\]"
  },
  {
    "objectID": "filters.html#filtry-nieliniowe",
    "href": "filters.html#filtry-nieliniowe",
    "title": "\n6  Filtry\n",
    "section": "\n6.2 Filtry nieliniowe",
    "text": "6.2 Filtry nieliniowe\n\n6.2.1 Filtry minimum i maksimum\nJak wszystkie inne filtry, filtry nieliniowe obliczają wynik w danej pozycji obrazu \\((u,v)\\) z pikseli znajdujących się wewnątrz ruchomego regionu \\(R_{u,v}\\) oryginalnego obrazu. Filtry te nazywane są “nieliniowymi”, ponieważ wartości pikseli źródłowych są łączone przez jakąś funkcję nieliniową. Najprostszymi ze wszystkich filtrów nieliniowych są filtry minimum i maksimum, zdefiniowane jako\n\\[\n\\begin{align}\n  I'(u,v)=&\\min_{(i,j)\\in R}\\{I(u+i, v+j)\\}\\\\\n  I'(u,v)=&\\max_{(i,j)\\in R}\\{I(u+i, v+j)\\}\n\\end{align}\n\\tag{6.18}\\]\ngdzie \\(R\\) oznacza region filtra (zbiór współrzędnych filtra, zwykle kwadrat o rozmiarach 3x3 pikseli). Rysunek 6.10 ilustruje wpływ minimalnego filtra 1D na różne lokalne struktury sygnału.\n\n\nRysunek 6.9: Zastosowanie filtru minimum 1D do różnych sygnałów wejściowych. Górny rząd przedstawia oryginalny sygnał, a dolny spleciony z filtrem minimum\n\n\n\nKodkwiat <- load.image(file = \"~/kwiat.jpg\") |> grayscale()\n\nkwiat_noisy <- load.image(file = \"~/kwiat_salt_pepper.jpg\") |> grayscale()\n\nmin_filter <- function(im, radius) {\n  stencil <- expand.grid(dx = -radius:radius, dy = -radius:radius)\n  filtered <- matrix(0, nrow = height(im), ncol = width(im))\n  range_x <- (1 + radius):(width(im) - radius)\n  range_y <- (1 + radius):(height(im) - radius)\n  dt <- expand.grid(range_x, range_y)\n  dt$min <- apply(dt, 1, function(row) min(get.stencil(im, \n                                                       stencil, \n                                                       x = row[1],\n                                                       y = row[2])))\n\n  filtered <- dt$min |> \n    matrix(ncol = height(im)-2*radius,\n           nrow = width(im)-2*radius) |> \n    as.cimg()\n    \n  return(filtered)\n}\n\nkwiat_filtered <- min_filter(kwiat, radius = 2)\n\nlayout(t(1:2))\nkwiat |> plot(main = 'oryginał')\nkwiat_filtered |> plot(main = \"kwiat z filtrem minimum\")\n\n\n\nRysunek 6.10: Zastosowanie filtru minimum"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Barnard, Stephen T., and Martin A. Fischler. 1982. “Computational\nStereo.” ACM Computing Surveys 14 (4):\n553–72. https://doi.org/10.1145/356893.356896.\n\n\nBarrow, H. G., and J. M. Tenenbaum. 1981. “Computational\nVision.” Proceedings of the IEEE 69 (5): 572–95. https://doi.org/10.1109/PROC.1981.12026.\n\n\nBlake, Andrew, and Michael Isard. 2012. Active\nContours: The Application of\nTechniques from Graphics, Vision,\nControl Theory and Statistics to Visual\nTracking of Shapes in Motion.\nSpringer Science & Business Media.\n\n\nBlake, Andrew, Andrew Zisserman, and Greg Knowles. 1985. “Surface\nDescriptions from Stereo and Shading.” Image and Vision\nComputing, Papers from the 1985 Alvey Computer Vision\nand Image Interpretation Meeting, 3 (4): 183–91. https://doi.org/10.1016/0262-8856(85)90006-X.\n\n\nBurger, Wilhelm, and Mark J. Burge. 2016. Digital Image\nProcessing: An Algorithmic Introduction Using\nJava. Texts in Computer Science.\nLondon: Springer. https://doi.org/10.1007/978-1-4471-6684-9.\n\n\nCanny, John. 1986. “A Computational Approach to\nEdge Detection.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\nChollet, Francois, and J. J. Allaire. 2018. Deep\nLearning with R. Manning\nPublications.\n\n\n“Cooperative Computation of Stereo\nDisparity | Science.” n.d.\nhttps://www.science.org/doi/10.1126/science.968482.\n\n\n“Deep Learning with Python, Second\nEdition.” n.d. Manning Publications.\nhttps://www.manning.com/books/deep-learning-with-python-second-edition.\n\n\nDev, Parvati. 1975. “Perception of Depth Surfaces in Random-Dot\nStereograms : A Neural Model.” International Journal of\nMan-Machine Studies 7 (4): 511–28. https://doi.org/10.1016/S0020-7373(75)80030-7.\n\n\nFelzenszwalb, Pedro F., and Daniel P. Huttenlocher. 2005.\n“Pictorial Structures for Object\nRecognition.” International Journal of Computer\nVision 61 (1): 55–79. https://doi.org/10.1023/B:VISI.0000042934.15159.49.\n\n\nFergus, R., P. Perona, and A. Zisserman. 2007. “Weakly\nSupervised Scale-Invariant Learning of Models\nfor Visual Recognition.” International Journal\nof Computer Vision 71 (3): 273–303. https://doi.org/10.1007/s11263-006-8707-x.\n\n\nFischler, M., and O. Firschein. 1987. “Readings in Computer\nVision: Issues, Problems, Principles, and Paradigms.” In.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. Illustrated edition. Cambridge,\nMassachusetts: The MIT Press.\n\n\nHanson, Allen. 1978. Computer Vision Systems.\nElsevier.\n\n\nHorn, Berthold K P. n.d. “Obtaining Shape from\nShading Information.”\n\n\nKass, Michael, Andrew Witkin, and Demetri Terzopoulos. 1988.\n“Snakes: Active Contour Models.”\nInternational Journal of Computer Vision 1 (4): 321–31. https://doi.org/10.1007/BF00133570.\n\n\nKetkar, Nikhil, and Eder Santana. 2017. Deep Learning with\nPython. Vol. 1. Springer.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep\nLearning.” Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nMalladi, R., J. A. Sethian, and B. C. Vemuri. 1995. “Shape\nModeling with Front Propagation: A Level Set Approach.” IEEE\nTransactions on Pattern Analysis and Machine Intelligence 17 (2):\n158–75. https://doi.org/10.1109/34.368173.\n\n\n“Mind as Machine: A History of Cognitive Science.” 2007.\nChoice Reviews Online 44 (11). https://doi.org/10.5860/choice.44-6202.\n\n\nMundy, Joseph L., and Andrew Zisserman, eds. 1992. Geometric\nInvariance in Computer Vision. Cambridge, MA, USA:\nMIT Press.\n\n\nNalwa, Vishvjit S., and Thomas O. Binford. 1986. “On\nDetecting Edges.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence PAMI-8 (6): 699–714. https://doi.org/10.1109/TPAMI.1986.4767852.\n\n\nPonce, Jean, Martial Hebert, Cordelia Schmid, and Andrew Zisserman.\n2007. Toward Category-Level Object Recognition.\nSpringer.\n\n\nRoberts, Lawrence G. 1980. Machine Perception of Three-dimensional Solids. Garland\nPub.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015.\n“U-Net: Convolutional Networks for\nBiomedical Image Segmentation.” arXiv.\nhttps://doi.org/10.48550/arXiv.1505.04597.\n\n\nSzeliski, Richard. 2022. Computer Vision:\nAlgorithms and Applications. Texts in\nComputer Science. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-030-34372-9.\n\n\nWinston, Patrick Henry. 1976. “The Psychology of Computer\nVision.” Pattern Recognition 8 (3): 193. https://doi.org/10.1016/0031-3203(76)90020-0.\n\n\nWitkin, Andrew P. 1981. “Recovering Surface Shape and Orientation\nfrom Texture.” Artificial Intelligence 17 (1): 17–45. https://doi.org/10.1016/0004-3702(81)90019-9.\n\n\nWoodham, Robert J. 1981. “Analysing Images of Curved\nSurfaces.” Artificial Intelligence 17 (1): 117–40. https://doi.org/10.1016/0004-3702(81)90022-9.\n\n\nZhou, Zongwei, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and\nJianming Liang. 2018. “UNet++: A Nested U-Net\nArchitecture for Medical Image Segmentation.”\narXiv. https://doi.org/10.48550/arXiv.1807.10165."
  }
]