[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automatyczna analiza obrazu",
    "section": "",
    "text": "Wstęp\nNiniejsza książka powstała na potrzeby prowadzenia wykładu z Automatycznej analizy obrazu. Jest wynikiem moich doświadczeń z automatyczną analizą obrazu. W pisaniu tego kompendium wiedzy na temat Computer Vision bardzo pomocne były dwie pozycje literaturowe (Burger i Burge 2016; Szeliski 2022). Oprócz wspomnianych książek poświęconych wizji komputerowej, ważne są również pozycje objaśniające arkana deep learning. Wśród nich należy wymienić (Goodfellow, Bengio, i Courville 2016; Ketkar i Santana 2017; „Deep Learning with Python, Second Edition”, b.d.; Chollet i Allaire 2018). Ponadto zostaną wykorzystane nieprzebrane zasoby internetu - na stronach takich jak https://stackoverflow.com czy https://github.com można znaleźć rozwiązania do niemal każdego zadania.\nNa potrzeby zajęć laboratoryjnych będą dodatkowo potrzebne pewne programy komputerowe i biblioteki:\n\nFiji - darmowy program będący nakładką na program ImageJ. Służy on do operacji na zdjęciach. Do pobrania ze strony https://fiji.sc/;\nPython - język programowania, w którym można wykonać niemal dowolne zadanie z zakresu automatycznej analizy obrazu. Przez instalacje Python-a rozumiem zainstalowanie odpowiedniej dystrybucji tego programu (np. dla Windows zaleca się instalację dystrybucji Anconda lub Miniconda). Po szczegóły dotyczące instalacji Pythona na Windows odsyłam na stronę https://support.posit.co/hc/en-us/articles/1500007929061-Using-Python-with-the-RStudio-IDE;\nPo zainstalowaniu Pythona, trzeba też zainstalować dwie bardzo ważne biblioteki pythonowe do budowania i uczenia sieci głębokiego uczenia:\n\ntensorflow - jest end-to-end platformą typu open-source do uczenia maszynowego. Jest to kompleksowy i elastyczny ekosystem narzędzi, bibliotek i innych zasobów, które zapewniają przepływy pracy z wysokopoziomowymi interfejsami API. Ramy oferują różne poziomy koncepcji, abyś mógł wybrać ten, którego potrzebujesz do budowania i wdrażania modeli uczenia maszynowego;\nkeras - jest wysokopoziomową biblioteką do budowy sieci neuronowych, która działa na bazie TensorFlow, CNTK i Theano. Wykorzystanie Keras w deep learningu pozwala na łatwe i szybkie prototypowanie, a także płynne działanie na CPU i GPU. Aby zainstalować zarówno tensorflow, jak i keras z obsługo CPU lub GPU polecam instrukcję w filmie https://youtu.be/PnK1jO2kXOQ;\n\nOpenCV - jest biblioteką (ale nie programu R) funkcji programistycznych skierowanych głównie do wizji komputerowej czasu rzeczywistego. Instalację w Windows można znaleźć pod adresem https://docs.opencv.org/4.x/d3/d52/tutorial_windows_install.html;\nBiblioteki R-owe potrzebne do budowy modeli i obsługi obrazów, to:\n\nreticulate - biblioteka pozwalająca na wykorzystanie bibliotek i funkcji Python-a w R;\nmagick - biblioteka potrzebna do różnego rodzaju transformacji obrazów;\ntensorflow - biblioteka R-owa pozwalająca na wykorzystanie funkcji tensorflow Pythona;\nkeras - biblioteka R-owa pozwalająca na korzystanie z funkcji pakietu keras Pythonowego.\n\n\n\n\n\n\nBurger, Wilhelm, i Mark J. Burge. 2016. Digital Image Processing: An Algorithmic Introduction Using Java. Texts w Computer Science. London: Springer. https://doi.org/10.1007/978-1-4471-6684-9.\n\n\nChollet, Francois, i J. J. Allaire. 2018. Deep Learning with R. Manning Publications.\n\n\n„Deep Learning with Python, Second Edition”. b.d. Manning Publications. https://www.manning.com/books/deep-learning-with-python-second-edition.\n\n\nGoodfellow, Ian, Yoshua Bengio, i Aaron Courville. 2016. Deep Learning. Illustrated edition. Cambridge, Massachusetts: The MIT Press.\n\n\nKetkar, Nikhil, i Eder Santana. 2017. Deep Learning with Python. T. 1. Springer.\n\n\nSzeliski, Richard. 2022. Computer Vision: Algorithms and Applications. Texts w Computer Science. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-34372-9."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Automatyczna analiza obrazu jest znana również pod inną nazwą wizja komputerowa (ang. Computer Vision). Można powiedzieć też, że AAO1 jest częścią jeszcze szerszej dziedziny automatycznej analizy sygnałów. Ponieważ różnice pomiędzy analizą obrazu i dźwięku w niektórych zadaniach będą się zacierać, to poznane metody w toku tego wykładu będzie można śmiało przenieść na inne dziedziny. Oczywiście uwzględniając szereg podobieństw pomiędzy analizą obrazu i analizą dźwięku, istniej wiele dedykowanych modeli stosowanych tylko w domenie fal dźwiękowych.1 skrót od Automatyczna Analiza Obrazu\n\n\n\n\n\nW ramach zadań realizowanych przez wizję komputerową można wymienić:\n\npozyskiwanie obrazów (opis procesu “robienia zdjęcia” cyfrowego);\nprzetwarzanie obrazów w celu zmiany ich parametrów (np. poprawy ostrości, usuwania szumów, itp);\nanalizowania zdjęć w celu poszukiwania wzorców:\n\nzastosowanie ML2 do klasyfikacji obiektów na zdjęciach;\nzastosowanie ML do zadań regresyjnych (np. wyznaczanie poziomu wylania na podstawie zdjęć satelitarnych);\nzastosowanie ML w lokalizacji obiektów na obrazie (np. wskazanie położenia samolotu na zdjęciu w postaci ujęcia go w prostokątną ramkę);\nautoidentyfikacja (np. rozpoznawanie twarzy czy odcisku palca);\ntworzenie obrazów na podstawie fraz (istnieją sieci np. GAN, które są w stanie wygenerować całkowicie fikcyjny obraz na podstawie zdania opisującego co ma się na nim znaleźć);\nśledzenie ruchów na podstawie obrazu wideo (np. automatyczne kadrowanie obrazu wideo na podstawie położenia twarzy podczas rozmowy przez komunikator);\nsegmentacja obrazu;\ni wiele innych\n\n\n2 Machine LearningŚmiało można stwierdzić, że AAO towarzyszy nam codziennie i na każdym kroku. Czasami nie jesteśmy nawet tego świadomi.\n\nPoniżej przedstawiam listę wybranych zastosowań AAO:\n\nTransport - Rosnące wymagania sektora transportowego napędzają rozwój technologiczny w tej branży, w którego centrum znajduje się wizja komputerowa. Od pojazdów autonomicznych po wykrywanie zajętości miejsc parkingowych, Inteligentny System Transportowy (ITS) stał się krytycznym obszarem promowania wydajności, efektywności i bezpieczeństwa transportu.\n\n\n\n\n\nMonitorowanie zajętości miejsc na parkingu\n\n\n\nMedycyna. Dane z obrazowania medycznego są jednym z najbogatszych źródeł informacji. Bez odpowiedniej technologii lekarze są zmuszeni spędzać godziny na ręcznym analizowaniu danych pacjentów i wykonywaniu prac administracyjnych. Na szczęście, wraz z upływem lat i rozwojem technologii, branża opieki zdrowotnej stała się jedną z najszybciej przyjmujących nowe rozwiązania automatyzacji, w tym wizję komputerową.\n\n\n\n\n\nSegmentacja obrazu MRI\n\n\n\nProdukcja. Przemysł produkcyjny przyjął już szeroką gamę rozwiązań automatyzacji z wizją komputerową w centrum. Pomaga ona zautomatyzować kontrolę jakości, zminimalizować zagrożenia bezpieczeństwa i zwiększyć wydajność produkcji. Oto niektóre z najczęstszych zastosowań wizji komputerowej w przemyśle produkcyjnym.\n\n\n\n\nAAO w kontroli jakości\n\n\n\n\n\nAAO w procesie magazynowania\n\n\n\nBudowa. Sektor budowlany szybko przyjmuje technologię wizji komputerowej i wykorzystuje ją do wykrywania sprzętu ochrony osobistej, kontroli aktywów infrastruktury, wykrywania zagrożeń w miejscu pracy lub konserwacji.\n\n\n\n\nWykrywanie zużytych części\n\n\n\n\n\nNaruszenia procedur bezpieczeństwa\n\n\n\nRolnictwo. Sektor rolniczy był świadkiem kilku przypadków zastosowania modeli sztucznej inteligencji (w tym wizji komputerowej) w takich dziedzinach, jak monitorowanie upraw i plonów, zautomatyzowane zbiory, analiza warunków pogodowych, monitorowanie zdrowia zwierząt gospodarskich czy wykrywanie chorób roślin. Technologia ta zdobyła już silną pozycję dzięki możliwościom automatyzacji i wykrywania, a jej zastosowania będą się tylko rozszerzać.\n\n\n\n\nWyrywanie nietypowych zachowań zwierząt\n\n\n\n\n\nChoroby roślin\n\n\n\nSprzedaż detaliczna. Kamery zainstalowane w sklepach detalicznych pozwalają sprzedawcom zbierać duże ilości danych wizualnych pomocnych w projektowaniu lepszych doświadczeń klientów i pracowników. Rozwój systemów wizji komputerowej do przetwarzania tych danych sprawia, że cyfrowa transformacja branży realnej staje się znacznie bardziej osiągalna.\n\n\n\n\nWykrywanie braków towaru\n\n\n\n\n\nWykrywanie nietypowych zachowań klientów lub badanie zatłoczenia\n\n\nPrzestrzeni do zastosowań AAO jest jeszcze dużo więcej ale nie sposób ich wszystkich opisać. Oto kilka przykładów z różnych kategorii.\n\n\n\nBadanie ruchu zawodnika\n\n\n\n\n\nWykrywanie naruszeń prawa\n\n\n\n\n\nWykrywanie twarzy\n\n\nMoże też służyć do zabawy\n\n\n\nKilka obrazów wygenerowanych jako wariacje na temat mojego zdjęcia z wakacji\n\n\n\n\n\nKilka przykładów obrazów wygenerowanych przez sieć DALL E2 jako odpowiedź na zdanie “narysuj bez odrywania ręki jedna linią misia na zakupach”\n\n\n\n\n\nTym razem sieć DALL E2 została poproszona o namalowanie kobiet w stylu Van Gogha"
  },
  {
    "objectID": "history.html#lata-70",
    "href": "history.html#lata-70",
    "title": "2  Historia wizji komputerowej",
    "section": "2.1 Lata ’70",
    "text": "2.1 Lata ’70\nKiedy wizja komputerowa po raz pierwszy pojawiła się na początku lat siedemdziesiątych, była postrzegana jako wizualny komponent percepcji ambitnego programu naśladowania ludzkiej inteligencji i obdarzenia robotów inteligentnym zachowaniem. W tym czasie niektórzy z pionierów sztucznej inteligencji i robotyki (w miejscach takich jak MIT, Stanford) wierzyli, że rozwiązanie problemu “wejścia wizualnego” będzie łatwym krokiem na drodze do rozwiązania trudniejszych problemów, takich jak rozumowanie na wyższym poziomie i planowanie. Według jednej ze znanych historii, w 1966 roku Marvin Minsky z MIT poprosił swojego studenta Geralda Jay Sussmana o “spędzenie lata na podłączeniu kamery do komputera i nakłonieniu komputera do opisania tego, co widział”. Obecnie wiemy, że problem jest nieco trudniejszy niż wówczas się wydawało.\nTym, co odróżniało widzenie komputerowe od istniejącej już dziedziny cyfrowej obróbki obrazów, była chęć odzyskania trójwymiarowej struktury świata z obrazów i wykorzystania tego jako kroku w kierunku pełnego zrozumienia prezentowanej sceny. Winston (1976) oraz Hanson (1978) dostarczają dwóch ładnych zbiorów klasycznych prac z tego wczesnego okresu. Wczesne próby zrozumienia sceny polegały na wyodrębnieniu krawędzi, a następnie wnioskowaniu o strukturze 3D obiektu lub “świata bloków” z topologicznej struktury linii 2D Roberts (1980).\nJakościowe podejście do rozumienia intensywności i zmienności cieniowania oraz wyjaśniania ich przez efekty zjawisk formowania się obrazu, takich jak orientacja powierzchni i cienie, zostało spopularyzowane przez Barrow i Tenenbaum (1981) w ich pracy na temat obrazów wewnętrznych. W tym czasie opracowano również bardziej ilościowe podejścia do wizji komputerowej, w tym pierwszy z wielu opartych na cechach algorytmów korespondencji stereo (Dev 1975; „Cooperative Computation of Stereo Disparity | Science”, b.d.; Barnard i Fischler 1982)."
  },
  {
    "objectID": "history.html#lata-80",
    "href": "history.html#lata-80",
    "title": "2  Historia wizji komputerowej",
    "section": "2.2 Lata ’80",
    "text": "2.2 Lata ’80\nW latach ’80 ubiegłego wieku wiele uwagi poświęcono bardziej wyrafinowanym technikom matematycznym służącym do przeprowadzania ilościowej analizy obrazów i scen. Piramidy obrazów zaczęły być powszechnie stosowane do wykonywania zadań takich jak mieszanie obrazów i wyszukiwanie korespondencji coarse-to-fine. Wykorzystanie stereo jako ilościowej wskazówki kształtu zostało rozszerzone o szeroką gamę technik shape-from-X, w tym shape from shading (Horn, b.d.; Blake, Zisserman, i Knowles 1985).\n\n\n\nRysunek 2.2: Przykład wykorzystania techniki piramid blending\n\n\nW tym okresie prowadzono również badania nad lepszym wykrywaniem krawędzi i konturów (Canny 1986; Nalwa i Binford 1986), stereografii fotometrycznej (Woodham 1981) oraz kształty z tekstur (Witkin 1981), w tym wprowadzono dynamicznie ewoluujące trackery konturów, takie jak węże, a także trójwymiarowe modele oparte na fizyce. Naukowcy zauważyli, że wiele algorytmów detekcji stereoskopowej, przepływu, shape-from-X i krawędzi może być zunifikowanych lub przynajmniej opisanych przy użyciu tych samych ram matematycznych, jeśli zostaną one postawione jako problemy optymalizacji wariacyjnej i uodpornione (dobrze postawione) przy użyciu regularyzacji.\nNieco później wprowadzono warianty on-line algorytmów MRF (ang. Markov Random Field), które modelowały i aktualizowały niepewności za pomocą filtru Kalmana. Podjęto również próby odwzorowania zarówno algorytmów regularyzowanych jak i MRF na sprzęt zrównoleglony (ang. parallel). Książka (Fischler i Firschein 1987) zawiera zbiór artykułów skupiających się na wszystkich tych tematach (stereo, przepływ, regularność, MRF, a nawet widzenie wyższego poziomu)."
  },
  {
    "objectID": "history.html#lata-90",
    "href": "history.html#lata-90",
    "title": "2  Historia wizji komputerowej",
    "section": "2.3 Lata ’90",
    "text": "2.3 Lata ’90\nPodczas gdy wiele z wcześniej wymienionych tematów było nadal eksplorowanych, kilka z nich stało się znacznie bardziej aktywnych. Nagły wzrost aktywności w zakresie wykorzystania niezmienników projekcyjnych do celów rozpoznania ruchu (Mundy i Zisserman 1992) przerodził się w skoordynowane wysiłki zmierzające do rozwiązania problemu structure from motion. Wiele początkowych działań skierowanych było na rekonstrukcje rzutowe, które nie wymagają kalibracji kamery. Równolegle, techniki faktoryzacji zostały opracowane w celu efektywnego rozwiązywania problemów, dla których miały zastosowanie przybliżenia ortograficzne kamery, a następnie rozszerzone na przypadek perspektywiczny.\nW końcu zaczęto stosować pełną optymalizację globalną, która później została uznana za tożsamą z technikami dopasowania wiązki, tradycyjnie stosowanymi w fotogrametrii. W pełni zautomatyzowane systemy modelowania 3D zostały zbudowane przy użyciu tych technik.\nPrace rozpoczęte w latach 80-tych nad wykorzystaniem szczegółowych pomiarów barwy i natężenia światła w połączeniu z dokładnymi modelami fizycznymi transportu promieniowania i tworzenia kolorowych obrazów stworzyły własną dziedzinę znaną jako widzenie oparte na fizyce. Algorytmy stereo na podstawie wielu obrazów, które tworzą kompletne powierzchnie 3D były również aktywnym tematem badań, który jest aktualny do dziś.\nAlgorytmy śledzenia również uległy dużej poprawie, w tym śledzenie konturów z wykorzystaniem aktywnych konturów, takich jak węże (Kass, Witkin, i Terzopoulos 1988), filtry cząsteczkowe (Blake i Isard 2012) i zbiorów poziomicowych (ang. level set) (Malladi, Sethian, i Vemuri 1995), a także techniki oparte na intensywności (bezpośrednie), często stosowane do śledzenia twarzy.\n\n\n\nRysunek 2.3: Przykład śledzenia twarzy przez algorytm\n\n\nSegmentacja obrazów, temat, który jest aktywny od początku wizji komputerowej, był również aktywnym tematem badań, produkując techniki oparte na minimalnej energii i minimalnej długości opisu, znormalizowanych cięciach i średnim przesunięciu.\nZaczęły pojawiać się techniki uczenia statystycznego, najpierw w zastosowaniu analizy składowych głównych, eigenface do rozpoznawania twarzy oraz liniowych systemów dynamicznych do śledzenia krzywych.\nByć może najbardziej zauważalnym rozwojem w dziedzinie widzenia komputerowego w tej dekadzie była zwiększona interakcja z grafiką komputerową, zwłaszcza w interdyscyplinarnym obszarze modelowania i renderowania opartego na obrazach. Pomysł manipulowania obrazami świata rzeczywistego bezpośrednio w celu tworzenia nowych animacji po raz pierwszy stał się znany dzięki technikom morfingu obrazu."
  },
  {
    "objectID": "history.html#lata-00",
    "href": "history.html#lata-00",
    "title": "2  Historia wizji komputerowej",
    "section": "2.4 Lata ’00",
    "text": "2.4 Lata ’00\nTa dekada kontynuowała pogłębianie interakcji pomiędzy dziedzinami wizji i grafiki, ale co ważniejsze, przyjęła podejścia oparte na danych i uczeniu się jako kluczowe komponenty wizji. Wiele z tematów wprowadzonych w rubryce renderingu opartego na obrazie, takich jak zszywanie obrazów, przechwytywanie i renderowanie pola świetlnego oraz przechwytywanie obrazów o wysokim zakresie dynamicznym (HDR) poprzez bracketing ekspozycji, zostało ponownie ochrzczonych mianem fotografii obliczeniowej, aby potwierdzić zwiększone wykorzystanie takich technik w codziennej fotografii cyfrowej. Na przykład, szybkie przyjęcie bracketingu ekspozycji do tworzenia obrazów o wysokim zakresie dynamicznym wymagało opracowania algorytmów kompresji dynamiki, aby przekształcić takie obrazy z powrotem do wyników możliwych do wyświetlenia. Oprócz łączenia wielu ekspozycji, opracowano techniki łączenia obrazów z lampą błyskową z ich odpowiednikami bez lampy błyskowej.\n\n\n\nRysunek 2.4: Przykład rozpoznawania obiektów\n\n\nDrugim wartym uwagi trendem w tej dekadzie było pojawienie się technik opartych na cechach (połączonych z uczeniem) do rozpoznawania obiektów. Niektóre z godnych uwagi prac w tej dziedzinie obejmują model konstelacji (Ponce i in. 2007; Fergus, Perona, i Zisserman 2007) oraz struktury obrazowe (Felzenszwalb i Huttenlocher 2005). Techniki oparte na cechach dominują również w innych zadaniach rozpoznawania, takich jak rozpoznawanie scen, panoram i lokalizacji. I chociaż cechy oparte na punktach kluczowych (patch-based) dominują w obecnych badaniach, niektóre grupy zajmują się rozpoznawaniem na podstawie konturów i segmentacji regionów.\nInnym istotnym trendem tej dekady było opracowanie bardziej wydajnych algorytmów dla złożonych problemów optymalizacji globalnej. Chociaż trend ten rozpoczął się od prac nad cięciami grafów, duży postęp dokonał się również w algorytmach przekazywania inforamcji, takich jak loopy belief propagation (LBP).\nNajbardziej zauważalnym trendem tej dekady, który do tej pory całkowicie opanował rozpoznawanie obrazu i większość innych aspektów widzenia komputerowego, było zastosowanie zaawansowanych technik uczenia maszynowego do problemów widzenia komputerowego. Trend ten zbiegł się w czasie ze zwiększoną dostępnością ogromnych ilości częściowo oznakowanych danych w Internecie, a także ze znacznym wzrostem mocy obliczeniowej, co sprawiło, że uczenie się kategorii obiektów bez użycia starannego nadzoru człowieka stało się bardziej realne."
  },
  {
    "objectID": "history.html#lata-10",
    "href": "history.html#lata-10",
    "title": "2  Historia wizji komputerowej",
    "section": "2.5 Lata ’10",
    "text": "2.5 Lata ’10\nTrend do wykorzystywania dużych etykietowanych zbiorów danych do rozwoju algorytmów uczenia maszynowego stał się falą, która całkowicie zrewolucjonizowała rozwój algorytmów rozpoznawania obrazów, a także innych aplikacji, takich jak denoising i przepływ optyczny, które wcześniej wykorzystywały techniki Bayesa i optymalizacji globalnej. Tendencję tę umożliwił rozwój wysokiej jakości wielkoskalowych anotowanych zbiorów danych, takich jak ImageNet, Microsoft COCO i LVIS. Te zbiory danych dostarczyły nie tylko wiarygodnych metryk do śledzenia postępów algorytmów rozpoznawania i segmentacji semantycznej, ale co ważniejsze, wystarczającej ilości etykietowanych danych do opracowania kompletnych rozwiązań opartych na uczeniu maszynowym.\nInnym ważnym trendem był dramatyczny wzrost mocy obliczeniowej dostępny dzięki rozwojowi algorytmów ogólnego przeznaczenia (data-parallel) na jednostkach przetwarzania graficznego (GPGPU). Przełomowa głęboka sieć neuronowa SuperVision (“AlexNet”), która jako pierwsza wygrała coroczne zawody w rozpoznawaniu obrazów na dużą skalę ImageNet, opierała się na treningu na GPU, a także na szeregu usprawnień technicznych, które przyczyniły się dramatycznie do wzrostu jej wydajności. Po opublikowaniu tej pracy postęp w wykorzystaniu głębokich architektur konwolucyjnych gwałtownie przyspieszył, do tego stopnia, że obecnie są one jedyną architekturą braną pod uwagę w zadaniach rozpoznawania i segmentacji semantycznej, a także preferowaną architekturą w wielu innych zadaniach wizyjnych, w tym w zadaniach przepływu optycznego, denoisingu i wnioskowania o głębi monokularnej (LeCun, Bengio, i Hinton 2015).\nDuże zbiory danych i architektury GPU, w połączeniu z szybkim upowszechnianiem pomysłów poprzez pojawiające się w odpowiednim czasie publikacje na arXiv, a także rozwój języków głębokiego uczenia i otwarty dostęp do modeli sieci neuronowych, przyczyniły się do gwałtownego rozwoju tej dziedziny, zarówno pod względem szybkich postępów i możliwości, jak i samej liczby publikacji i badaczy zajmujących się obecnie tymi tematami. Umożliwiły one również rozszerzenie podejść do rozpoznawania obrazów na zadania związane z rozumieniem wideo, takie jak rozpoznawanie akcji, a także zadania regresji strukturalnej, takie jak estymacja w czasie rzeczywistym wieloosobowej pozy ciała.\nSpecjalistyczne czujniki i sprzęt do zadań związanych z widzeniem komputerowym również stale się rozwijały. Wprowadzona w 2010 r. kamera głębi Microsoft Kinect szybko stała się podstawowym elementem wielu systemów modelowania 3D i śledzenia osób. W ciągu dekady systemy modelowania i śledzenia kształtu ciała 3D nadal się rozwijały, do tego stopnia, że obecnie możliwe jest wnioskowanie o modelu 3D osoby wraz z gestami i ekspresją na podstawie jednego obrazu.\n\n\n\n\nBarnard, Stephen T., i Martin A. Fischler. 1982. „Computational Stereo”. ACM Computing Surveys 14 (4): 553–72. https://doi.org/10.1145/356893.356896.\n\n\nBarrow, H. G., i J. M. Tenenbaum. 1981. „Computational Vision”. Proceedings of the IEEE 69 (5): 572–95. https://doi.org/10.1109/PROC.1981.12026.\n\n\nBlake, Andrew, i Michael Isard. 2012. Active Contours: The Application of Techniques from Graphics, Vision, Control Theory and Statistics to Visual Tracking of Shapes in Motion. Springer Science & Business Media.\n\n\nBlake, Andrew, Andrew Zisserman, i Greg Knowles. 1985. „Surface Descriptions from Stereo and Shading”. Image and Vision Computing, Papers from the 1985 Alvey Computer Vision i Image Interpretation Meeting, 3 (4): 183–91. https://doi.org/10.1016/0262-8856(85)90006-X.\n\n\nCanny, John. 1986. „A Computational Approach to Edge Detection”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\n„Cooperative Computation of Stereo Disparity | Science”. b.d. https://www.science.org/doi/10.1126/science.968482.\n\n\nDev, Parvati. 1975. „Perception of Depth Surfaces in Random-Dot Stereograms : A Neural Model”. International Journal of Man-Machine Studies 7 (4): 511–28. https://doi.org/10.1016/S0020-7373(75)80030-7.\n\n\nFelzenszwalb, Pedro F., i Daniel P. Huttenlocher. 2005. „Pictorial Structures for Object Recognition”. International Journal of Computer Vision 61 (1): 55–79. https://doi.org/10.1023/B:VISI.0000042934.15159.49.\n\n\nFergus, R., P. Perona, i A. Zisserman. 2007. „Weakly Supervised Scale-Invariant Learning of Models for Visual Recognition”. International Journal of Computer Vision 71 (3): 273–303. https://doi.org/10.1007/s11263-006-8707-x.\n\n\nFischler, M., i O. Firschein. 1987. „Readings in Computer Vision: Issues, Problems, Principles, and Paradigms”. W.\n\n\nHanson, Allen. 1978. Computer Vision Systems. Elsevier.\n\n\nHorn, Berthold K P. b.d. „Obtaining Shape from Shading Information”.\n\n\nKass, Michael, Andrew Witkin, i Demetri Terzopoulos. 1988. „Snakes: Active Contour Models”. International Journal of Computer Vision 1 (4): 321–31. https://doi.org/10.1007/BF00133570.\n\n\nLeCun, Yann, Yoshua Bengio, i Geoffrey Hinton. 2015. „Deep Learning”. Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nMalladi, R., J. A. Sethian, i B. C. Vemuri. 1995. „Shape Modeling with Front Propagation: A Level Set Approach”. IEEE Transactions on Pattern Analysis and Machine Intelligence 17 (2): 158–75. https://doi.org/10.1109/34.368173.\n\n\n„Mind as Machine: A History of Cognitive Science”. 2007. Choice Reviews Online 44 (11). https://doi.org/10.5860/choice.44-6202.\n\n\nMundy, Joseph L., i Andrew Zisserman, red. 1992. Geometric Invariance in Computer Vision. Cambridge, MA, USA: MIT Press.\n\n\nNalwa, Vishvjit S., i Thomas O. Binford. 1986. „On Detecting Edges”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 699–714. https://doi.org/10.1109/TPAMI.1986.4767852.\n\n\nPonce, Jean, Martial Hebert, Cordelia Schmid, i Andrew Zisserman. 2007. Toward Category-Level Object Recognition. Springer.\n\n\nRoberts, Lawrence G. 1980. Machine Perception of Three-Dimensional Solids. Garland Pub.\n\n\nWinston, Patrick Henry. 1976. „The Psychology of Computer Vision”. Pattern Recognition 8 (3): 193. https://doi.org/10.1016/0031-3203(76)90020-0.\n\n\nWitkin, Andrew P. 1981. „Recovering Surface Shape and Orientation from Texture”. Artificial Intelligence 17 (1): 17–45. https://doi.org/10.1016/0004-3702(81)90019-9.\n\n\nWoodham, Robert J. 1981. „Analysing Images of Curved Surfaces”. Artificial Intelligence 17 (1): 117–40. https://doi.org/10.1016/0004-3702(81)90022-9."
  },
  {
    "objectID": "digt_img.html#skale",
    "href": "digt_img.html#skale",
    "title": "3  Obrazy cyfrowe",
    "section": "3.1 Skale",
    "text": "3.1 Skale\n\nSkala szarości - dane obrazu w skali szarości składają się z pojedynczego kanału (ang. channel), który reprezentuje intensywność, jasność lub gęstość obrazu. W większości przypadków sens mają tylko wartości dodatnie, ponieważ liczby reprezentują natężenie energii świetlnej lub gęstość filmu, a więc nie mogą być ujemne, więc zwykle używa się całych liczb całkowitych z zakresu \\(0, \\ldots , 2^{k - 1}\\) są używane. Na przykład typowy obraz w skali szarości wykorzystuje \\(k = 8\\) bitów (1 bajt) na piksel i wartości intensywności z zakresu \\(0,\\ldots,255\\), gdzie wartość 0 oznacza minimalną jasność (czerń), a 255 maksymalną jasność (biel). W wielu zastosowaniach profesjonalnej fotografii i druku, a także w medycynie i astronomii, 8 bitów na piksel nie jest wystarczające. W tych dziedzinach często spotyka się głębię obrazu 12, 14, a nawet 16 bitów. Zauważ, że głębia bitowa zwykle odnosi się do liczby bitów używanych do reprezentowania jednego składnika koloru, a nie liczby bitów potrzebnych do reprezentowania koloru piksela. Na przykład, zakodowany w RGB kolorowy obraz z 8-bitową głębią wymagałby 8 bitów dla każdego kanału, co daje w sumie 24 bity, podczas gdy ten sam obraz z 12-bitową głębią wymagałby w sumie 36 bitów.\nObrazy binarne (ang. binary images) - to specjalny rodzaj obrazu, w którym piksele mogą przyjmować tylko jedną z dwóch wartości, czarną lub białą. Wartości te są zwykle kodowane przy użyciu pojedynczego bitu (0/1) na piksel. Obrazy binarne są często wykorzystywane do reprezentowania grafiki liniowej, archiwizacji dokumentów, kodowania transmisji faksowych i oczywiście w druku elektronicznym.\nObrazy kolorowe (ang. color images) - większość kolorowych obrazów opiera się na kolorach podstawowych: czerwonym, zielonym i niebieskim (RGB), zwykle wykorzystując 8 bitów dla każdego kanału. W tego rodzaju obrazach kolorowych, każdy piksel wymaga 3×8 = 24 bity do zakodowania wszystkich trzech składowych, a zakres każdej indywidualnej składowej koloru wynosi [0, 255]. Podobnie jak w przypadku obrazów w skali szarości, kolorowe obrazy z 30, 36 i 42 bitami na piksel są powszechnie używane w profesjonalnych aplikacjach. Wreszcie, podczas gdy większość obrazów kolorowych zawiera trzy składowe, obrazy z czterema lub więcej składowymi koloru są powszechne w druku, zwykle oparte na modelu koloru CMYK (Cyan-Magenta-Yellow- Black). Główna różnica między tymi dwiema paletami polega na tym, że RGB ma więcej możliwości kolorów, ponieważ jest w stanie wygenerować więcej odcieni niż CMYK, ale kolory wyświetlane przez RGB nie są takie same jak te, które otrzymujemy przy druku z CMYK. Kolory drukowane z CMYK mogą również różnić się od tych wyświetlanych na ekranie.\nObrazy specjalne - są wymagane, jeżeli żaden z powyższych formatów standardowych nie jest wystarczający do przedstawienia wartości obrazu. Dwa popularne przykłady obrazów specjalnych to obrazy z wartościami ujemnymi oraz obrazy z wartościami zmiennoprzecinkowymi. Obrazy z wartościami ujemnymi powstają podczas etapów przetwarzania obrazu, takich jak filtrowanie w celu wykrywania krawędzi, a obrazy z wartościami zmiennoprzecinkowymi są często spotykane w zastosowaniach medycznych, biologicznych lub astronomicznych, gdzie wymagany jest zwiększony zakres liczbowy i precyzja. Te specjalne formaty są w większości przypadków specyficzne dla danego zastosowania i dlatego mogą być trudne do wykorzystania przez standardowe narzędzia do przetwarzania obrazów."
  },
  {
    "objectID": "digt_img.html#formaty-zapisu",
    "href": "digt_img.html#formaty-zapisu",
    "title": "3  Obrazy cyfrowe",
    "section": "3.2 Formaty zapisu",
    "text": "3.2 Formaty zapisu\n\n3.2.1 TIFF\nTIFF (ang. Tagged Image File Format) jest formatem pliku, który jest używany do przechowywania i wymiany obrazów cyfrowych. Jest to format bezstratny, co oznacza, że po zapisaniu i odczytaniu obrazu jego jakość pozostaje taka sama. TIFF jest obsługiwany przez wiele programów do edycji obrazów i może być używany do przechowywania różnych rodzajów obrazów, w tym obrazów w skali szarości, kolorowych oraz map bitowych. Format TIFF jest często używany przez profesjonalnych fotografów i grafików, ponieważ pozwala na zachowanie wysokiej jakości obrazu i jest kompatybilny z wieloma programami i urządzeniami.\n\n\n3.2.2 GIF\nGIF (ang. Graphics Interchange Format) jest formatem pliku graficznego, który jest używany do przechowywania i wymiany obrazów w internecie. GIF jest formatem bezstratnym, ale jest kompresowany, co pozwala na zmniejszenie rozmiaru pliku i przyspieszenie jego przesyłania. Co ważne, GIF jest formatem obsługującym animacje, co oznacza, że może on przechowywać kilka klatek jako jeden plik, co pozwala na tworzenie animowanych obrazów, często używanych jako emotikony, ikony lub małe animacje na stronach internetowych. GIF jest również ograniczony do 256 kolorów, co oznacza, że nie jest on dobrym rozwiązaniem do przechowywania zdjęć o wysokiej jakości.\n\n\n3.2.3 PNG\nPNG (ang. Portable Network Graphics) jest bezstratnym formatem pliku graficznego, który jest używany do przechowywania i wymiany obrazów w internecie. Podobnie jak GIF może być kompresowany w celu zmniejszenia rozmiaru pliku. Co ważne, format PNG jest formatem obsługującym przezroczystość, co oznacza, że może on przechowywać kanał alfa, który jest odpowiedzialny za przezroczystość obrazu, co pozwala na zastosowanie efektu przezroczystości na obrazie bez konieczności dodatkowego tworzenia specjalnego tła. PNG jest również w stanie przechowywać więcej kolorów niż GIF, co oznacza, że jest to lepsze rozwiązanie dla obrazów o wysokiej jakości.\n\n\n3.2.4 JPEG\nJPEG (ang. Joint Photographic Experts Group) to popularny format zapisu obrazów cyfrowych, który jest szczególnie przydatny do przechowywania zdjęć. Format ten pozwala na kompresję pliku (stratną), dzięki czemu pliki JPEG są mniejsze niż pliki niekompresowane. Format ten jest szczególnie przydatny do przechowywania zdjęć z wysokim poziomem szczegółów, takich jak zdjęcia przyrody czy portrety.\n\n\n3.2.5 EXIF\nEXIF (ang. Exchangeable Image File Format) to format danych, który jest zapisywany w pliku obrazu cyfrowego, takim jak JPEG lub TIFF. Informacje EXIF zawierają szczegółowe dane dotyczące zdjęcia, takie jak data i godzina utworzenia zdjęcia, parametry aparatu fotograficznego (np. przysłona, czas naświetlania, ISO), dane dotyczące obiektywu, a także współrzędne GPS, jeśli zdjęcie zostało zrobione z użyciem aparatu z GPS. EXIF jest przydatny dla fotografów i programów do obróbki zdjęć, ponieważ pozwala na łatwe odczytanie i wykorzystanie tych danych.\n\n\n3.2.6 BMP\nBMP (ang. Bitmap) to format pliku obrazu, który jest przeznaczony do przechowywania obrazów rastrowych, takich jak zdjęcia, grafiki i mapy bitowe. BMP jest formatem pliku natywnym dla systemów operacyjnych Windows, co oznacza, że pliki tego formatu są bezpośrednio obsługiwane przez system Windows i nie wymagają dodatkowego oprogramowania do odczytu.\nBMP jest formatem bezstratnym, co oznacza, że po zapisie obrazu w tym formacie, jego jakość pozostaje taka sama jak przed zapisem. Pliki BMP są jednak dość duże, ponieważ nie są skompresowane, co oznacza, że zajmują więcej miejsca na dysku niż pliki skompresowane innymi formatami. BMP jest często używany do przechowywania obrazów w celach archiwizacyjnych, ponieważ zachowuje pełną jakość obrazu.\n\n\n\nPorównanie formatów\n\n\n\n\n3.2.7 Operacje na plikach\nIstnieje wiele zewnętrznych (w stosunku do środowiska R) profesjonalnych narzędzi do obróbki zdjęć. Wśród nich z pewnością należy wymienić: Adobe Photoshop, CorelDRAW, Gimp, PIXLR, FIji (ImageJ) i wiele innych. Część z nich jest komercyjna, a część darmowa. Osobiście do przetwarzania obrazów pochodzących z badań biologicznych, medycznych, czy inżynierskich polecam darmowy program Fiji, będący rozszerzeniem swojego pierwowzoru, czyli ImageJ.\nRównież w samym środowisku R istnieje szereg bibliotek do obsługi obrazów:\n\nimager - pozwala na szybkie przetwarzanie obrazów w maksymalnie 4 wymiarach (dwa wymiary przestrzenne, jeden wymiar czasowy/głębokościowy, jeden wymiar koloru). Udostępnia większość tradycyjnych narzędzi do przetwarzania obrazów (filtrowanie, morfologia, transformacje, itp.), jak również różne funkcje do łatwej analizy danych obrazowych przy użyciu R.\nimagerExtra - poszerzenie zestawu funkcji pakietu imager.\nmagick - dostarcza nowoczesnego i prostego zestawu narzędzi do przetwarzania obrazów w R. Obejmuje on ImageMagick STL, który jest najbardziej wszechstronną biblioteką przetwarzania obrazów typu open-source dostępną obecnie.\nimageseg - pakiet ogólnego przeznaczenia do segmentacji obrazów z wykorzystaniem modeli TensorFlow opartych na architekturze U-Net autorstwa Ronneberger, Fischer, i Brox (2015) oraz architekturze U-Net++ autorstwa Zhou i in. (2018). Dostarcza wstępnie wytrenowane modele do oceny gęstości łanu i gęstości roślinności podszytu na podstawie zdjęć roślinności. Ponadto pakiet zapewnia workflow do łatwego tworzenia wejściowych modeli i architektur modeli dla segmentacji obrazów ogólnego przeznaczenia na podstawie obrazów w skali szarości lub kolorowych, zarówno dla segmentacji obrazów binarnych, jak i wieloklasowych.\npliman - jest pakietem do analizy obrazów, ze szczególnym uwzględnieniem obrazów roślin. Jest użytecznym narzędziem do uzyskania informacji ilościowej dla obiektów docelowych. W kontekście obrazów roślin, ilościowe określanie powierzchni liści, nasilenia chorób, liczby zmian chorobowych, liczenie liczby ziaren, uzyskiwanie statystyk ziaren (np. długość i szerokość) można wykonać stosując pakiet pliman.\n\n\nPrzykład 3.1 W tym przykładzie przedstawiona zostanie procedura importu i eksportu obrazów do różnych formatów. Najpierw wczytamy zdjęcie wykonane telefonem (IPhone 12) zapisane w formacie TIFF, a następnie zapiszemy to zdjęcie w kilku innych formatach, by na końcu wczytać je wszystkie i porównać.\n\n\nKod\nlibrary(magick)\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\n\n# wczytywanie obrazu \nimg_orig &lt;- image_read(\"images/IMG_3966.tiff\")\n\n# informacje o obrazie\nimg_orig_info &lt;- image_info(img_orig)\nimg_orig_info\n\n\n# A tibble: 1 × 7\n  format width height colorspace matte filesize density\n  &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n1 TIFF    3024   4032 sRGB       FALSE 36614010 72x72  \n\n\nJak widać obraz w formacie TIFF zajmuje bardzo dużo miejsca na dysku (36,6MB).\n\n\nKod\n# eksport do GIF\nimage_write(img_orig, path = \"images/img.gif\", \n            format = \"gif\")\n\n# eksport do PNG\nimage_write(img_orig, path = \"images/img.png\", \n            format = \"png\")\n\n# eksport do JPEG z jakością 100%\nimage_write(img_orig, path = \"images/img.jpeg\", \n            quality = 100, format = \"jpeg\")\n\n# eksport do JPEG z jakością 75%\nimage_write(img_orig, path = \"images/img2.jpeg\", \n            quality = 75, format = \"jpeg\")\n\n# eksport do JPEG z jakością 50%\nimage_write(img_orig, path = \"images/img3.jpeg\", \n            quality = 50, format = \"jpeg\")\n\n# eksport do JPEG z jakością 25%\nimage_write(img_orig, path = \"images/img4.jpeg\", \n            quality = 25, format = \"jpeg\")\n\n\nPo zapisie do innych formatów pliki znacznie zmniejszyły swoją wielkość.\n\n\nKod\nimg_gif &lt;- image_read(\"images/img.gif\")\nimg_gif_info &lt;- image_info(img_gif)\n\nimg_png &lt;- image_read(\"images/img.png\")\nimg_png_info &lt;- image_info(img_png)\n\nimg_jpeg1 &lt;- image_read(\"images/img.jpeg\")\nimg_jpeg1_info &lt;- image_info(img_jpeg1)\n\nimg_jpeg2 &lt;- image_read(\"images/img2.jpeg\")\nimg_jpeg2_info &lt;- image_info(img_jpeg2)\n\nimg_jpeg3 &lt;- image_read(\"images/img3.jpeg\")\nimg_jpeg3_info &lt;- image_info(img_jpeg3)\n\nimg_jpeg4 &lt;- image_read(\"images/img4.jpeg\")\nimg_jpeg4_info &lt;- image_info(img_jpeg4)\n\nbind_rows(img_orig_info, img_gif_info, img_png_info,\n          img_jpeg1_info, img_jpeg2_info, img_jpeg3_info,\n          img_jpeg4_info) |&gt; \n  gt()\n\n\n\n\n\n\n\n\nTabela 3.1: Podstawowe informacje o obrazach\n\n\nformat\nwidth\nheight\ncolorspace\nmatte\nfilesize\ndensity\n\n\n\n\nTIFF\n3024\n4032\nsRGB\nFALSE\n36614010\n72x72\n\n\nGIF\n3024\n4032\nsRGB\nFALSE\n7422827\n72x72\n\n\nPNG\n3024\n4032\nsRGB\nFALSE\n11994144\n28x28\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n8211226\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n1546461\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n1043337\n72x72\n\n\nJPEG\n3024\n4032\nsRGB\nFALSE\n691309\n72x72\n\n\n\n\n\n\n\n\nPomimo znacznej różnicy w wielkości obrazów (w sensie miejsca zajmowanego na dysku) nie różnią się one znacznie jakością (przynajmniej na pierwszy rzut oka)\n\n\nKod\nimg_orig\nimg_jpeg1\nimg_gif\nimg_png\nimg_jpeg2\nimg_jpeg3\nimg_jpeg4\n\n\n\n\n\n\n\nRysunek 3.3: TIFF\n\n\n\n\n\n\n\nRysunek 3.4: JPEG 100%\n\n\n\n\n\n\n\n\n\nRysunek 3.5: GIF\n\n\n\n\n\n\n\nRysunek 3.6: PNG\n\n\n\n\n\n\n\n\n\nRysunek 3.7: JPEG 75%\n\n\n\n\n\n\n\nRysunek 3.8: JPEG 50%\n\n\n\n\n\n\n\n\n\nRysunek 3.9: JPEG 25%\n\n\n\n\n\n\n\n\nKod\nmagick2cimg(img_orig) |&gt; \n  as.data.frame() |&gt; \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |&gt;\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_jpeg1) |&gt; \n  as.data.frame() |&gt; \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |&gt;\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_gif) |&gt; \n  as.data.frame() |&gt; \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |&gt;\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\nmagick2cimg(img_png) |&gt; \n  as.data.frame() |&gt; \n  mutate(channel = factor(cc,labels=c('R','G','B'))) |&gt;\n  ggplot(aes(value,col=channel))+\n  geom_histogram()+\n  facet_wrap(~ channel)\n\n\n\n\n\n\n\nRysunek 3.10: Histogramy RGB dla pliku TIFF\n\n\n\n\n\n\n\nRysunek 3.11: Histogray RGB dla pliku JPEG 100%\n\n\n\n\n\n\n\n\n\nRysunek 3.12: Histogray RGB dla pliku GIF\n\n\n\n\n\n\n\nRysunek 3.13: Histogray RGB dla pliku PNG\n\n\n\n\n\n\n\n\n\n\n\nRonneberger, Olaf, Philipp Fischer, i Thomas Brox. 2015. „U-Net: Convolutional Networks for Biomedical Image Segmentation”. arXiv. https://doi.org/10.48550/arXiv.1505.04597.\n\n\nZhou, Zongwei, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, i Jianming Liang. 2018. „UNet++: A Nested U-Net Architecture for Medical Image Segmentation”. arXiv. https://doi.org/10.48550/arXiv.1807.10165."
  },
  {
    "objectID": "transformations.html#przykłady-różnych-transformacji",
    "href": "transformations.html#przykłady-różnych-transformacji",
    "title": "4  Transformacje geometryczne",
    "section": "4.1 Przykłady różnych transformacji",
    "text": "4.1 Przykłady różnych transformacji\n\n\nKod\nlibrary(magick)\nfrink &lt;- image_read(\"https://jeroen.github.io/images/frink.png\")\nfrink\n\n\n\n\n\nFrink w oryginalnym rozmiarze\n\n\n\n\n\n\nKod\nimage_scale(frink, \"250%x250%\")\n\n\n\n\n\nFrink powiększony o 150%\n\n\n\n\n\n\nKod\nimage_scale(frink, \"300x100!\")\n\n\n\n\n\nFrink poszerzony do proporcji 3:1\n\n\n\n\n\n\nKod\nimage_rotate(frink, 90)\n\n\n\n\n\nRotacja o 90 stopni"
  },
  {
    "objectID": "transformations.html#rotacje-obrazów",
    "href": "transformations.html#rotacje-obrazów",
    "title": "4  Transformacje geometryczne",
    "section": "4.2 Rotacje obrazów",
    "text": "4.2 Rotacje obrazów\nObroty o inne kąty niż pełne wielokrotności 90\\(\\degree\\) powodują pewne problemy, ponieważ po rotacji powstają piksele, które nie pokrywają żadnej wartości z oryginalnego obrazu, a część z nich zawiera kilka wartości (Rysunek 4.1). Podobne zjawisko może powstać w sytuacji zmiany rozmiaru obrazów. W tych sytuacjach konieczna jest interpolacja pikseli zarówno “pustych”, jak i “wypełnionych”.\n\n\n\nRysunek 4.1: Przykład rotacji obrazu\n\n\n\n\n\nKod\nlibrary(imager)\nlibrary(keras)\n\ndata &lt;- dataset_mnist()\nim &lt;- t(data$train$x[2023,,])\nim &lt;- as.cimg(im)\n\nim_list &lt;- map_il(0:6, ~imrotate(im, # Nearest Neighbour interp.\n                                 angle = 15*.x, \n                                 interpolation = 0)) \nrow1 &lt;- imappend(im_list[1:4], axis = \"x\")  \nrow2 &lt;- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |&gt; \n  plot(interp=F) # antialiasing off\n\nim_list &lt;- map_il(0:6, ~imrotate(im, # linear interpolation\n                                 angle = 15*.x, \n                                 interpolation = 1)) \nrow1 &lt;- imappend(im_list[1:4], axis = \"x\")  \nrow2 &lt;- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |&gt; \n  plot(interp=F)\n\nim_list &lt;- map_il(0:6, ~imrotate(im, # cubic interpolation\n                                 angle = 15*.x, \n                                 interpolation = 2)) \nrow1 &lt;- imappend(im_list[1:4], axis = \"x\")  \nrow2 &lt;- imappend(im_list[5:7], axis = \"x\")\nimappend(list(row1,row2), \"y\") |&gt; \n  plot(interp=F) \n\n\n\n\n\n(a) Nearest Neighbour interpolation\n\n\n\n\n\n\n\n(b) Linear interpolation\n\n\n\n\n\n\n\n(c) Cubic interpolation\n\n\n\nRysunek 4.2: Przykład rotacji o wielokrotnośc \\(15\\degree\\)\n\n\nJak widać z powyższych wykresów rotacja wpływa na jakość obrazu, dlatego zaleca się nie stosować kilku występujących po sobie rotacji o kąty niebędące wielokrotnościami \\(90\\degree\\).\n\n\nKod\nim2 &lt;- imlist(im)\nfor(i in 1:6){\n  im2[[i+1]] &lt;- imrotate(im2[[i]], angle = 15)\n}\nimappend(im2, axis = \"x\") |&gt; plot(interp=F)\n\n\n\n\n\nRysunek 4.3: Kilka iteracji rotacji obrazu. Każdy nastepny powstaje jako rotacja poprzedniego."
  },
  {
    "objectID": "transformations.html#zmiana-rozmiaru-obrazu",
    "href": "transformations.html#zmiana-rozmiaru-obrazu",
    "title": "4  Transformacje geometryczne",
    "section": "4.3 Zmiana rozmiaru obrazu",
    "text": "4.3 Zmiana rozmiaru obrazu\nW przypadku gdy zmieniamy rozmiar obrazu w proporcji 0.5, 2, 3 funkcja imresize korzysta z algorytmu opisanego na stronie http://www.scale2x.it/algorithm.html.\n\n\nKod\nim_list &lt;- map_il(c(0.5,1, 2), ~imresize(im, \n                                         scale = .x)) \nimappend(im_list, \"x\") |&gt;\n  plot(interp = F) # antialiasing off\n\n\n\n\n\nRysunek 4.4: Zmiana rozmiaru obrazu\n\n\n\n\nW innych przypadkach wykorzystuje jedną z 7 opcji interpolacji:\n\nbrak interpolacji (opcja -1);\nbrak interpolacji ale dodatkowo powstała przestrzeń jest wypełniana zgodnie z warunkiem brzegowym boundary_conditions (opcja 0);\ninterpolacja metodą najbliższego sąsiada (opcja 1);\ninterpolacja metodą średniej kroczącej (opcja 2);\ninterpolacja liniowa (opcja 3);\ninterpolacja na siatce (opcja 4);\ninterpolacja kubiczna (opcja 5);\ninterpolacja Lanczosa (opcja 6).\n\n\n\nKod\nim_list &lt;- map_il(c(-1:6), ~imresize(im, \n                                     scale = 1.5,\n                                     interpolation = .x))\nrow1 &lt;- imappend(im_list[1:4], \"x\")\nrow2 &lt;- imappend(im_list[5:8], \"x\")\nimappend(list(row1, row2), \"y\") |&gt;\n  plot(interp = F)\n\n\n\n\n\nRysunek 4.5: Zastosowania różnych interpolacji"
  },
  {
    "objectID": "point_trans.html",
    "href": "point_trans.html",
    "title": "5  Transformacje punktowe",
    "section": "",
    "text": "Transformacje obrazów, które odbywają się na poziomie pojedynczych pikseli nazywane są w literaturze tematu transformacjami punktowymi (ang. point transformation). Należą do nich między innymi:\n\nmodyfikacja kontrastu,\nmodyfikacja jasności,\nzmiana intensywności,\nodwracanie wartości piksela,\nkwantyzacja obrazów (ang. posterizing),\nprogowanie,\nkorekta gammy,\ntransformacje kolorów.\n\nWszystkie można opisać formułą\n\\[\ng(x) = h(f(x)),\n\\]\ngdzie \\(x = (i,j)\\) jest położeniem transformowanego piksela, \\(f\\) jest funkcja oryginalnego obrazu (przed przekształceniem)1, natomiast \\(h\\) jest zastosowaną transformacją. Wówczas \\(g\\) opisuje transformację jako funkcję lokalizacji.1 informuje o nasyceniu barw w danej lokalizacji - pikselu\nNa potrzeby zmian w kontraście, czy jasności stosuje się przekształcenia postaci:\n\\[\ng(x) = a(x)\\cdot f(x)+b(x),\n\\]\ngdzie \\(a(x)\\) jest parametrem zmiany kontrastu, a \\(b(x)\\) jest parametrem zmiany jasności2. Należy jednak pamiętać, że wartości \\(g(x)\\) tak określonej transformacji mogą znaleźć się poza przedziałem [0,255]. Oczywiście powoduje to problem, z którym można sobie radzić poprzez progowanie wartości, tak aby znalazły się w przedziale [0,255]. Obrazy zapisane w formacie cimg wartości poszczególnych kanałów mają znormalizowane do przedziału [0,1]. Dodatkowo należy pamiętać, że funkcja plot ma domyślnie włączoną flagę rescale=TRUE co oznacza, że wartości kanałów zostaną znormalizowane do przedziału [0,1] automatycznie. W rezultacie oznacza to, że transformacja liniowa \\(g(x)\\) nie odniesie żadnego skutku.2 jeśli zmieniamy te parametry globalnie (dla całego obrazu) wówczas funkcje te są stałe\n\n\nKod\nlibrary(imager)\nlibrary(imagerExtra)\nlibrary(tidyverse)\n\nadjust &lt;- function(x, contrast, brithness){\n  \n  # transformacji dokonujemy od razu na wszystkich kanałach\n  x_trans &lt;- x |&gt; \n    as.data.frame() |&gt; \n    mutate(value = ifelse(value*(contrast+1)+brithness&gt;1, \n                          1, \n                          ifelse(value*(contrast+1)+brithness&lt;0, 0,\n                                 value*(contrast+1)+brithness)), \n           .by  = cc) |&gt; \n    as.cimg(dims = dim(x))\n  \n  return(x_trans)\n}\n\nlayout(t(1:2))\nplot(boats, rescale = F, interpolate = F)\nadjust(boats, 0.7, 0) |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.1: Zmiana kontrastu obrazu (podbicie o 70%)\n\n\n\n\n\n\nKod\nlayout(t(1:2))\nplot(boats, rescale = F, interpolate = F)\nadjust(boats, -0.3, 0) |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.2: Zmiana kontrastu obrazu (redukcja o 30%)\n\n\n\n\n\n\nKod\nlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nadjust(boats, 0, 0.2) |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.3: Zmiana jasności obrazu (podbicie o 20% pełnej skali)\n\n\n\n\n\n\nKod\nlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nadjust(boats, 0, -0.4) |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.4: Zmiana jasności obrazu (redukcja o 40% pełnej skali)\n\n\n\n\nAby uniknąć przekraczania wartości dla poszczególnych kanałów wprowadza się często funkcję, która podnosi kontrast (tzw. autokontrast) przez poszerzenie spektrum wartości z obserwowanych \\([x_{\\min},x_{\\max}]\\) do przedziału \\([lower, upper]\\) (często przyjmowane jako [0,1]):\n\\[\ng(x) = x_{lower}+(x-x_{\\min})\\cdot\\frac{x_{\\max}-x_{\\min}}{x_{upper}-x_{lower}}.\n\\] Funkcja EqualizeDP pozwala na autokontrast. Dodatkowo umożliwia ustawić wartości skrajne dla spektrum kanału inne niż min i max.\n\n\nKod\nlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |&gt; \n  imsplit(\"c\") |&gt; \n  map_il(~EqualizeDP(.x, t_down = min(.x),t_up = max(.x), range = c(0,1))) |&gt; \n  imappend(\"c\") |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.5: Autokontrast w zakresie min, max\n\n\n\n\n\n\nKod\nlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |&gt; \n  imsplit(\"c\") |&gt; \n  map_il(~EqualizeDP(.x, t_down = 50,t_up = 170, range = c(0,1))) |&gt; \n  imappend(\"c\") |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.6: Autokontrast w zakresie 50, 170\n\n\n\n\n\n\nKod\nlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |&gt; \n  imsplit(\"c\") |&gt; \n  map_il(~{\n    max(.x)-.x\n  }) |&gt; \n  imappend(\"c\") |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.7: Przykład odwrócenia wartości pikseli\n\n\n\n\n\n\nKod\nlayout(t(1:2))\nplot(boats,rescale = F, interpolate = F)\nboats |&gt; \n  imsplit(\"c\") |&gt; \n  map_il(~round(.x/0.2, digits = 0)) |&gt; \n  imappend(\"c\") |&gt; \n  plot(interpolate = F)\n\n\n\n\n\nRysunek 5.8: Przykład kwantyzacji obrazu\n\n\n\n\n\n\nKod\nlist(\"10%\",\"25%\", \"50%\", \"75%\", \"auto\") |&gt; \n  map_il(~threshold(im = boats, thr = .x)) |&gt; \n  imappend(\"x\") |&gt; \n  plot()\n\n\n\n\n\nRysunek 5.9: Przykład progowania obrazu z różnymi poziomami progowania\n\n\n\n\n\n\nKod\n2^(-3:3) |&gt; \n  map_il(~{\n    boats^.x\n  }) |&gt; \n  imappend(\"x\") |&gt; \n  plot()\n\n\n\n\n\nRysunek 5.10: Przykład korekty gamma dla różnych potęg\n\n\n\n\nTransformację kolorów możemy wykonywać na dwa sposoby:\n\nprzeprowadzając transformację oddzielnie na każdym kanale,\nzachowując barwę (ang. hue) obrazu, przetwarzać składową intensywności, a następnie obliczać wartości RGB z nowej składowej intensywności.\n\n\n\nKod\nlayout(t(1:2))\nplot(boats)\nboats |&gt; \n  imsplit(\"c\") |&gt; \n  map_il(~BalanceSimplest(.x, 2, 2, range = c(0,1))) |&gt; \n  imappend(\"c\") |&gt; \n  plot(rescale = F, interpolate = F)\n\n\n\n\n\nRysunek 5.11: Transformacja kolorów niezależnie modyfikując każdy kanał\n\n\n\n\nAby wykonać transformację drugą metodą zapiszemy plik w skali szarości, czyli dokonamy faktycznie agregacji postaci:\n\\[\ng_{grayscale}(x)=0.3R+0.59G+0.11B.\n\\]\nPonadto będziemy potrzebowali intensywności barw. Do zapisu nasycenia barw użyjemy funkcji GetHue pakietu imagerExtra.\n\n\nKod\n# zapisujemy obraz w skali szarości\ng &lt;- grayscale(boats)\n# zapisujemy nasycenia barw\nhueim &lt;- GetHue(boats)\n# transformujemy obraz w skali szarości\ng &lt;- BalanceSimplest(g, 2, 2, range = c(0,1))\n# oddtwarzamy kolory\ny &lt;- RestoreHue(g, hueim)\n\nlayout(t(1:2))\nplot(boats)\nplot(y)\n\n\n\n\n\nRysunek 5.12: Transformacja kolorów modyfikując kanały łącznie"
  },
  {
    "objectID": "filters.html#filtry-liniowe",
    "href": "filters.html#filtry-liniowe",
    "title": "6  Filtry",
    "section": "6.1 Filtry liniowe",
    "text": "6.1 Filtry liniowe\nFiltry liniowe są nazywane w ten sposób, ponieważ łączą wartości pikseli w otoczeniu w sposób liniowy, czyli jako suma ważona. Szczególnym przykładem jest omówiony na początku proces uśredniania lokalnego (Równanie 6.1), gdzie wszystkie dziewięć pikseli w lokalnym otoczeniu 3 × 3 jest dodawanych z identycznymi wagami (1/9). Dzięki temu samemu mechanizmowi można zdefiniować mnóstwo filtrów o różnych właściwościach, modyfikując po prostu rozkład poszczególnych wag.\n\\[\nI'(u,v) = \\frac19\\sum_{j = -1}^1\\sum_{i = -1}^1I(u+i,v+j),\n\\tag{6.1}\\]\nDla dowolnego filtra liniowego rozmiar i kształt regionu wsparcia (ang. support region), jak również wagi poszczególnych pikseli, są określone przez jądro filtra (ang. kernel) \\(H(i,j)\\). Rozmiar jądra \\(H\\) równa się rozmiarowi regionu filtrującego, a każdy element \\((i, j)\\) określa wagę odpowiedniego piksela w sumowaniu. Dla filtra wygładzającego 3x3 w równaniu (Równanie 6.1), jądro filtra to\n\\[\nH = \\begin{bmatrix}\n  1/9,&1/9,&1/9\\\\\n  1/9,&1/9,&1/9\\\\\n  1/9,&1/9,&1/9\n\\end{bmatrix}=\n\\frac19\\begin{bmatrix}\n  1,&1,&1\\\\\n  1,&1,&1\\\\\n  1,&1,&1\n\\end{bmatrix}\n\\]\nponieważ każda z wartości filtra wnosi 1/9 do piksela wynikowego.\nW istocie, jądro filtra \\(H(i, j)\\) jest, podobnie jak sam obraz, dyskretną, dwuwymiarową funkcją o rzeczywistą, \\(H : \\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{R}\\). Filtr ma swój własny układ współrzędnych z początkiem - często określanym jako hot spot - przeważnie (ale niekoniecznie) znajdującym się w środku. Tak więc współrzędne filtra są na ogół dodatnie i ujemne (Rysunek 6.2). Funkcja filtra ma nieskończony zakres i jest uważana za zerową poza obszarem zdefiniowanym przez macierz \\(H\\).\n\n\n\nRysunek 6.2: Schemat działania filtru\n\n\nDla filtru liniowego wynik jest jednoznacznie i całkowicie określony przez współczynniki jądra filtru. Zastosowanie filtru do obrazu jest prostym procesem, który został zilustrowany na Rysunek 6.2. W każdej pozycji obrazu \\((u, v)\\) wykonywane są następujące kroki:\n\nJądro filtra \\(H\\) jest przesuwane nad oryginalnym obrazem \\(I\\) tak, że jego początek \\(H(0, 0)\\) pokrywa się z aktualną pozycją obrazu \\((u, v)\\).\nWszystkie współczynniki filtra \\(H(i, j)\\) są mnożone z odpowiadającym im elementem obrazu \\(I(u+i,v+j)\\), a wyniki są sumowane.\nNa koniec otrzymana suma jest zapisywana w aktualnej pozycji w nowym obrazie \\(I'(u, v)\\).\n\nOpisując formalnie, wartości pikseli nowego obrazu \\(I'(u,v)\\) są obliczane przez operację\n\\[\nI'(u,v) = \\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(i,j),\n\\tag{6.2}\\]\ngdzie \\(R_H\\) oznacza zbiór współrzędnych pokrytych przez filtr \\(H\\). Nie całkiem dla wszystkich współrzędnych, aby być dokładnym. Istnieje oczywisty problem na granicach obrazu, gdzie filtr sięga poza obraz i nie znajduje odpowiadających mu wartości pikseli, które mógłby wykorzystać do obliczenia wyniku. Na razie ignorujemy ten problem granic, ale w dalszej części tego wykładu się tym zajmiemy.\n\n\n\nRysunek 6.3: Ilustracja działania filtru\n\n\nSkoro rozumiemy już zasadnicze działanie filtrów i wiemy, że granice wymagają szczególnej uwagi, możemy pójść dalej i zaprogramować prosty filtr liniowy. Zanim jednak to zrobimy, możemy chcieć rozważyć jeszcze jeden szczegół. W operacji punktowej każda nowa wartość piksela zależy tylko od odpowiadającej jej wartości piksela w oryginalnym obrazie, dlatego nie było problemu z zapisaniem wyników z powrotem do tego samego obrazu - obliczenia są wykonywane “w locie” bez potrzeby pośredniego przechowywania. Obliczenia w miejscu nie są generalnie możliwe dla filtra, ponieważ każdy oryginalny piksel przyczynia się do zmiany więcej niż jednego piksela wynikowego i dlatego nie może być zmodyfikowany przed zakończeniem wszystkich operacji.\nPotrzebujemy zatem dodatkowego miejsca na przechowywanie obrazu wynikowego, który następnie może być ponownie skopiowany do obrazu źródłowego (jeśli jest to pożądane). Tak więc kompletna operacja filtrowania może być zaimplementowana na dwa różne sposoby (Rysunek 6.4):\n\nWynik obliczeń filtra jest początkowo zapisywany w nowym obrazie, którego zawartość jest ostatecznie zapisywana z powrotem do obrazu oryginalnego.\nOryginalny obraz jest najpierw kopiowany do obrazu pośredniego, który służy jako źródło dla właściwej operacji filtrowania. Wynik zastępuje piksele w oryginalnym obrazie.\n\n\n\n\nRysunek 6.4: Dwa schematy implementacji filtrów do obrazów\n\n\nDla obu wersji wymagana jest taka sama ilość pamięci masowej, a więc żadna z nich nie oferuje szczególnej przewagi. W poniższych przykładach używamy na ogół wersji B.\nW filtrze prezentowanym powyżej wagi nie muszą być wszystkie takie same. Przykładowo filtr \\(H(u,v)\\) określony następująco\n\\[\nH(u,v)=\\begin{bmatrix}\n  0.075,&0.125,&0.075\\\\\n  0.125,&0.200,&0.125\\\\\n  0.075,&0.125,&0.075\n\\end{bmatrix}\n\\tag{6.3}\\]\nrównież uśrednia wartości w regionie wsparcia filtru ale nadając największe wagi wartościom w środku.\nZauważmy, że wagi filtra \\(H\\) są tak dobrane aby się sumowały do 1. Oznacza to, że filtr ten jest znormalizowany. Normalizacji filtrów używa się po to aby uniknąć sytuacji, w której wartość wyjściowa z filtra byłaby większa niż 255.\n\n\n\n\n\n\nWskazówka\n\n\n\nPonieważ funkcja plot pakietu imager ma włączoną opcję rescale = TRUE co oznacza, że wartości wynikowe i tak będą przekształcone do przedziału [0,1], to nie unormowane filtry i tak będą wyświetlać poprawnie przefiltrowane obrazy.\n\n\n\n\nKod\nlibrary(MASS)\nfilter1 &lt;- matrix(c(1,1,1,\n                   1,1,1,\n                   1,1,1), \n                  ncol = 3)\nfilter1\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\n\nKod\nfilter2 &lt;- filter1/9\nfilter2 |&gt; \n  fractions() # aby ładnie wyświetlić ułamki\n\n\n     [,1] [,2] [,3]\n[1,] 1/9  1/9  1/9 \n[2,] 1/9  1/9  1/9 \n[3,] 1/9  1/9  1/9 \n\n\nKod\n# filtry w funkcji convolve muszą być zapisane jako obraz (as.cimg)\nlayout(t(1:2))\nconvolve(boats, as.cimg(filter1)) |&gt; plot()\nconvolve(boats, as.cimg(filter2)) |&gt; plot()\n\n\n\n\n\nRysunek 6.5: Przykład użycia filtra przed i po normalizacji\n\n\n\n\n\n\nKod\ntry(\n  convolve(boats, as.cimg(filter1)) |&gt; \n  plot(rescale = FALSE)\n)\n\n\nError in colourscale(v[[1]], v[[2]], v[[3]]) : \n  color intensity 1.57913, not in [0,1]\n\n\nZatem możemy w konstruowaniu filtrów stosować wartości całkowite i wspólnej wartości normalizacyjnej.\n\\[\nH(u,v)=\\begin{bmatrix}\n  0.075,&0.125,&0.075\\\\\n  0.125,&0.200,&0.125\\\\\n  0.075,&0.125,&0.075\n\\end{bmatrix}=\n\\frac1{40}\\cdot\n\\begin{bmatrix}\n  3,&5,&3\\\\\n  5,&8,&5\\\\\n  3,&5,&3\n\\end{bmatrix}.\n\\]\nAby uniknąć wartości ujemnych, które mogłyby się pojawić w przypadku gdy jądro filtra zawierałoby wartości ujemne, stosuje się stałą przesunięcia (ang. offset). Wówczas Równanie 6.2 przyjmuje postać\n\\[\nI'(u,v) = Offset+\\frac{1}{Scale}\\cdot \\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(i,j),\n\\]\na \\(H(i,j)\\) jest zdefiniowany na \\(\\mathbb{Z}\\times\\mathbb{Z}\\). Chociaż najczęściej używa się filtrów kwadratowych, to nie ma przeszkód aby stosować również filtry prostokątne. Powyższa formuła ma zastosowanie do filtrów o dowolnym rozmiarze i kształcie.\n\n6.1.1 Przykładowe filtry wygładzające (ang. smoothing)\n\nFiltr pudełkowy (ang box), którego zasada działania została już przedstawiona w Równanie 6.1 dla rozmiaru filtra 3x3 jest jednym z filtrów wygładzających. Im większy jest rozmiar filtra, tym większy stopień wygładzenia obrazu wyjściowego. Ten najprostszy ze wszystkich filtrów wygładzających, którego kształt 3D przypomina pudełko (Rysunek 6.6 (a)), jest dobrze znany. Niestety, filtr pudełkowy jest daleki od optymalnego filtra wygładzającego ze względu na swoje dziwne zachowanie w przestrzeni częstotliwości, które jest spowodowane ostrym odcięciem wokół jego boków. Opisane w kategoriach częstotliwościowych wygładzanie odpowiada tzw. filtracji dolnoprzepustowej, czyli efektywnemu tłumieniu wszystkich składowych sygnału powyżej danej częstotliwości odcięcia. Filtr pudełkowy wytwarza jednak silne “dzwonienie” w przestrzeni częstotliwości i dlatego nie jest uważany za wysokiej jakości filtr wygładzający. Przypisanie tej samej wagi wszystkim pikselom obrazu w regionie filtru może też wydawać się dość doraźne. Zamiast tego należałoby prawdopodobnie oczekiwać, że silniejszy nacisk zostanie położony na piksele znajdujące się w pobliżu centrum filtra niż na te bardziej odległe. Ponadto filtry wygładzające powinny ewentualnie działać “izotropowo” (tzn. jednolicie w każdym kierunku), co z pewnością nie ma miejsca w przypadku filtra prostokątnego.\nFiltr gaussowski - z pewnością lepszy w tym kontekście wygładzania z względu na brak ostrych krawędzi jądra. Definiuje się go następująco \\[\nH^{G,\\sigma}(x,y) = e^{-\\frac{x^2+y^2}{2\\sigma^2}},\n\\] gdzie \\(\\sigma\\) oznacza odchylenie standardowe rozkładu.\n\n\n\n6.1.2 Przykład filtru różnicującego\nJeśli niektóre współczynniki filtra są ujemne, to obliczenie filtra można zinterpretować jako różnicę dwóch sum: suma ważona wszystkich pikseli z przypisanymi współczynnikami dodatnimi minus suma ważona pikseli z ujemnymi współczynnikami w regionie filtra RH , czyli\n\\[\n\\begin{align}\n  I'(u,v)=&\\sum_{(i,j)\\in R^+}I(u+i, v+j)\\cdot \\vert H(i,j)\\vert -\\\\\n-&\\sum_{(i,j)\\in R^-}I(u+i, v+j)\\cdot \\vert H(i,j)\\vert,\n\\end{align}\n\\tag{6.4}\\]\ngdzie \\(R^-, R^+\\) oznaczają podział filtra na współczynniki ujemne \\(H(i,j)&lt;0\\) i dodatnie \\(H(i,j)&gt;0\\) odpowiednio. Na przykład filtr Laplace’a 5x5 na Rysunek 6.6 (c) oblicza różnicę między pikselem środkowym (o wadze 16) a sumą ważoną 12 otaczających go pikseli (o wagach -1 lub -2). Pozostałe 12 pikseli ma przypisane zerowe współczynniki i dlatego są one ignorowane w obliczeniach. Podczas gdy lokalne zmiany intensywności są wygładzane przez uśrednianie, możemy oczekiwać, że w przypadku różnic stanie się dokładnie odwrotnie - lokalne zmiany intensywności zostaną wzmocnione.\n\n\n\nRysunek 6.6: Przykłady różnych filtrów o rozmiarze 5x5. (a) filtr pudełkowy, (b) gaussowski, (c) Laplace’a (zwany także Mexican Hut)\n\n\n\n\n6.1.3 Formalny zapis operatorów filtracji\nWspomniany zapis w równaniu Równanie 6.2 nazywany jest w literaturze operatorem korelacyjnym (ang. correlation operator). Ma on jedną poważną wadę, ponieważ filtr zastosowany do obrazu z pojedynczym wtrąceniem (jednym pikselem świecącym) w rezultacie daje w obrazie wynikowym wartości filtra zrotowane o \\(180\\degree\\) (patrz Rysunek 6.7).\n\n\n\nRysunek 6.7: Zastosowanie operatora korelacyjnego na obrazie z jednym wtrąceniem\n\n\nRozwiązaniem tej niedogodności jest wprowadzenie operatora konwolucyjnego (ang. convolution operator). Definiuje się go w następujący sposób:\n\\[\nI'(u,v) = \\sum_{i,j\\in R_H}I(u-i, v-j)\\cdot H(i,j),\n\\tag{6.5}\\]\nzapisywany również w bardziej zwartej formie\n\\[\nI'=I*H.\n\\tag{6.6}\\]\nAby pokazać związek pomiędzy oboma sposobami filtracji przekształćmy wzór Równanie 6.5\n\\[\n\\begin{align}\n  I'(u,v) =& \\sum_{i,j\\in R_H}I(u-i, v-j)\\cdot H(i,j)=\\\\\n  =&\\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H(-i,-j)=\\\\\n  =&\\sum_{i,j\\in R_H}I(u+i, v+j)\\cdot H^*(i,j),\n\\end{align}\n\\tag{6.7}\\]\ngdzie \\(H^*(i,j)=H(-i,-j)\\). Zmiana parametryzacji powoduje obrócenie wyniku o \\(180\\degree\\).\n\n\n\nRysunek 6.8: Zastosowanie operatora konwolucyjnego do obrazu z jednym wtrąceniem\n\n\n\n\n6.1.4 Własności filtrów liniowych\nKonwolucja liniowa jest odpowiednim modelem dla wielu rodzajów zjawisk naturalnych, w tym układów mechanicznych, akustycznych i optycznych. W szczególności istnieją silne formalne powiązania z reprezentacją Fouriera sygnałów w dziedzinie częstotliwości, które są niezwykle cenne dla zrozumienia złożonych zjawisk, takich jak próbkowanie i aliasing. Poniżej przedstawione zostaną własności konwolucji liniowej.\n\n6.1.4.1 Przemienność\nKonwolucja liniowa jest przemienna, czyli dla dowolnego obrazu \\(I\\) i jądra filtru \\(H\\), zachodzi\n\\[\nI ∗ H = H ∗ I.\n\\tag{6.8}\\]\nWynik jest więc taki sam, jeśli obraz i jądro filtra są wzajemnie zamienione, i nie ma różnicy, czy składamy obraz \\(I\\) z jądrem \\(H\\), czy odwrotnie.\n\n\n6.1.4.2 Liniowość\nFiltry liniowe nazywane są tak ze względu na właściwości liniowości operacji konwolucji, która przejawia się w różnych aspektach. Na przykład, jeśli obraz jest mnożony przez skalar \\(s\\in\\mathbb{R},\\) to wynik konwolucji mnoży się o ten sam czynnik, czyli\n\\[\n(s\\cdot I)∗H = I ∗(s\\cdot H) = s\\cdot(I ∗H).\n\\tag{6.9}\\]\nPodobnie, jeśli dodamy dwa obrazy \\(I_1\\), \\(I_2\\) piksel po pikselu i spleciemy wynikowy obraz za pomocą pewnego jądra \\(H\\), to taki sam wynik uzyskamy splatając każdy obraz osobno i dodając potem oba wyniki, czyli\n\\[\n(I_1 +I_2)∗H = (I_1 ∗H)+(I_2 ∗H).\n\\tag{6.10}\\]\nZaskakujące może być jednak to, że samo dodanie do obrazu stałej wartości \\(b\\) nie powiększa wyniku splotu o taką samą ilość,\n\\[\n(b+I)∗H\\neq b+(I∗H),\n\\]\na więc nie jest częścią własności liniowości. Chociaż liniowość jest ważną własnością teoretyczną, należy zauważyć, że w praktyce filtry “liniowe” są często tylko częściowo liniowe z powodu błędów zaokrąglenia lub ograniczonego zakresu wartości wyjściowych.\n\n\n6.1.4.3 Łączność\nKonwolucja liniowa jest łączna, co oznacza, że kolejność operacji na filtrze nie ma znaczenia, czyli,\n\\[\n(I∗H_1)∗H_2 =I∗(H_1 ∗H_2).\n\\tag{6.11}\\]\nTak więc wiele filtrów może być zastosowanych w dowolnej kolejności, jak również wiele filtrów może być dowolnie łączonych w nowe filtry.\nBezpośrednią konsekwencją łączności jest rozdzielność filtrów liniowych. Jeżeli jądro konwolucyjne \\(H\\) można wyrazić jako złożenie wielu jąder \\(H_i\\) w postaci\n\\[\nH = H_1 ∗ H_2 ∗ \\ldots∗ H_n,\n\\tag{6.12}\\]\nwówczas (jako konsekwencja Równanie 6.11) operacja filtru \\(I ∗ H\\) może być wykonana jako ciąg konwolucji z jądrami składowymi \\(H_i,\\)\n\\[\nI ∗ H = I ∗ (H_1 ∗ H_2 ∗ . . . ∗ H_n)\n= (\\ldots((I ∗H_1)∗H_2)∗\\ldots∗H_n).\n\\tag{6.13}\\]\nW zależności od rodzaju dekompozycji może to przynieść znaczne oszczędności obliczeniowe. O rozdzielczości filtrów możemy myśleć również nieco inaczej\n\\[\nI'=(I*h_x)*h_y,\n\\] gdzie \\(h_x, h_y\\) są filtrami 1D, które po wymnożeniu tworzą filtr o wymiarze \\(k\\times m\\). Przykładowo\n\\[\n\\begin{bmatrix}\n  1,&1,&1,&1,&1\\\\\n  1,&1,&1,&1,&1\\\\\n  1,&1,&1,&1,&1\n\\end{bmatrix}\n= H = h_x*h_y=\n\\begin{bmatrix}\n  1\\\\\n  1\\\\\n  1\n\\end{bmatrix}\n\\cdot\n    \\begin{bmatrix}\n        1,&1,&1,&1,&1\n       \\end{bmatrix}\n\\tag{6.14}\\]\nStosując tą własność rozłączności możemy również przedstawić filtr o jądrze gassuowskim 2D, za pomocą mnożenia filtrów gaussowskich 1D.\n\\[\nH^{G,\\sigma}=h^{G,\\sigma}_x*h^{G,\\sigma}_y,\n\\tag{6.15}\\]\ngdzie \\(h^{G,\\sigma}_x,h^{G,\\sigma}_y\\) są filtrami gaussowskimi 1D. Kolejność użytych filtrów ponownie nie ma znaczenia. Jeśli dla poszczególnych składowych odchylenia standardowe nie są równe, to filtr gaussowski 2D ma charakter eliptyczny.\nW algebrze filtrów liniowych istniej coś na kształt elementu neutralnego dla operacji splotu\n\\[\nI = \\delta*I,\n\\tag{6.16}\\]\ngdzie\n\\[\n\\delta(u,v)=\n\\begin{cases}\n  1, &\\text{ jeśli }u=v=0\\\\\n  0, &\\text{ w przeciwnym przypadku}\n\\end{cases}\n\\tag{6.17}\\]\nCo ciekawe filtr ten nie zmienia jedynie samego obrazu ale również innych filtrów\n\\[\nH=\\delta*H=H*\\delta.\n\\tag{6.18}\\]"
  },
  {
    "objectID": "filters.html#filtry-nieliniowe",
    "href": "filters.html#filtry-nieliniowe",
    "title": "6  Filtry",
    "section": "6.2 Filtry nieliniowe",
    "text": "6.2 Filtry nieliniowe\n\n6.2.1 Filtry minimum i maksimum\nJak wszystkie inne filtry, filtry nieliniowe obliczają wynik w danej pozycji obrazu \\((u,v)\\) z pikseli znajdujących się wewnątrz ruchomego regionu \\(R_{u,v}\\) oryginalnego obrazu. Filtry te nazywane są “nieliniowymi”, ponieważ wartości pikseli źródłowych są łączone przez jakąś funkcję nieliniową. Najprostszymi ze wszystkich filtrów nieliniowych są filtry minimum i maksimum, zdefiniowane jako\n\\[\n\\begin{align}\n  I'(u,v)=&\\min_{(i,j)\\in R}\\{I(u+i, v+j)\\}\\\\\n  I'(u,v)=&\\max_{(i,j)\\in R}\\{I(u+i, v+j)\\}\n\\end{align}\n\\tag{6.19}\\]\ngdzie \\(R\\) oznacza region filtra (zbiór współrzędnych filtra, zwykle kwadrat o rozmiarach 3x3 pikseli). Rysunek 6.10 ilustruje wpływ minimalnego filtra 1D na różne lokalne struktury sygnału.\n\n\n\nRysunek 6.9: Zastosowanie filtru minimum 1D do różnych sygnałów wejściowych. Górny rząd przedstawia oryginalny sygnał, a dolny spleciony z filtrem minimum\n\n\n\n\nKod\nkwiat &lt;- load.image(file = \"~/kwiat.jpg\") |&gt; grayscale()\n\nmin_filter &lt;- function(im, radius) {\n  stencil &lt;- expand.grid(dx = -radius:radius, dy = -radius:radius)\n  filtered &lt;- matrix(0, nrow = height(im), ncol = width(im))\n  range_x &lt;- (1 + radius):(width(im) - radius)\n  range_y &lt;- (1 + radius):(height(im) - radius)\n  dt &lt;- expand.grid(range_x, range_y)\n  dt$min &lt;- apply(dt, 1, function(row) min(get.stencil(im, \n                                                       stencil, \n                                                       x = row[1],\n                                                       y = row[2])))\n\n  filtered &lt;- dt$min |&gt; \n    matrix(ncol = height(im)-2*radius,\n           nrow = width(im)-2*radius) |&gt; \n    as.cimg()\n    \n  return(filtered)\n}\n\nkwiat_filtered &lt;- min_filter(kwiat, radius = 2)\n\nlayout(t(1:2))\nkwiat |&gt; plot(main = 'oryginał')\nkwiat_filtered |&gt; plot(main = \"filtr minimum\")\n\n\n\n\n\nRysunek 6.10: Zastosowanie filtru minimum\n\n\n\n\nW przypadku obrazów z wtrąceniami w kolorze białym lub czarnym efekt występowania tych wtrąceń jest potęgowany (patrz Rysunek 6.11). Filtr minimum wyciąga kolor czarny, natomiast filtr maksimum kolor biały.\n\n\nKod\nkwiat_noisy &lt;- load.image(file = \"~/kwiat_salt_pepper.jpg\") |&gt; grayscale()\n\nkwiat_noisy |&gt; plot()\nkwiat_noisy |&gt; \n  erode_square(size = 5) |&gt; \n  plot()\nkwiat_noisy |&gt; \n  dilate_square(size = 5) |&gt; \n  plot()\n\n\n\n\n\n\n\n\n(a) Oryginał\n\n\n\n\n\n\n\n(b) Filtr minimum\n\n\n\n\n\n\n\n(c) Filtr maximum\n\n\n\n\nRysunek 6.11: Porównanie trzech obrazów\n\n\n\n\n\n6.2.2 Filtr medianowy\nNie da się oczywiście zaprojektować filtra, który usunie każdy szum i zachowa wszystkie ważne struktury obrazu, ponieważ żaden filtr nie jest w stanie rozróżnić, która zawartość obrazu jest ważna dla widza, a która nie. Popularny filtr medianowy jest z pewnością dobrym krokiem w tym kierunku.\nFiltr medianowy zastępuje każdy piksel obrazu medianą pikseli w bieżącym regionie filtra \\(R\\), czyli\n\\[\nI'(u,v)=\\operatorname{Median}_{(i,j)\\in R}\\{I(u+i, v+j)\\}.\n\\tag{6.20}\\]\n\n\n\nRysunek 6.12: Zasada działania filtra medianowego\n\n\nRównanie (Równanie 6.20) definiuje medianę zbioru wartości o nieparzystej liczebności. Jeśli długość boku filtrów prostokątnych jest nieparzysta (co zwykle ma miejsce), to liczba elementów w regionie filtrów również jest nieparzysta. W takim przypadku filtr medianowy nie tworzy żadnych nowych wartości pikseli. Jeśli jednak liczba elementów jest parzysta, to mediana posortowanego ciągu \\(A = (a_0,\\ldots,a_{2n-1})\\) jest definiowana jako średnia arytmetyczna dwóch sąsiednich wartości środkowych \\(a_{n-1}\\) i \\(a_n\\). W ten sposób mogą być wprowadzone nowe wartości do obrazu.\n\n\nKod\nbox_flt &lt;- kwiat_noisy |&gt; \n  boxblur(boxsize = 3)\nmedian_flt &lt;- kwiat_noisy |&gt; \n  medianblur(n = 3)\nlist(kwiat_noisy, box_flt, median_flt) |&gt; \n  imappend(\"x\") |&gt; \n  plot()\n\n\n\n\n\nRysunek 6.13: Porównanie metod usuwania szumu. Na obrazie po lewej stronie naniesiony jest szum (znany jako salt and papper). Na środkowym szum jest usuwany filtrem pudełkowym (ang. box filter). Na obrazie po prawej szum jest usuwany filtrem medianowym.\n\n\n\n\n\n\nKod\nbox_flt_big &lt;- imsub(box_flt, x %inr% c(200, 450), y %inr% c(300, 550)) \nmedian_flt_big &lt;- imsub(median_flt, x %inr% c(200, 450), y %inr% c(300, 550)) \nimappend(list(box_flt_big, median_flt_big), \"x\") |&gt; \n  plot()\n\n\n\n\n\nRysunek 6.14: W powiększeniu ten efekt jest jeszcze lepiej widoczny\n\n\n\n\nMediana jest statystyką porządkową i w pewnym sensie “większość” uwzględnianych wartości pikseli określa wynik. Pojedyncza wyjątkowo wysoka lub niska wartość (“odstająca”) nie może wpłynąć na wynik, a jedynie przesunąć go w górę lub w dół do następnej wartości. Dlatego mediana (w przeciwieństwie do średniej) jest uważana za miarę “odporną”. W zwykłym filtrze medianowym każdy piksel w regionie filtra ma taki sam wpływ, niezależnie od jego odległości od środka."
  },
  {
    "objectID": "filters.html#obramowania-obrazów",
    "href": "filters.html#obramowania-obrazów",
    "title": "6  Filtry",
    "section": "6.3 Obramowania obrazów",
    "text": "6.3 Obramowania obrazów\nJak to zostało zaznaczone wcześniej zastosowanie jakiegokolwiek filtru (liniowego lub nieliniowego) wiąże się z pewna niedogodnością. Mianowicie wszystkie filtry kwadratowe czy prostokątne są kłopotliwe w zastosowaniu jeśli centrum (hot spot) filtra leży blisko brzegu obrazu. Teoretycznie filtry nie mogą być stosowane w miejscach, gdzie macierz filtrów nie jest w pełni zawarta w macierzy obrazu. Zatem każda operacja filtrująca zmniejszyłaby rozmiar obrazu wynikowego, co w większości zastosowań jest nie do przyjęcia. Choć nie istnieje formalnie poprawne remedium, istnieje kilka mniej lub bardziej praktycznych metod obsługi regionów granicznych:\n\nUstaw nieprzetworzone piksele na granicach na jakąś stałą wartość (np. “czarny”). Jest to z pewnością najprostsza metoda, ale w wielu sytuacjach nie do przyjęcia, ponieważ rozmiar obrazu jest stopniowo zmniejszany przez każdą operację filtra.\nUstaw nieprzetworzone piksele na oryginalne (niefiltrowane) wartości obrazu. Zazwyczaj wyniki są również nie do przyjęcia, ze względu na zauważalną różnicę między przefiltrowanymi i nieprzetworzonymi fragmentami obrazu.\nRozwiń obraz, “wypełniając” (ang. padding) dodatkowe piksele wokół niego i zastosuj filtr również do regionów granicznych. Na Rysunek 6.15 pokazano różne opcje wypełniania obrazów (obraz (a) jest oryginałem).\n\nPiksele poza obrazem mają stałą wartość (np. “czarny” lub “szary”, patrz Rysunek 6.15 (b)). Może to powodować silne artefakty na granicach obrazu, szczególnie w przypadku stosowania dużych filtrów.\nPiksele graniczne wykraczają poza granice obrazu (Rysunek 6.15 (c)). W miejscach występowania granic można spodziewać się jedynie niewielkich artefaktów. Metoda ta jest również prosta obliczeniowo i dlatego często jest uważana za najlepszy wybór.\nObraz jest odbijany na każdej ze swoich czterech granic (Rysunek 6.15 (d)). Wyniki będą podobne jak w przypadku poprzedniej metody, o ile nie zostaną użyte bardzo duże filtry.\nObraz powtarza się cyklicznie w kierunku poziomym i pionowym (Rysunek 6.15 (e)). Może się to początkowo wydawać dziwne, a wyniki na ogół nie są zadowalające. Jednak w dyskretnej analizie spektralnej obraz jest pośrednio traktowany również jako funkcja okresowa. Jeśli więc obraz jest filtrowany w dziedzinie częstotliwości, to wyniki będą równe filtracji w dziedzinie przestrzeni w ramach tego powtarzalnego modelu.\n\n\n\n\n\nRysunek 6.15: Różne sposoby obsługi obramowania\n\n\nŻadna z tych metod nie jest doskonała i zwykle, właściwy wybór zależy od rodzaju obrazu i zastosowanego filtra."
  },
  {
    "objectID": "edge.html#wykrywanie-krawędzi-na-podstawie-gradientu",
    "href": "edge.html#wykrywanie-krawędzi-na-podstawie-gradientu",
    "title": "7  Wykrywanie krawędzi i konturów",
    "section": "7.1 Wykrywanie krawędzi na podstawie gradientu",
    "text": "7.1 Wykrywanie krawędzi na podstawie gradientu\nDla uproszczenia, najpierw zbadamy sytuację tylko w jednym wymiarze, zakładając, że obraz zawiera pojedynczy jasny obszar w centrum otoczony ciemnym tłem (Rysunek 7.2 (a)). W tym przypadku profil natężenia wzdłuż jednej linii obrazu wyglądałby jak funkcja 1D \\(f(x)\\), jak pokazano na Rysunek 7.2 (b). Biorąc pierwszą pochodną funkcji \\(f\\)\n\\[\nf'(x)=\\frac{df}{dx}(x),\n\\tag{7.1}\\]\npowoduje dodatnie wahnięcie w tych miejscach, gdzie intensywność wzrasta i ujemne wahnięcie tam, gdzie wartość funkcji spada (Rysunek 7.2 (c)).\n\n\n\nRysunek 7.2: Przykładowy obraz z wyraźną zmianą jasności\n\n\nW przeciwieństwie do przypadku ciągłego, pierwsza pochodna jest nieokreślona dla funkcji dyskretnej \\(f(u)\\) i potrzebna jest jakaś metoda, by ją oszacować. Rysunek 7.3 pokazuje podstawową ideę, ponownie dla przypadku 1D: pierwsza pochodna funkcji ciągłej w pozycji \\(x\\) może być interpretowana jako nachylenie jej stycznej w tej pozycji. Jedną z prostych metod przybliżonego określenia nachylenia stycznej dla funkcji dyskretnej \\(f(u)\\) jest dopasowanie linii prostej przechodzącej przez wartości funkcji \\(f(u-1)\\) i \\(f(u+1)\\) w sąsiedztwie \\(u\\)\n\\[\n\\frac{df}{dx}(u)\\approx\\frac{f(u+1)-f(u-1)}{2}.\n\\tag{7.2}\\]\nOczywiście tę samą metodę można zastosować w kierunku pionowym, aby oszacować pierwszą pochodną wzdłuż osi y, czyli wzdłuż kolumn obrazu.\n\n\n\nRysunek 7.3: Oszacowanie gradientu dla funkcji dyskretnej\n\n\nW przypadku funkcji wielowymiarowej obliczamy pochodne cząstkowe \\(I_u = \\frac{\\partial I(u,v)}{\\partial u}\\) i \\(I_v = \\frac{\\partial I(u,v)}{\\partial v}\\) funkcji obrazu 2D \\(I(u, v)\\) odpowiednio wzdłuż osi \\(u\\) i \\(v\\). Wektor\n\\[\n\\nabla I(u,v)= \\begin{bmatrix}\n  I_u(u,v)\\\\\n  I_v(u,v)\n\\end{bmatrix}\n\\tag{7.3}\\]\nnazywamy gradientem funkcji \\(I\\) w punkcie \\((u, v)\\). Długość gradientu\n\\[\n\\vert \\nabla I\\vert = \\sqrt{I_u^2+I_v^2}\n\\tag{7.4}\\]\njest niezmiennicza ze względu na obroty obrazu, a więc niezależna od orientacji leżących u jego podstaw struktur. Własność ta jest istotna dla izotropowej1 lokalizacji krawędzi, a zatem \\(\\vert\\nabla I\\vert\\) jest podstawą wielu praktycznych metod detekcji krawędzi.1 we wszystkich kierunkach wykazują one te same właściwości fizyczne\nSkładowe funkcji gradientu (Równanie 7.3) są po prostu pierwszymi pochodnymi wierszy i kolumn obrazu odpowiednio wzdłuż osi poziomej i pionowej. Aproksymacja pierwszych pochodnych poziomych (Równanie 7.2) może być łatwo zrealizowana przez filtr liniowy z jądrem 1D\n\\[\nH_u^D = [-0.5,0,0.5]\n\\tag{7.5}\\]\ngdzie współczynniki -0.5 i 0.5 stosujemy do elementów obrazu \\(I(u-1, v)\\) i \\(I(u+1, v)\\) odpowiednio. Zauważmy, że sam piksel środkowy \\(I(u, v)\\) jest z wagą zerową, a więc jest ignorowany. Analogicznie, pionowa składowa gradientu jest uzyskiwana za pomocą filtru liniowego\n\\[\nH^D_v= \\begin{bmatrix}\n  -0.5\\\\\n  0\\\\\n  0.5\n\\end{bmatrix}\n\\tag{7.6}\\]\nRysunek 6.4 przedstawia wynik zastosowania filtrów gradientowych zdefiniowanych w Równanie 7.5 i Równanie 7.6 do syntetycznego obrazu testowego. Widać wyraźnie zależność odpowiedzi filtrów od orientacji. Filtr gradientu poziomego \\(H_u^D\\) reaguje najsilniej na szybkie zmiany wzdłuż kierunku poziomego, (czyli na krawędzie pionowe); analogicznie filtr gradientu pionowego \\(H_v^D\\) reaguje najsilniej na krawędzie poziome. W płaskich obszarach obrazu (przedstawionych jako szare na Rysunek 7.4 (b, c)) reakcja filtra jest zerowa.\n\n\n\nRysunek 7.4: Zastosowanie metody gradientowej do obrazu 2D\n\n\nLokalne gradienty funkcji obrazu są podstawą wielu klasycznych operatorów detekcji krawędzi. Praktycznie różnią się one jedynie rodzajem filtra stosowanego do estymacji składowych gradientu oraz sposobem łączenia tych składowych. W wielu sytuacjach człowiek jest zainteresowany nie tylko natężeniem punktów krawędziowych, ale także lokalnym kierunkiem krawędzi. Oba rodzaje informacji zawarte są w funkcji gradientu i mogą być łatwo obliczone ze składowych kierunkowych. Poniższy niewielki zbiór opisuje kilka często używanych, prostych operatorów krawędziowych, które istnieją od wielu lat, a więc są interesujące również z historycznego punktu widzenia.\n\n7.1.1 Przegląd filtrów do detekcji krawędzi\n\n7.1.1.1 Filtry Prewitta i Sobela\nOperatory krawędziowe autorstwa Prewitta („Picture Processing and Psychopictorics - 1st Edition”, b.d.) i Sobela (Davis 1975) to dwie klasyczne metody, które różnią się tylko nieznacznie stosowanymi przez nie filtrami pochodnymi.\nOba operatory wykorzystują filtry liniowe, które rozciągają się odpowiednio na trzy sąsiednie wiersze i kolumny, aby przeciwdziałać wrażliwości na szumy prostych (pojedynczy wiersz/kolumna) operatorów gradientowych. Operator Prewitta wykorzystuje jądra postaci\n\\[\nH^P_u= \\begin{bmatrix}\n  -1& 0& 1\\\\\n  -1& 0& 1\\\\\n  -1& 0& 1\n\\end{bmatrix}\n\\quad\\text{i}\\quad\nH^P_v=\\begin{bmatrix}\n  -1&-1&-1\\\\\n  0&0&0\\\\\n  1&1&1\n\\end{bmatrix}\n\\tag{7.7}\\]\nktóre obliczają średnie gradientu odpowiednio w trzech sąsiednich wierszach lub kolumnach. Podczas gdy filtry Sobela definiuje się w postaci\n\\[\nH^S_u= \\begin{bmatrix}\n  -1&0&1\\\\\n  -2&0&2\\\\\n  -1&0&1\n\\end{bmatrix}\n\\quad \\text{i}\\quad\nH^S_v=\\begin{bmatrix}\n  -1&-2&-1\\\\\n  0&0&0\\\\\n  1&2&1\n\\end{bmatrix}\n\\tag{7.8}\\]\nczyli jak widać są podobne do filtrów Prewitta z tą różnicą, że część wygładzająca przypisuje większą wagę do aktualnej linii środkowej i kolumny odpowiednio. W przypadku obu filtrów długość wektora gradientu krawędzi jest określona przez\n\\[\nE(u,v)=\\sqrt{I_u^2(u,v)+I_v^2(u,v)},\n\\tag{7.9}\\]\ngdzie \\(I_u\\) i \\(I_v\\) powstały przez mnożenie \\(I*H\\) (\\(H\\) jest filtrem Prewitta lub Sobela). Natomiast lokalny kąt orientacji krawędzi jest równy\n\\[\n\\Phi(u,v)=\\tan^{-1}\\left(\\frac{I_v(u,v)}{I_u(u,v)}\\right)\n\\tag{7.10}\\]\n\n\n\nRysunek 7.5: Ilustracja dla długości wektora gradientu i orientacji krawędzi\n\n\nCały proces ekstrakcji wielkości i orientacji krawędzi jest podsumowany na Rysunek 7.6. Najpierw oryginalny obraz \\(I\\) jest niezależnie splatany z dwoma filtrami gradientowymi \\(H_u\\) i \\(H_v\\), a następnie z filtrów obliczana jest długość wektora gradientu krawędzi \\(E\\) i orientacja \\(\\Phi\\).\n\n\n\nRysunek 7.6: Przykładowy proces ekstrakcji krawędzi\n\n\n\n\n7.1.1.2 Filtry Roberts’a\nJako jeden z najprostszych i najstarszych detektorów krawędzi, operator Robertsa („Optical and Electro-Optical Information Processing”, b.d.) ma dziś głównie znaczenie historyczne. Wykorzystuje on dwa niezwykle małe filtry o rozmiarach 2 × 2 do estymacji gradientu kierunkowego wzdłuż przekątnych obrazu. Zdefiniowane są one za pomocą jądra\n\\[\nH^R_1= \\begin{bmatrix}\n  0&1\\\\\n  -1&0\n\\end{bmatrix}\\quad\\text{i}\\quad\nH^R_2= \\begin{bmatrix}\n  -1&0\\\\\n  0&1\n\\end{bmatrix}\n\\tag{7.11}\\]\nProjektowanie liniowych filtrów krawędziowych wiąże się z pewnym kompromisem: im silniej filtr reaguje na struktury podobne do krawędzi, tym bardziej wrażliwy jest na orientację. Innymi słowy, filtry niewrażliwe na orientację mają tendencję do reagowania na struktury nie będące krawędziami, podczas gdy najbardziej dyskryminujące filtry krawędziowe reagują tylko na krawędzie w wąskim zakresie orientacji. Jednym z rozwiązań jest zastosowanie nie tylko pojedynczej pary stosunkowo “szerokich” filtrów dla dwóch kierunków (takich jak Prewitta i Sobela), ale większego zestawu filtrów o ciasno rozłożonych orientacjach.\n\n\n7.1.1.3 Rozszerzony filtr Sobela\nTym razem zastosujemy 8 filtrów zmieniając orientację o \\(45\\degree\\).\n\n\n\n\n\n\n\n7.1.1.4 Filtr Kirscha\nInnym klasycznym operatorem kompasu jest operator zaproponowany przez Kirscha (Kirsch 1971), który również wykorzystuje osiem zorientowanych filtrów o następujących kernelach\n\n\n\n\n\nJednym z problemów z operatorami do wykrywania krawędzi opartymi na pierwszych pochodnych jest to, że każda wynikowa krawędź jest tak szeroka jak przedział zmiany intensywności, a zatem krawędzie mogą być trudne do precyzyjnego zlokalizowania. Alternatywna klasa operatorów krawędzi wykorzystuje drugie pochodne funkcji obrazu.\n\n\n\nRysunek 7.7: Zastosowanie drugiej pochodnej w detekcji krawędzi\n\n\nDruga pochodna funkcji mierzy jej lokalną krzywiznę. Idea jest taka, że krawędzie można znaleźć w miejscach zerowych lub - jeszcze lepiej - w miejscach zerowych drugich pochodnych funkcji obrazu, jak pokazano na Rysunek 7.7 dla przypadku 1D. Ponieważ drugie pochodne mają tendencję do wzmacniania szumu obrazu, zwykle stosuje się pewien rodzaj wygładzania wstępnego za pomocą odpowiednich filtrów dolnoprzepustowych.\nPopularnym przykładem jest operator “Laplacian-of-Gaussian” (LoG) (Marr i Hildreth 1980), który łączy wygładzanie gussowskie i obliczanie drugich pochodnych w jeden filtr liniowy. Przykład na Rysunek 7.8 pokazuje, że krawędzie uzyskane za pomocą operatora LoG są dokładniej zlokalizowane niż te dostarczone przez operatory Prewitta i Sobela.\nNiestety, wyniki działania prostych operatorów krawędziowych, o których mówiliśmy do tej pory, często odbiegają od tego, co my jako ludzie postrzegamy jako ważne krawędzie. Dwie główne przyczyny tego stanu rzeczy to:\n\nPo pierwsze, operatory krawędziowe reagują jedynie na lokalne różnice intensywności, podczas gdy nasz system wzrokowy jest w stanie rozpoznać krawędzie na obszarach o minimalnym lub zanikającym kontraście.\nPo drugie, krawędzie istnieją nie w jednej stałej rozdzielczości czy w pewnej skali, ale w całym szeregu różnych skal.\n\nTypowe małe operatory krawędzi, takie jak operator Sobela, mogą reagować tylko na różnice intensywności, które występują w obrębie ich regionów filtrów 3x3 piksele. Aby rozpoznać wtrącenia podobne do krawędzi w większym zakresie, potrzebowalibyśmy albo większych operatorów krawędzi (z odpowiednio dużymi filtrami), albo użyć oryginalnych (małych) operatorów na zredukowanych (tj. przeskalowanych) obrazach. Jest to główna idea technik “multiresolution” (zwanych również “hierarchicznymi” lub “piramidowymi”), które tradycyjnie są wykorzystywane w wielu zastosowaniach przetwarzania obrazów. W kontekście wykrywania krawędzi, sprowadza się to zazwyczaj do wykrywania najpierw krawędzi na różnych poziomach skali, a następnie decydowania, która krawędź (jeśli w ogóle) na danym poziomie skali jest dominująca w każdej pozycji obrazu.\nW wielu sytuacjach, kolejnym krokiem po wzmocnieniu krawędzi (przez jakiś operator krawędziowy) jest wybór punktów krawędziowych, czyli binarna decyzja o tym, czy piksel obrazu jest punktem krawędziowym czy nie. Najprostszą metodą jest zastosowanie operacji progowania do długości wektora gradientu krawędzi dostarczonej przez operator krawędzi, przy użyciu stałej lub adaptacyjnej wartości progowej, co daje binarny obraz krawędzi lub “mapę krawędzi”.\nW praktyce mapy krawędzi rzadko zawierają idealne kontury, ale zamiast tego wiele małych, niepołączonych fragmentów konturów, przerwanych w miejscach o niewystarczającej sile krawędzi2. Po progowaniu puste miejsca nie zawierają oczywiście żadnej informacji o krawędziach, która mogłaby zostać wykorzystana w kolejnym kroku, np. do łączenia sąsiadujących segmentów krawędzi. Pomimo tej słabości, globalne progowanie jest często stosowane w tym momencie ze względu na swoją prostotę.2 wyrażonej długością wektora gradientu krawędzi\nIdea sekwencyjnego śledzenia konturów wzdłuż odkrytych punktów krawędziowych nie jest rzadkością i wydaje się dość prosta w założeniu. Rozpoczynając od punktu obrazu o dużej sile krawędzi, podąża się iteracyjnie w obu kierunkach, aż do momentu, gdy oba ślady spotkają się i powstanie zamknięty kontur. Niestety, istnieje kilka przeszkód, które sprawiają, że zadanie to jest trudniejsze niż się początkowo wydaje, m.in:\n\nkrawędzie mogą kończyć się w regionach zanikającego gradientu intensywności,\nprzecinające się krawędzie prowadzą do niejednoznaczności,\nkontury mogą się rozgałęziać w kilku kierunkach.\n\nZe względu na te problemy, śledzenie konturów zazwyczaj nie jest stosowane do obrazów oryginalnych lub obrazów o ciągłej wartości krawędzi, z wyjątkiem bardzo prostych sytuacji, takich jak wyraźne oddzielenie obiektów (pierwszego planu) od tła. Śledzenie konturów w segmentowanych obrazach binarnych jest oczywiście znacznie prostsze.\n\n\n7.1.1.5 Filtry Canny’ego\nOperator zaproponowany przez Canny’ego jest szeroko stosowany i nadal uważany za “state of the art”3 w detekcji krawędzi. Metoda ta stara się osiągnąć trzy główne cele:3 najwyższe osiągnięcie w danej dziedzinie\n\nzminimalizować liczbę fałszywych punktów krawędziowych,\nosiągnąć dobrą lokalizację krawędzi\ndostarczyć tylko pojedynczy znak na każdej krawędzi.\n\nWłaściwości te nie są zwykle osiągane przez proste operatory krawędziowe (najczęściej oparte na pierwszych pochodnych i późniejszym progowaniu). W swej istocie filtr Canny’ego jest metodą gradientową (opartą na pierwszych pochodnych), ale do precyzyjnej lokalizacji krawędzi wykorzystuje miejsca zerowe drugich pochodnych. Pod tym względem metoda ta jest podobna do detektorów krawędzi, które bazują na drugich pochodnych funkcji obrazu (Canny 1986).\n\n\n\nRysunek 7.8: Przykłady zastosowania różnych detektorów krawędzi\n\n\nW pełni zaimplementowany detektor Canny’ego wykorzystuje zestaw stosunkowo dużych, zorientowanych filtrów przy wielu rozdzielczościach obrazu i łączy poszczególne wyniki we wspólną mapę krawędzi. Często jednak stosuje się tylko jednoskalową implementację algorytmu z regulowanym promieniem filtra (parametr wygładzania \\(\\sigma\\)), która jednak przewyższa większość prostych operatorów krawędziowych. Ponadto algorytm ten daje nie tylko binarną mapę krawędzi, ale także połączone łańcuchy pikseli krawędziowych, co znacznie upraszcza kolejne etapy przetwarzania. Dlatego nawet w swojej podstawowej (jednoskalowej) postaci operator Canny’ego jest często preferowany w stosunku do innych metod detekcji krawędzi. W swojej podstawowej (jednoskalowej) postaci operator Canny’ego wykonuje następujące kroki:\n\nObróbka wstępna - Oryginalny obraz intensywności \\(I\\) jest najpierw wygładzany jądrem filtra gaussowskiego \\(H^{G,\\sigma}\\); jego szerokość \\(\\sigma\\) określa skalę przestrzenną, w której mają być wykrywane krawędzie. Następnie na wygładzony obraz \\(\\bar{I}\\) nakładane są filtry różnicowe pierwszego rzędu w celu obliczenia składowych \\(\\bar{I}_u,\\bar{I}_v\\) lokalnych wektorów gradientu. Następnie obliczana jest lokalna wielkość \\(E_{mag}\\) jako norma odpowiadającego wektora gradientu. Ze względu na późniejsze progowanie pomocne może być znormalizowanie wartości siły krawędzi do standardowego zakresu (np. do [0, 100]).\nLokalizacja krawędzi - Kandydackie piksele krawędziowe są izolowane poprzez lokalne “non-maximum suppression” siły krawędzi \\(E_{mag}\\). W tym kroku zachowywane są tylko te piksele, które reprezentują lokalne maksimum wzdłuż profilu 1D w kierunku gradientu, czyli prostopadle do stycznej krawędzi (patrz Rysunek 7.9). Chociaż gradient może być skierowany w dowolnym kierunku, to dla ułatwienia efektywnego przetwarzania stosuje się zwykle tylko cztery dyskretne kierunki. Piksel w pozycji \\((u,v)\\) jest traktowany jako kandydat na krawędź tylko wtedy, gdy wielkość jego gradientu jest większa niż obu jego bezpośrednich sąsiadów w kierunku określonym przez wektor gradientu \\((dx,dy)\\) w pozycji \\((u,v)\\). Jeśli piksel nie jest lokalnym maksimum, jego wartość siły krawędzi jest ustawiona na zero (tj. “stłumiona”).\nŚledzenie krawędzi i progowanie z histerezą - W ostatnim kroku zbiory połączonych punktów brzegowych są zbierane z wartości, które pozostały nietłumione w poprzedniej operacji. Dokonuje się tego za pomocą techniki zwanej “progowaniem z histerezą” z wykorzystaniem dwóch różnych wartości progowych , \\(t_{lo}\\) (przy czym \\(t_{hi} &gt; t_{lo}\\)). Obraz jest skanowany w poszukiwaniu pikseli o wielkości krawędzi \\(E_{nms}(u, v) \\geq t_{hi}\\). W każdym przypadku znalezienia takiego (wcześniej nie odwiedzanego) miejsca, uruchamiany jest nowy ślad krawędziowy i dodawane są do niego wszystkie połączone piksele krawędziowe \\((u',v')\\), dopóki \\(E_{nms}(u',v') \\geq t_{lo}\\). Pozostają tylko te ślady krawędzi, które zawierają co najmniej jeden piksel o wielkości krawędzi większej niż \\(t_{hi}\\) i żadnych pikseli o wielkości krawędzi mniejszej niż \\(t_{lo}\\). Typowe wartości progów dla 8-bitowych obrazów w skali szarości to \\(t_{hi} = 5.0\\) i \\(t_{lo} = 2.5\\).\n\n\n\n\nRysunek 7.9: Zasada działania filtru Canny’ego"
  },
  {
    "objectID": "edge.html#wyostrzanie-obrazu",
    "href": "edge.html#wyostrzanie-obrazu",
    "title": "7  Wykrywanie krawędzi i konturów",
    "section": "7.2 Wyostrzanie obrazu",
    "text": "7.2 Wyostrzanie obrazu\nWyostrzanie obrazów (ang. sharpening) jest częstym zadaniem, np. w celu poprawy ostrości po zeskanowaniu lub przeskalowaniu obrazu lub w celu wstępnej kompensacji późniejszej utraty ostrości w trakcie drukowania lub wyświetlania obrazu. Powszechnym podejściem do wyostrzania obrazu jest wzmacnianie wysokoczęstotliwościowych (ang. hight-frequency amplifying) składowych obrazu, które są głównie odpowiedzialne za postrzeganą ostrość obrazu i które występują przy szybkich zmianach intensywności. W dalszej części opisujemy dwie metody sztucznego wyostrzania obrazu, które opierają się na technikach podobnych do detekcji krawędzi.\n\n7.2.1 Filtr Laplace’a\n\n\n\nRysunek 7.10: Wyostrzanie obrazu za pomocą drugich pochodnych\n\n\nPopularną metodą lokalizacji szybkich zmian intensywności są filtry oparte na drugich pochodnych funkcji obrazu. Rysunek 7.10 ilustruje tę ideę dla obrazu 1D i ciągłej funkcji \\(f(x)\\). Druga pochodna \\(f''(x)\\) funkcji schodkowej pokazuje dodatni impuls na dolnym końcu przejścia i ujemny impuls na górnym końcu. Krawędź wyostrza się przez odjęcie pewnego ułamka w drugiej pochodnej \\(f''(x)\\) od oryginalnej funkcji \\(f(x)\\)\n\\[\n\\hat{f}(x)=f(x)-w\\cdot f''(x).\n\\tag{7.12}\\]\nW zależności od współczynnika wagowego \\(w\\geq0\\), wyrażenie w Równanie 7.12 powoduje, że funkcja intensywności przeskakuje po obu stronach krawędzi, co powoduje wyolbrzymienie krawędzi i zwiększenie postrzeganej ostrości.\nWyostrzenie funkcji 2D można uzyskać za pomocą drugich pochodnych w kierunku poziomym i pionowym połączonych tzw. operatorem Laplace’a. Operator Laplace’a \\(\\nabla^2\\) funkcji 2D \\(f(x, y)\\) jest zdefiniowany jako suma drugich pochodnych cząstkowych wzdłuż kierunków \\(x\\) i \\(y\\):\n\\[\n(\\nabla^2f)(x,y)=\\frac{\\partial^2 f}{\\partial^2 x}(x,y)+\\frac{\\partial^2 f}{\\partial^2 y}(x,y).\n\\tag{7.13}\\]\nPodobnie jak w przypadku pierwszych pochodnych, również drugie pochodne funkcji obrazu dyskretnego mogą być estymowane za pomocą zestawu prostych filtrów liniowych. Również w tym przypadku zaproponowano kilka wersji. Na przykład, dwa filtry 1D:\n\\[\n\\frac{\\partial^2 f}{\\partial^2 x}\\approx H_x^L= \\begin{bmatrix}\n  1 &-2 &1\n\\end{bmatrix}\\quad\\text{i}\\quad \\frac{\\partial^2 f}{\\partial^2 y}\\approx H_y^L=\n\\begin{bmatrix}\n  1\\\\\n  -2\\\\\n  1\n\\end{bmatrix}.\n\\tag{7.14}\\]\ndo szacowania drugich pochodnych odpowiednio wzdłuż kierunku \\(x\\) i \\(y\\), łączą się w filtr Laplace’a 2D:\n\\[\nH^L= \\begin{bmatrix}\n  0&1&0\\\\\n  1&-4&1\\\\\n  0&1&0\n\\end{bmatrix}.\n\\tag{7.15}\\]\nWyostrzanie odbywa się poprzez odjęcie przekształconego obrazu przez filtr Laplace’a od oryginalnego z odpowiednią wagą \\(w\\)\n\\[\nI'\\leftarrow I-w\\cdot (H^L*I).\n\\tag{7.16}\\]\nRysunek 7.11 przedstawia przykład zastosowania filtru Laplace’a \\(H^L\\) do obrazu w skali szarości, gdzie wyraźnie widoczne są pary dodatnio i ujemnych szczytów po obu stronach każdej krawędzi. Filtr wydaje się niemal izotropowy pomimo grubego przybliżenia za pomocą małych filtrów.\n\n\n\nRysunek 7.11: Porównanie filtru Laplace’a z filtrami drugich pochodnych\n\n\n\n\n7.2.2 Metoda nieostrej maski\nMetoda nieostrej maski (ang. unsharp masking) (USM) to technika wyostrzania krawędzi, która jest szczególnie popularna w astronomii, druku cyfrowym i wielu innych dziedzinach przetwarzania obrazów. Termin ten wywodzi się z klasycznej fotografii, gdzie ostrość obrazu była optycznie zwiększana poprzez połączenie go z wygładzoną (“nieostrą”) kopią. Proces ten jest w zasadzie taki sam dla obrazów cyfrowych.\nPierwszym krokiem w filtrze USM jest odjęcie od oryginału wygładzonej wersji obrazu, która uwydatnia krawędzie. Wynik ten nazywany jest “maską”. W fotografii analogowej wymagane wygładzenie uzyskiwano przez zwykłe rozogniskowanie obiektywu4. Następnie maska jest ponownie dodawana do oryginału, w taki sposób, że krawędzie w obrazie są wyostrzone. Podsumowując, kroki zaangażowane w filtrowanie USM to:4 utratę ostrości\n\nObraz maski \\(M\\) jest generowany przez odjęcie od obrazu oryginalnego \\(I\\) jego wygładzonej wersji, uzyskanej przez filtrowanie za pomocą filtru \\(\\tilde{H}\\) \\[\nM\\leftarrow I-(I*\\tilde{H})=I-\\tilde{I}.\n\\tag{7.17}\\] się, że jądro \\(\\tilde{H}\\) filtra wygładzającego jest znormalizowane.\nAby uzyskać wyostrzony obraz \\(\\check{I}\\), maska \\(M\\) jest dodawana do oryginalnego obrazu \\(I\\) z odpowiednią wagą \\(a\\), za pomocą której kontrolujemy stopień wyostrzenia \\[\n\\check{I}\\leftarrow I+a\\cdot M,\n\\tag{7.18}\\] zatem podstawiając Równanie 7.17 otrzymujemy \\[\n\\check{I}\\leftarrow I+a\\cdot (I-\\tilde{I})=(1+a)\\cdot I-a\\cdot \\tilde{I}.\n\\]\n\nJako filtr wygładzający \\(\\tilde{H}\\) można właściwie zastosować dowolny filtr wygładzający, choć najczęściej spotyka się w tym miejscu filtry gassowskie \\(H^{G,\\sigma}\\) o \\(\\sigma \\in [1,20]\\). Natomiast najczęściej przyjmowane wartości dla \\(a\\) mieszczą się w przedziale [0.2,4].\n\n\nKod\nlena &lt;- Rvision::image(\"images/lena.png\")\n\nlayout(matrix(1:6, 2, 3, byrow = TRUE))\n\nplot(lena)\nlena_blr &lt;- Rvision::gaussianBlur(lena, 10, 10, 5, 5)\nplot(lena_blr)\nmaska &lt;- lena-lena_blr\nplot(maska)\nlena_sharp &lt;- lena+maska\nplot(lena_sharp)\n\nlena_flt &lt;- Rvision::filter2D(lena, matrix(c(0,1,0,\n                                    1,-4,1,\n                                    0,1,0), byrow = T, ncol = 3))\nplot(lena_flt)\n(lena-lena_flt) |&gt;  plot()\n\n\n\n\n\nRysunek 7.12: Wyostrzenie obrazu Leny. Pierwszy obraz w górnym rzędzie to oryginał, środkowy w górnym rzędzie to rozmyty obraz Leny filtrem gaussowskim, obraz po prawej w górnym rzędzie to maska. Obraz po lewej stronie w dolnym rzędzie to wyostrzony obraz Leny metodą USM. Środkowy obraz w dolnym rzędzie to filtr Laplace’a nałożony na Lenę, a obraz po prawej w dolnym rzędzie to wyostrzony obraz Leny metodą Laplace’a\n\n\n\n\n\n\n\n\nCanny, John. 1986. „A Computational Approach to Edge Detection”. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\nDavis, Larry S. 1975. „A Survey of Edge Detection Techniques”. Computer Graphics and Image Processing 4 (3): 248–70. https://doi.org/10.1016/0146-664X(75)90012-X.\n\n\nKirsch, Russell A. 1971. „Computer Determination of the Constituent Structure of Biological Images”. Computers and Biomedical Research 4 (3): 315–28. https://doi.org/10.1016/0010-4809(71)90034-6.\n\n\nMarr, D., i E. Hildreth. 1980. „Theory of Edge Detection”. Proceedings of the Royal Society of London. Series B, Biological Sciences 207 (1167): 187–217. https://doi.org/10.1098/rspb.1980.0020.\n\n\n„Optical and Electro-Optical Information Processing”. b.d. MIT Press.\n\n\n„Picture Processing and Psychopictorics - 1st Edition”. b.d. https://www.elsevier.com/books/picture-processing-and-psychopictorics/lipkin/978-0-12-451550-5."
  },
  {
    "objectID": "morpho.html#identyfikacja-sąsiadów",
    "href": "morpho.html#identyfikacja-sąsiadów",
    "title": "8  Filtry morfologiczne",
    "section": "8.1 Identyfikacja sąsiadów",
    "text": "8.1 Identyfikacja sąsiadów\nW przypadku prostokątnych siatek pikseli powszechnie stosowane są dwie definicje “sąsiedztwa” ():\n\ncztery piksele sąsiadujące z danym pikselem w kierunku poziomym i pionowym;\nosiem sąsiadujących pikseli z analizowanym.\n\n\n\n\nRysunek 8.4: Definicje sąsiedztwa"
  },
  {
    "objectID": "morpho.html#podstawowe-operacje-morfologiczne",
    "href": "morpho.html#podstawowe-operacje-morfologiczne",
    "title": "8  Filtry morfologiczne",
    "section": "8.2 Podstawowe operacje morfologiczne",
    "text": "8.2 Podstawowe operacje morfologiczne\nZmniejszanie i zwiększanie to rzeczywiście dwie najbardziej podstawowe operacje morfologiczne, które są określane odpowiednio jako “erozja” (ang. erosion) i “dylatacja” (ang. dilation). Operacje te są jednak znacznie bardziej ogólne niż zilustrowane w przykładzie. Wykraczają one daleko poza usuwanie lub dołączanie warstw pojedynczych pikseli i - w połączeniu - mogą wykonywać znacznie bardziej złożone operacje.\n\n8.2.1 Element strukturyzujący\nPodobnie jak w przypadku macierzy współczynników filtru liniowego, właściwości filtru morfologicznego są określone przez elementy macierzy zwanej “elementem strukturyzującym”. W morfologii binarnej element strukturyzujący (podobnie jak sam obraz) zawiera tylko wartości 0, a “gorący punkt” (ang. hot spot) wyznacza początek układu współrzędnych \\(H\\) (Rysunek 8.5). Przy czym, hot spot nie musi znajdować się w środku elementu strukturyzującego, ani jego wartość nie musi być równa 1.\n\n\n\nRysunek 8.5: Element strukturyzujący\n\n\n\n8.2.1.1 Zbiór punktowy\nDla formalnej specyfikacji operacji morfologicznych pomocne jest czasem opisanie obrazów binarnych jako zbiorów punktów o współrzędnych 2D. Dla obrazu binarnego \\(I(u, v) \\in \\{0, 1\\}\\), odpowiadający zbiór punktów \\(\\mathcal{Q}_I\\) składa się z par współrzędnych \\(p = (u, v)\\) wszystkich pikseli pierwszego planu,\n\\[\n\\mathcal{Q}_I =\\{p\\vert I(p)=1\\}.\n\\tag{8.1}\\]\nOczywiście, jak pokazano na rys, nie tylko obraz binarny \\(I\\), ale również element strukturyzujący \\(H\\) może być opisany jako zbiór punktów.\n\n\n\nRysunek 8.6: Obraz binarny z elementem strukturyzującym\n\n\nDzięki temu, że opisujemy je jako zbiory punktów, podstawowe operacje na obrazach binarnych można również wyrazić jako proste operacje na zbiorach. Na przykład, odwrócenie obrazu binarnego \\(I \\rightarrow \\bar{I}\\) (czyli zamiana pierwszego planu i tła) jest równoważne zbudowaniu zbioru dopełniającego\n\\[\n\\mathcal{Q}_{\\bar{I}}=\\bar{\\mathcal{Q}}_I=\\{p\\in \\mathbb{Z}^2\\vert p\\in \\mathcal{Q}_I\\}.\n\\tag{8.2}\\]\nŁącząc dwa obrazy binarne \\(I_1\\) i \\(I_2\\) za pomocą operacji OR między odpowiadającymi im pikselami, otrzymany zbiór punktów jest unią indywidualnych zbiorów punktów \\(Q_{I_1}\\) i \\(Q_{I_2}\\). Ponieważ zbiór punktów \\(Q_I\\) jest tylko alternatywną reprezentacją obrazu binarnego \\(I\\) (tzn. \\(I = Q_I\\)), to w dalszej części będziemy używać synonimicznie obu notacji: obrazu i zbioru.\n\n\n8.2.1.2 Dylatacja\nDylatacja jest operacją morfologiczną, która odpowiada naszemu intuicyjnemu pojęciu “wzrastania”, omówionemu już wcześniej. Jako operacja na zbiorach, jest ona zdefiniowana jako\n\\[\nI\\oplus H= \\{(p+q)| p\\in I, q\\in H \\}.\n\\tag{8.3}\\]\n\n\n\nRysunek 8.7: Przykład dylatacji\n\n\nTak więc zbiór punktów powstały w wyniku dylatacji jest sumą (wektorową) wszystkich możliwych par punktów współrzędnych z oryginalnych zbiorów \\(I\\) i \\(H\\), co ilustruje prosty przykład na Rysunek 8.7. Alternatywnie można postrzegać dylatację jako powielenie elementu struktury \\(H\\) na każdy piksel pierwszego planu obrazu \\(I\\) lub, odwrotnie, powielenie obrazu \\(I\\) na każdy element pierwszego planu \\(H\\).\n\n\n8.2.1.3 Erozja\nQuasi-odwrotnością dylatacji jest operacja erozji, ponownie zdefiniowana w notacji zbiorów jako\n\\[\nI\\ominus H= \\{p\\in \\mathbb{Z}^2| (p+q)\\in I, q\\in H \\}.\n\\tag{8.4}\\]\nOperację tę można zinterpretować następująco. Pozycja \\(p\\) jest zawarta w wyniku \\(I \\ominus H\\) wtedy (i tylko wtedy), gdy element strukturyzujący \\(H\\) - po umieszczeniu w tej pozycji \\(p\\) - jest w całości zawarty w pikselach pierwszego planu oryginalnego obrazu. Rysunek 8.8 przedstawia przykład erozji.\n\n\n\nRysunek 8.8: Przykład erozji\n\n\nFiltr morfologiczny jest jednoznacznie określony przez (a) rodzaj operacji i (b) zawartość elementu strukturyzującego. Odpowiedni rozmiar i kształt elementu strukturyzującego zależy od zastosowania, rozdzielczości obrazu itp. W praktyce często stosuje się elementy strukturyzujące o quasi-kolistym kształcie.\n\n\n\nRysunek 8.9: Wyniki dylatacji i erozji z różnymi promieniami\n\n\nDylatacja z wykorzystaniem okrągłego (dyskowego) elementu strukturyzującego o promieniu \\(r\\) powoduje dodanie warstwy o szerokości \\(r\\) do każdej struktury pierwszoplanowej w obrazie. I odwrotnie, erozja z tym elementem strukturyzującym usuwa warstwy o tej samej szerokości. Na Rysunek 8.9 przedstawiono wyniki dylatacji i erozji z użyciem dyskowych elementów strukturyzujących o różnych średnicach, zastosowanych do oryginalnego obrazu z rysunku.\nWyniki dylatacji i erozji dla różnych innych elementów strukturyzujących pokazano na Rysunek 8.10.\n\n\n\nRysunek 8.10: Wyniki dylatacji i erozji z różnymi filtrami\n\n\n\n\n8.2.1.4 Otwarcie i zamknięcie\nZłożenia funkcji dylatacji i erozji w odpowiedniej kolejności tworzą nowe transformacje nazywane otwarcie (ang. opening) i zamknięcie (ang. closing). Są one prawdopodobnie najczęściej używanymi przekształceniami morfologicznymi.\nOtwarcie binarne \\(I\\circ H\\) oznacza erozję, po której następuje dylatacja z tym samym elementem strukturyzującym \\(H\\), czyli\n\\[\nI\\circ H=(I\\ominus H)\\oplus H.\n\\tag{8.5}\\]\nGłównym efektem otwarcia jest to, że wszystkie struktury pierwszego planu, które są mniejsze niż element strukturyzujący, są eliminowane w pierwszym kroku (erozja). Pozostałe struktury są wygładzane przez dylatację i powiększane w przybliżeniu do ich pierwotnego rozmiaru, jak pokazano na przykładach na Rysunek 8.11. Ten proces kurczenia się, a następnie wzrostu odpowiada idei eliminacji małych struktur, którą wstępnie naszkicowaliśmy wcześniej.\n\n\n\nRysunek 8.11: Przykładowe zastosowania opening i closing\n\n\nGdy odwrócimy kolejność erozji i dylatacji, otrzymaną operację nazywamy domknięciem i oznaczamy \\(I\\bullet H\\)\n\\[\nI\\bullet H=(I\\oplus H)\\ominus H.\n\\tag{8.6}\\]\nZamknięcie usuwa (zamyka) otwory i szczeliny w strukturach pierwszego planu, które są mniejsze niż element strukturyzujący \\(H\\). Kilka przykładów z typowymi elementami strukturyzującymi w kształcie dysku pokazano na Rysunek 8.11.\n\n\n8.2.1.5 Przerzedzanie\nRozrzedzanie (ang. thininng) zwana też szkieletyzacją (ang. skeletonization) jest powszechną techniką morfologiczną, której celem jest zmniejszenie struktur binarnych do maksymalnej szerokości jednego piksela bez dzielenia ich na wiele części. Osiąga się to poprzez iteracyjną erozję “warunkową”. Jest ona stosowana w lokalnym sąsiedztwie tylko wtedy, gdy pozostaje wystarczająco szeroka struktura i operacja nie powoduje separacji. Wymaga to, aby w zależności od lokalnej struktury obrazu, w każdej pozycji obrazu podejmowana była decyzja, czy można zastosować kolejny krok erozji, czy też nie. Operacja jest kontynuowana do momentu, gdy w obrazie wynikowym nie pojawią się już żadne zmiany. Wynika z tego, że w porównaniu ze zwykłą (“homogeniczną”) morfologią omówioną wcześniej, thinning jest kosztowny obliczeniowo. Częstym zastosowaniem rozrzedzania jest obliczanie “szkieletu” regionu binarnego, np. w celu strukturalnego dopasowania kształtów 2D.\nRozrzedzanie znane jest również pod terminami wykrywanie linii środkowej i transformacji osi środkowej. Istnieje wiele różnych implementacji o zróżnicowanej złożoności i wydajności. Poniżej opisujemy klasyczny algorytm autorstwa Zhang i Suen (1984) oraz jego implementację jako reprezentatywny przykład.\nWejściem do tego algorytmu jest obraz binarny \\(I\\), w którym piksele pierwszego planu mają wartość 1, a piksele tła wartość 0. Algorytm skanuje obraz i w każdej pozycji \\((u, v)\\) bada sąsiedztwo rozmiaru 3x3 z centralnym elementem \\(P\\) i otaczającymi go wartościami \\(N = (N_0, N_1, \\ldots , N_7)\\).\nW celu sklasyfikowania zawartości lokalnego sąsiedztwa \\(N\\) definiujemy najpierw funkcję\n\\[\nB(N)=N_0+N_1+\\dots+N_7=\\sum_{i=0}^7N_i,\n\\tag{8.7}\\]\nktóra zlicza piksele otaczającego tła. Definiujemy również tzw. liczbę łączności, która wyraża ile składowych binarnych jest połączonych poprzez bieżący piksel centralny w pozycji $(u, v)$. Wielkość ta jest równoważna liczbie przejść \\(1\\to 0\\) w ciągu \\((N_0, . . . , N_7, N_0)\\), lub wyrażona w sposób arytmetyczny\n\\[\nC(N)=\\sum_{i=0}^7N_i\\cdot [N_i-N_{(i+1) \\text{ mod }8}].\n\\tag{8.8}\\]\nRysunek 9.19 przedstawia kilka wybranych przykładów dla sąsiedztwa \\(N\\) oraz związane z nimi wartości funkcji \\(B(N)\\) i \\(C(N)\\). Na podstawie powyższych funkcji definiujemy ostatecznie dwa predykaty booleańskie \\(R_1,R_2\\) na sąsiedztwie \\(N\\).\n\n\n\nRysunek 8.12: Przebieg rozrzedzania\n\n\nW zależności od wyniku \\(R_1(N)\\) i \\(R_2(N)\\) piksel pierwszego planu na środkowej pozycji \\(N\\) jest albo usuwany (czyli erodowany), albo oznaczany jako nieusuwalny.\n\n\n\nRysunek 8.13: Kolejne iteracje rozrzedzania\n\n\nRysunek 8.13 ilustruje efekt rozrzedzania warstwa po warstwie wykonywanego przez kolejne iteracje tej procedury. W każdej iteracji selektywnie usuwana jest tylko jedna “warstwa” pikseli pierwszego planu."
  },
  {
    "objectID": "morpho.html#operacje-morfologiczne-w-skali-szarości-i-na-obrazach-kolorowych",
    "href": "morpho.html#operacje-morfologiczne-w-skali-szarości-i-na-obrazach-kolorowych",
    "title": "8  Filtry morfologiczne",
    "section": "8.3 Operacje morfologiczne w skali szarości i na obrazach kolorowych",
    "text": "8.3 Operacje morfologiczne w skali szarości i na obrazach kolorowych\nOperacje morfologiczne nie ograniczają się do obrazów binarnych, ale dotyczą również obrazów intensywnościowych (w skali szarości). W rzeczywistości, definicja morfologii skali szarości jest uogólnieniem morfologii binarnej, przy czym binarne operatory OR i AND zostały zastąpione przez arytmetyczne operatory MAX i MIN, odpowiednio. W konsekwencji procedury przeznaczone do morfologii w skali szarości mogą również wykonywać morfologię binarną (ale nie odwrotnie). W przypadku obrazów kolorowych, operacje w skali szarości są zwykle stosowane indywidualnie do każdego kanału kolorystycznego.\n\n8.3.1 Element strukturyzujący\nInaczej niż w schemacie binarnym, elementy strukturyzujące dla morfologii w skali szarości nie są zdefiniowane jako zbiory punktów, lecz jako funkcje rzeczywiste 2D, czyli\n\\[\nH(i,j)\\in\\mathbb{R}, \\quad (i,j)\\in\\mathbb{Z}^2.\n\\tag{8.9}\\]\nWartości w \\(H\\) mogą być ujemne lub zerowe. Zauważmy jednak, że w przeciwieństwie do konwolucji liniowej, elementy zerowe w morfologii skali szarości mają na ogół swój udział w wyniku. Element strukturyzujący dla morfologii w skali szarości musi zatem wyraźnie rozróżniać komórki zawierające wartość 0 i komórki puste, na przykład\n\n\n\n\n\n\n8.3.1.1 Dylatacja i erozja\nWynik dylatacji w skali szarości \\(I\\oplus H\\) definiujemy jako maksimum wartości w \\(H\\) dodane do wartości bieżącego podobrazu \\(I\\), czyli\n\\[\n(I\\oplus H)(u,v)=\\max_{(i,j)\\in H}(I(u+i,v+j)+H(i,j)).\n\\tag{8.10}\\]\nPodobnie wynik erozji w skali szarości to minimum różnic\n\\[\n(I\\ominus H)(u,v)=\\max_{(i,j)\\in H}(I(u+i,v+j)-H(i,j)).\n\\tag{8.11}\\]\nNa Rysunek 8.14 i Rysunek 8.15 zademonstrowano na prostym przykładzie podstawowy proces, odpowiednio, dylatacji i erozji w skali szarości.\n\n\n\nRysunek 8.14: Przykład dylatacji w skali szarości\n\n\n\n\n\nRysunek 8.15: Przykład erozji w skali szarości\n\n\n\n\n8.3.1.2 Otwarcie i domknięcie\nOtwarcie i domknięcie na obrazach w skali szarości definiuje się, identycznie jak w przypadku binarnym, jako operacje złożone z dylatacji i erozji z tym samym elementem strukturyzującym. Kilka przykładów pokazano na rysunku 9.27 dla elementów strukturyzujących w kształcie dysku oraz na rysunku 9.29 dla różnych niestandardowych elementów strukturyzujących. Zauważ, że można uzyskać interesujące efekty, szczególnie w przypadku elementów strukturyzujących przypominających kształtem pędzel lub inne wzory pociągnięć.\n\n\n\nRysunek 8.16: Przykład otwarcia i zamknięcia z elementem strukturyzującym w kształcie dysku\n\n\n\n\n\nRysunek 8.17: Przykłady otwarcia i domknięcia z zastosowaniem różnych kształtów elementu strukturyzującego\n\n\n\n\n\n\nZhang, T. Y., i C. Y. Suen. 1984. „A Fast Parallel Algorithm for Thinning Digital Patterns”. Communications of the ACM 27 (3): 236–39. https://doi.org/10.1145/357994.358023."
  },
  {
    "objectID": "fourier.html#przykłady-zastosowań-dft",
    "href": "fourier.html#przykłady-zastosowań-dft",
    "title": "9  Transformata Fouriera",
    "section": "9.1 Przykłady zastosowań DFT",
    "text": "9.1 Przykłady zastosowań DFT\nPoniższe przykłady demonstrują pewne podstawowe właściwości DFT na rzeczywistych obrazach. Wszystkie przykłady na Rysunek 9.5 - Rysunek 9.10 pokazują wyśrodkowane i skwantowane widmo z logarytmicznymi wartościami natężenia.\n\n9.1.1 Skalowanie\nNa Rysunek 9.5 widać, że skalowanie obrazu w przestrzeni sygnału ma odwrotny efekt w przestrzeni częstotliwości\n\n\n\nRysunek 9.5: Skalowanie\n\n\n\n\n9.1.2 Powtarzane wzorce\nObrazy na Rysunek 9.6 zawierają powtarzające się wzory periodyczne w różnych orientacjach i skalach. Pojawiają się one jako wyraźne piki w odpowiednich miejscach w widmie.\n\n\n\nRysunek 9.6: Powtarzający się wzór\n\n\n\n\n9.1.3 Rotacje\nRysunek 9.7 pokazuje, że obrót obrazu o pewien kąt \\(\\alpha\\) powoduje obrót widma w tym samym kierunku i - jeśli obraz jest kwadratowy - o ten sam kąt.\n\n\n\nRysunek 9.7: Rotacje obrazu\n\n\n\n\n9.1.4 Struktury zorientowane, wydłużone\nObrazy przedmiotów pochodzenia ludzkiego często wykazują regularne wzory lub wydłużone struktury, które pojawiają się dominująco w widmie. Obrazy na Rysunek 9.7 pokazują kilka wydłużonych struktur, które pojawiają się w widmie jako jasne smugi zorientowane prostopadle do głównego kierunku wzorów obrazu.\n\n\n\nObrazy z wyraźnymi wzorcami\n\n\n\n\n9.1.5 Obrazy naturalne\nProste i regularne struktury są zwykle mniej dominujące w obrazach naturalnych obiektów i scen, a zatem efekty wizualne w widmie nie są tak oczywiste jak w przypadku obiektów sztucznych. Kilka przykładów tej klasy obrazów pokazano na Rysunek 9.8 i Rysunek 9.9.\n\n\n\nRysunek 9.8: Wzorce występujące w naturze\n\n\n\n\n\nRysunek 9.9: Wzorce naturalne\n\n\n\n\n9.1.6 Wzorce druku\nRegularne wzory generowane przez powszechnie stosowane techniki druku rastrowego (Rysunek 9.10) są typowymi przykładami periodycznych struktur wielokierunkowych, które wyraźnie wyróżniają się w odpowiednim widmie Fouriera.\n\n\n\nRysunek 9.10: Wzorce druku rastrowego"
  },
  {
    "objectID": "fourier.html#zastosowania-dft",
    "href": "fourier.html#zastosowania-dft",
    "title": "9  Transformata Fouriera",
    "section": "9.2 Zastosowania DFT",
    "text": "9.2 Zastosowania DFT\nDFT można stosować na wiele sposobów w zagadnieniach z zakresu analizy obrazu. Nie mniej jednak najczęściej można spotkać połączenie DFT i filtrów liniowych. Polega to na zastosowaniu transformaty Fouriera do oryginalnego obrazu, a następnie nałożenie filtrów na widmo powstałe z DFT. Ostatecznie na przekształcony obraz (po DFT i filtrach) nakłada się odwrotną transformatę Fouriera.\nFiltrowanie w dziedzinie częstotliwości otwiera jeszcze jedną ciekawą perspektywę: odwrócenie efektów działania filtra, przynajmniej w ograniczonych warunkach.\nZałóżmy, że mamy obraz \\(g_{blur}\\), który został wygenerowany z oryginalnego obrazu \\(g_{orig}\\) przez pewien filtr liniowy, na przykład rozmycie ruchu wywołane przez poruszającą się kamerę. Przy założeniu, że ta modyfikacja obrazu może być wystarczająco zamodelowana przez funkcję filtru liniowego \\(h_{blur}\\) możemy stwierdzić, że\n\\[\ng_{blur}(u,v)=(g_{orig}*h_{blur})(u,v).\n\\tag{9.6}\\]\nPrzypomnijmy, że w przestrzeni częstotliwości odpowiada to mnożeniu odpowiednich widm, czyli\n\\[\nG_{blur}(m,n)=G_{orig}(m,n)\\cdot H_{blur}(m,n)\n\\] więc powinno być możliwe odtworzenie oryginalnego (nie zamazanego) obrazu poprzez obliczenie odwrotnej transformaty Fouriera wyrażenia\n\\[\nG_{orig}(m,n)=\\frac{G_{blur}(m,n)}{H_{blur}(m,n)}.\n\\] Niestety “odwrotny filtr” działa tylko wtedy, gdy współczynniki spektralne \\(H_{blur}\\) są niezerowe, ponieważ w przeciwnym razie wartości wynikowe są nieskończone. Jednak nawet małe wartości \\(H_{blur}\\), które są typowe dla wysokich częstotliwości, prowadzą do dużych współczynników w zrekonstruowanym widmie i w konsekwencji do dużych ilości szumów obrazu. Ważne jest również dokładne przybliżenie rzeczywistej funkcji filtru, gdyż w przeciwnym razie zrekonstruowany obraz może silnie odbiegać od oryginału. Przykład na rysunku Rysunek 9.11 przedstawia obraz rozmyty przez płynny ruch poziomy, którego efekt można łatwo zamodelować jako liniową konwolucję. Jeśli funkcja filtrująca, która spowodowała rozmycie, jest dokładnie znana, to rekonstrukcja oryginalnego obrazu może być wykonana bez problemów (Rysunek 9.11 (b)). Jednak, jak widać na Rysunek 9.11 (c), duże błędy pojawiają się, jeśli filtr odwrotny tylko nieznacznie odbiega od filtra rzeczywistego, co szybko czyni metodę bezużyteczną.\n\n\n\nRysunek 9.11: Przykłady usunięcia efektu rozmycia (poruszenia) za pomocą DFT\n\n\nIstnieje kilka transformacji spektralnych, które mają właściwości podobne do DFT, ale nie działają na dziedzinie zespolonej. Dyskretna transformata kosinusowa (DCT) jest dobrze znanym przykładem, który jest szczególnie interesujący w naszym kontekście, ponieważ jest często używany do kompresji obrazów i wideo. DCT wykorzystuje jako funkcje bazowe wyłącznie funkcje kosinus różnych częstotliwości i operuje na sygnałach i współczynnikach spektralnych o wartościach rzeczywistych. Podobnie istnieje również dyskretna transformata sinusoidalna (DST) oparta na układzie funkcji sinusoidalnych."
  },
  {
    "objectID": "deeplearning.html#uczenie-się-reprezentacji-na-podstawie-danych",
    "href": "deeplearning.html#uczenie-się-reprezentacji-na-podstawie-danych",
    "title": "10  Deep learning",
    "section": "10.1 Uczenie się reprezentacji na podstawie danych",
    "text": "10.1 Uczenie się reprezentacji na podstawie danych\nNa to aby zdefiniować głębokie uczenie i zrozumieć różnicę między głębokim uczeniem a innymi podejściami do uczenia maszynowego, najpierw musimy mieć pewne pojęcie o tym, co robią algorytmy uczenia maszynowego. Właśnie stwierdziliśmy, że uczenie maszynowe odkrywa reguły wykonywania zadania przetwarzania danych, biorąc pod uwagę przykłady tego, co jest oczekiwane na wyjściu. Zatem, aby przeprowadzić uczenie maszynowe, potrzebujemy trzech rzeczy:\n\nPunkty danych wejściowych - na przykład, jeśli zadaniem jest rozpoznawanie mowy, tymi punktami danych mogą być pliki dźwiękowe osób mówiących. Jeśli zadanie polega na oznaczaniu obrazów, mogą to być pliki z obrazami.\nPrzykłady oczekiwanych wyników - w zadaniu rozpoznawania mowy mogą to być generowane przez człowieka transkrypcje plików dźwiękowych. W zadaniu dotyczącym obrazów, oczekiwanymi danymi wyjściowymi mogą być tagi takie jak “pies”, “kot” itd.\nSposób pomiaru, czy algorytm dobrze wykonuje swoją pracę - jest on niezbędny do określenia odległości między aktualnym wyjściem algorytmu a jego oczekiwanym wyjściem. Pomiar jest używany jako sygnał zwrotny do dostosowania sposobu działania algorytmu. Ten krok dostosowawczy nazywamy uczeniem się modelu.\n\nModel uczenia maszynowego przekształca dane wejściowe w sensowne dane wyjściowe, a proces ten jest “uczony” przez ekspozycję na znane przykłady danych wejściowych i wyjściowych. Dlatego centralnym problemem w uczeniu maszynowym i głębokim uczeniu jest sensowne przekształcanie danych: innymi słowy, uczenie się użytecznych reprezentacji danych wejściowych - reprezentacji, które przybliżają nas do oczekiwanych wyników. Zanim przejdziemy dalej: co to jest reprezentacja? W gruncie rzeczy jest to inny sposób patrzenia na dane - reprezentacja lub kodowanie danych. Na przykład, kolorowy obraz może być zakodowany w formacie RGB lub w formacie HSV (barwa-nasycenie-wartość): są to dwie różne reprezentacje tych samych danych. Niektóre zadania, które mogą być trudne do rozwiązania przy jednej reprezentacji, mogą stać się łatwe przy drugiej. Na przykład zadanie “wybierz wszystkie czerwone piksele na obrazie” jest prostsze w formacie RBG, natomiast “spraw, by obraz był mniej nasycony” jest prostsze w formacie HSV. Modele uczenia maszynowego polegają na znalezieniu odpowiednich reprezentacji dla danych wejściowych - przekształceń danych, które czynią je bardziej przydatnymi do wykonania zadania, np.zadania klasyfikacji.\nWszystkie algorytmy uczenia maszynowego polegają na automatycznym znajdowaniu takich przekształceń, które zmieniają dane w bardziej użyteczne reprezentacje dla danego zadania. Operacje te mogą być zmianami współrzędnych lub rzutami liniowymi, tłumaczeniami, operacjami nieliniowymi (takimi jak wybierz wszystkie punkty takie, że \\(x &gt;0\\)) i tak dalej. Algorytmy uczenia maszynowego zazwyczaj nie są kreatywne w znajdowaniu tych przekształceń; po prostu przeszukują wcześniej zdefiniowany zestaw operacji, zwany przestrzenią hipotez.\nTak więc, technicznie rzecz biorąc, uczenie maszynowe polega na poszukiwaniu użytecznych reprezentacji pewnych danych wejściowych, w ramach predefiniowanej przestrzeni możliwości, przy użyciu wskazówek pochodzących z jakiegoś sygnału zwrotnego. Ta prosta idea pozwala na rozwiązywanie niezwykle szerokiego zakresu zadań naturalnych dla człowieka, od rozpoznawania mowy po autonomiczne prowadzenie samochodu.\nGłębokie uczenie jest specyficzną dziedziną uczenia maszynowego: nowe podejście do uczenia się reprezentacji z danych, które kładzie nacisk na uczenie się kolejnych warstw coraz bardziej znaczących reprezentacji. Głębokie uczenie nie jest odniesieniem do jakiegokolwiek głębszego zrozumienia osiąganego przez to podejście; raczej oznacza ideę kolejnych warstw reprezentacji. To, ile warstw składa się na model danych, nazywane jest głębokością modelu. Innymi właściwymi nazwami dla tej dziedziny mogłyby być uczenie się reprezentacji warstwowych i uczenie się reprezentacji hierarchicznych. Nowoczesne uczenie głębokie często obejmuje dziesiątki, a nawet setki kolejnych warstw - i wszystkie one są uczone automatycznie na podstawie danych treningowych. Tymczasem inne podejścia do uczenia maszynowego koncentrują się na uczeniu się tylko jednej lub dwóch warstw reprezentacji danych; stąd czasem nazywa się je uczeniem płytkim.\nW głębokim uczeniu, te warstwowe reprezentacje są (prawie zawsze) uczone za pomocą modeli zwanych sieciami neuronowymi, zbudowanymi w dosłownych warstwach ułożonych jedna za drugą. Termin sieć neuronowa jest odniesieniem do neurobiologii, jednak mimo że niektóre z głównych koncepcji głębokiego uczenia zostały opracowane częściowo poprzez czerpanie inspiracji z naszego rozumienia mózgu, modele głębokiego uczenia nie są modelami mózgu. Nie ma dowodów na to, że mózg implementuje cokolwiek w rodzaju mechanizmów uczenia się wykorzystywanych w nowoczesnych modelach głębokiego uczenia. Możesz natknąć się na artykuły popularno-naukowe głoszące, że głębokie uczenie działa jak mózg lub było wzorowane na mózgu, ale to nie jest prawda. Byłoby to mylące, aby myśleć o głębokim uczeniu jako w jakikolwiek sposób związanym z neurobiologią. Dla naszych celów, głębokie uczenie jest matematyczną strukturą do uczenia się reprezentacji danych.\nJak wyglądają reprezentacje wyuczone przez algorytm głębokiego uczenia? Przyjrzyjmy się, jak sieć o głębokości kilku warstw (patrz Rysunek 10.2) przekształca obraz cyfry w celu rozpoznania, jaka to cyfra.\n\n\n\nRysunek 10.2: Schemat działania sieci rozpoznającej cyfry\n\n\nJak widać na Rysunek 10.3, sieć przekształca obraz cyfry w reprezentacje coraz bardziej różniące się od obrazu oryginalnego i coraz bardziej informujące o wyniku końcowym. Można myśleć o sieci głębokiej jak o wielostopniowej operacji destylacji informacji, gdzie informacja przechodzi przez kolejne filtry i wychodzi coraz bardziej oczyszczona (czyli przydatna w odniesieniu do jakiegoś zadania).\n\n\n\nRysunek 10.3: Przedstawienie zasady działania poszczególnych warstw sieci neuronowej w rozpoznawaniu cyfr\n\n\nTak właśnie wygląda głębokie uczenie, technicznie rzecz biorąc: jest to wieloetapowy sposób uczenia się reprezentacji danych. To prosty pomysł - ale jak się okazuje, bardzo proste mechanizmy, odpowiednio skalowane, mogą w końcu wyglądać jak magia."
  },
  {
    "objectID": "deeplearning.html#jak-działa-deep-learning",
    "href": "deeplearning.html#jak-działa-deep-learning",
    "title": "10  Deep learning",
    "section": "10.2 Jak działa deep learning?",
    "text": "10.2 Jak działa deep learning?\nW tym wiemy już, że uczenie maszynowe polega na mapowaniu danych wejściowych (takich jak obrazy) na dane docelowe (takie jak etykieta “kot”), co odbywa się poprzez obserwację wielu przykładów danych wejściowych i danych docelowych. Wiemy też, że głębokie sieci neuronowe wykonują odwzorowanie danych wejściowych na docelowe poprzez głęboką sekwencję prostych transformacji danych (warstwy) i że te transformacje danych są uczone przez ekspozycję na przykłady. Przyjrzyjmy się teraz, jak to uczenie przebiega, konkretnie.\nSpecyfikacja tego, co warstwa robi ze swoimi danymi wejściowymi, jest przechowywana w wagach warstwy (zwanych wagami synaptycznymi), które w istocie są zbiorem liczb. W sensie technicznym można powiedzieć, że transformacja wykonywana przez warstwę jest sparametryzowana przez jej wagi (patrz Rysunek 10.4). W tym kontekście uczenie oznacza znalezienie zestawu wartości dla wag wszystkich warstw w sieci, tak aby sieć poprawnie odwzorowywała przykładowe wejścia na przypisane im cele. Rzecz w tym, że głęboka sieć neuronowa może zawierać dziesiątki milionów parametrów. Znalezienie poprawnej wartości dla wszystkich z nich może wydawać się trudnym zadaniem, szczególnie biorąc pod uwagę fakt, że zmiana wartości jednego parametru wpłynie na zachowanie wszystkich pozostałych!\n\n\n\nRysunek 10.4: Sieć neuronowa parametryzowana przez wagi\n\n\nAby coś kontrolować, trzeba najpierw móc to obserwować. Aby kontrolować wyjście sieci neuronowej, musisz być w stanie zmierzyć, jak daleko to wyjście jest od tego, czego się spodziewałeś. Jest to zadanie funkcji straty sieci, zwanej również funkcją celu. Funkcja straty bierze predykcje sieci oraz prawdziwy wynik (to, co chciałeś, aby sieć “wypluła”) i oblicza wynik odległości, ujmując, jak dobrze sieć poradziła sobie z tym konkretnym przykładem (patrz Rysunek 10.5).\n\n\n\nRysunek 10.5: Funkcja straty mierząca jakość predykcji\n\n\nPodstawową sztuczką w uczeniu głębokim jest wykorzystanie wyniku jako sygnału zwrotnego do skorygowania wartości wag w kierunku, który obniży wynik straty dla bieżącego przykładu (patrz Rysunek 10.6). Ta korekta jest zadaniem optymalizatora, który implementuje coś, co nazywa się algorytmem wstecznej propagacji (ang. backpropagation): główny algorytm w uczeniu głębokim. W dalszej części wyjaśnimy bardziej szczegółowo, jak działa wsteczna propagacja.\n\n\n\nRysunek 10.6: Korekta wag wykorzystująca wartość funkcji straty\n\n\nPoczątkowo wagom sieci przypisane są losowe wartości, więc sieć wykonuje jedynie serię losowych przekształceń. Oczywiście jej wynik jest daleki od tego, jaki powinien być w idealnej sytuacji, a wynik funkcji straty jest bardzo wysoki. Ale z każdym przykładem, który sieć przetwarza, wagi są dostosowywane w prawidłowym kierunku, a wynik strat maleje. Jest to pętla treningowa, która powtarzana odpowiednią ilość razy (zwykle dziesiątki iteracji na tysiącach przykładów) daje wartości wag, które minimalizują funkcję straty. Sieć z minimalną stratą to taka, dla której wyjścia są tak bliskie celom, jak to tylko możliwe - sieć wytrenowana."
  },
  {
    "objectID": "deeplearning.html#krótki-rys-historyczny-dl",
    "href": "deeplearning.html#krótki-rys-historyczny-dl",
    "title": "10  Deep learning",
    "section": "10.3 Krótki rys historyczny DL",
    "text": "10.3 Krótki rys historyczny DL\nOkoło 2010 roku, mimo że sieci neuronowe były niemal całkowicie odrzucane przez ogół społeczności naukowej, kilka osób wciąż pracujących nad sieciami neuronowymi zaczęło dokonywać ważnych przełomów: grupy Geoffreya Hintona z Uniwersytetu w Toronto, Yoshua Bengio z Uniwersytetu w Montrealu, Yann LeCun z Uniwersytetu Nowojorskiego oraz IDSIA w Szwajcarii.\nW 2011 roku Dan Ciresan z IDSIA zaczął wygrywać akademickie konkursy klasyfikacji obrazów za pomocą trenowanych na GPU głębokich sieci neuronowych - był to pierwszy praktyczny sukces nowoczesnego uczenia głębokiego. Jednak przełomowy moment nastąpił w 2012 roku, gdy grupa Hintona wzięła udział w corocznym wyzwaniu ImageNet dotyczącym klasyfikacji obrazów na dużą skalę. Wyzwanie ImageNet było w tamtym czasie wyjątkowo trudne, polegało na klasyfikacji kolorowych obrazów o wysokiej rozdzielczości do 1000 różnych kategorii po przeszkoleniu na 1,4 mln obrazów. W 2011 roku dokładność zwycięskiego modelu, opartego na klasycznym podejściu do widzenia komputerowego, wyniosła zaledwie 74,3%. Następnie, w 2012 roku, zespół kierowany przez Alexa Krizhevsky’ego i wspierany przez Geoffreya Hintona był w stanie osiągnąć dokładność w pierwszej piątce1 na poziomie 83,6% - był to znaczący przełom. Od tego czasu co roku konkurs był zdominowany przez głębokie konwencjonalne sieci neuronowe. W 2015 roku zwycięzca osiągnął dokładność 96,4%, a zadanie klasyfikacji na ImageNet uznano za całkowicie rozwiązany problem.1 (ang. top 5 accuracy) - oznacza, że wśród 5 kategorii z najwyższym prawdopodobieństwem jest prawdziwa klasa\n\n\n\n\n\nOd 2012 r. głębokie konwolucyjne sieci neuronowe (CovNets - Convolutional Networks) stały się algorytmem pierwszego wyboru dla wszystkich zadań widzenia komputerowego. Na najważniejszych konferencjach poświęconych widzeniu komputerowemu w 2015 i 2016 r. niemal niemożliwe było znalezienie prezentacji, które w jakiejś formie nie wiązałyby się z CovNets. Jednocześnie głębokie uczenie znalazło zastosowanie w wielu innych typach problemów, takich jak np. przetwarzanie języka naturalnego. W szerokim zakresie zastosowań całkowicie zastąpiło klasyczne modele SVM i drzewa decyzyjne. Na przykład przez kilka lat Europejska Organizacja Badań Jądrowych (CERN), używała metod opartych na drzewach decyzyjnych do analizy danych cząstek z detektora ATLAS w Wielkim Zderzaczu Hadronów (LHC); ale CERN ostatecznie przeszedł na głębokie sieci neuronowe oparte na Keras ze względu na ich wyższą wydajność i łatwość szkolenia na dużych zbiorach danych.\nPodstawowym powodem, dla którego uczenie głębokie odniosło sukces tak szybko, jest to, że oferowało lepszą wydajność w wielu problemach. Ale to nie jest jedyny powód. Głębokie uczenie ułatwia również rozwiązywanie problemów, ponieważ całkowicie automatyzuje to, co kiedyś było najbardziej kluczowym krokiem w procesie uczenia maszynowego: inżynierię cech.\nPoprzednie techniki uczenia maszynowego - uczenie głębokie - polegały jedynie na przekształceniu danych wejściowych w jedną lub dwie kolejne przestrzenie reprezentacji, zwykle poprzez proste przekształcenia, takie jak wielowymiarowe projekcje nieliniowe (SVM) lub drzewa decyzyjne. Jednak wyrafinowane reprezentacje wymagane przez złożone problemy zazwyczaj nie mogą być realizowane przez wspomniane techniki. W związku z tym, ludzie musieli zadać sobie wiele trudu, aby uczynić początkowe dane wejściowe bardziej podatnymi na przetwarzanie przez te metody: to znaczy, musieli ręcznie oparcować dobre warstwy reprezentacji dla swoich danych. Nazywa się to inżynierią cech. Uczenie głębokie całkowicie automatyzuje ten krok: w przypadku uczenia głębokiego, uczysz się wszystkich cech w jednym przejściu i nie musisz ich samodzielnie opracowywać. To znacznie uprościło przepływy pracy związane z uczeniem maszynowym, często zastępując skomplikowane, wieloetapowe potoki jednym, prostym, kompleksowym modelem uczenia głębokiego.\nMożna zapytać, skoro sednem sprawy jest posiadanie wielu kolejnych warstw reprezentacji, to czy płytkie metody mogą być stosowane wielokrotnie, aby emulować efekty głębokiego uczenia? W praktyce, korzyść z zastosowania kilku metod płytkiego uczenia szybko maleje, ponieważ optymalna pierwsza warstwa reprezentacji w modelu trójwarstwowym nie jest optymalną pierwszą warstwą w modelu jedno- lub dwuwarstwowym. To, co jest przełomowe w uczeniu głębokim, to fakt, że pozwala ono modelowi uczyć się wszystkich warstw reprezentacji wspólnie, w tym samym czasie, a nie po kolei (zachłannie). Dzięki wspólnemu uczeniu cech, gdy model dostosowuje jedną ze swoich wewnętrznych cech, wszystkie inne cechy, które od niej zależą, automatycznie dostosowują się do tej zmiany, bez konieczności interwencji człowieka. Wszystko jest nadzorowane przez pojedynczy sygnał zwrotny: każda zmiana w modelu służy celowi końcowemu. Jest to znacznie potężniejsze niż składanie płytkich modeli, ponieważ pozwala na uczenie się złożonych, abstrakcyjnych reprezentacji poprzez rozbicie ich na długie serie pośrednich warstw; każda warstwa jest tylko prostym przekształceniem w stosunku do poprzedniej.\nSą to dwie zasadnicze cechy tego, jak głębokie uczenie uczy się z danych: przyrostowy, warstwa po warstwie sposób, w jaki korygowane są coraz bardziej złożone reprezentacje, oraz fakt, że te pośrednie, przyrostowe reprezentacje są uczone wspólnie, a każda warstwa jest aktualizowana, aby podążać zarówno za potrzebami reprezentacyjnymi warstwy powyżej, jak i potrzebami warstwy poniżej. Razem, te dwie właściwości sprawiły, że głębokie uczenie jest znacznie bardziej skuteczne niż poprzednie podejścia do uczenia maszynowego.\nŚwietnym sposobem na poznanie aktualnego krajobrazu algorytmów i narzędzi uczenia maszynowego jest przyjrzenie się konkursom uczenia maszynowego na Kaggle. Dzięki wysoce konkurencyjnemu środowisku (niektóre konkursy mają tysiące uczestników i milionowe nagrody) i szerokiej gamie problemów uczenia maszynowego, Kaggle oferuje realistyczny sposób oceny tego, co działa, a co nie. Jaki więc rodzaj algorytmu niezawodnie wygrywa konkursy? Z jakich narzędzi korzystają najlepsi uczestnicy?\n\nW 2016 roku Kaggle został zdominowany przez dwa podejścia: gradient boosting machines i deep learning. Konkretnie, gradient boosting jest używany do problemów, w których dostępne są ustrukturyzowane dane, podczas gdy głębokie uczenie jest używane do problemów percepcyjnych, takich jak klasyfikacja obrazów. Zwolennicy tego pierwszego rozwiązania prawie zawsze korzystają ze znakomitej biblioteki XGBoost. Tymczasem większość uczestników Kaggle wykorzystujących uczenie głębokie używa biblioteki Keras, ze względu na jej łatwość użycia i elastyczność. Zarówno XGBoost, jak i Keras wspierają dwa najpopularniejsze języki data science: R i Python.\n\n10.3.1 Hardware\nW latach 1990-2010 procesory dostępne na rynku stały się szybsze o około 5000 razy. W rezultacie, obecnie możliwe jest uruchomienie małych modeli głębokiego uczenia na laptopie, podczas gdy 25 lat temu byłoby to niewykonalne.\nJednak typowe modele głębokiego uczenia wykorzystywane w wizji komputerowej lub rozpoznawaniu mowy wymagają mocy obliczeniowej o kilka rzędów wielkości większej niż ta, którą może zapewnić laptop. Przez całą dekadę XXI wieku firmy takie jak NVIDIA i AMD inwestowały miliardy dolarów w rozwój szybkich, równoległych układów (procesorów graficznych [GPU]), które napędzały grafikę w coraz bardziej fotorealistycznych grach wideo - tanich, osobistych komputerów zaprojektowanych do renderowania złożonych scen 3D na ekranie w czasie rzeczywistym. Inwestycja ta przyniosła korzyści społeczności naukowej, gdy w 2007 roku NVIDIA wprowadziła CUDA (https://developer.nvidia.com/about-cuda), interfejs programistyczny dla swojej linii układów GPU. Niewielka liczba procesorów graficznych zaczęła zastępować klastry CPU w różnych złożonych zadaniach, począwszy od modelowania w fizyce. Głębokie sieci neuronowe, składające się głównie z wielu mnożeń macierzy, są również wysoce paralelizowalne i około 2011 roku niektórzy badacze zaczęli pisać implementacje CUDA sieci neuronowych - jednymi z pierwszych byli Dan Ciresan(Ciresan i in., b.d.) i Alex Krizhevsky(Krizhevsky, Sutskever, i Hinton 2017).\n\nStało się tak, że rynek gier dofinansował superkomputery dla następnej generacji aplikacji sztucznej inteligencji. Czasami wielkie rzeczy zaczynają się do zabawy 🙈. Dziś NVIDIA Titan X, procesor graficzny dla graczy, który kosztował 1000 dolarów pod koniec 2015 roku, może zapewnić szczytową wydajność 6,6 TLOPS w pojedynczej precyzji: to znaczy 6,6 biliona operacji float32 na sekundę. To około 350 razy więcej niż to, co można wyciągnąć z nowoczesnego laptopa. Na Tytanie X trenowanie modelu ImageNet, który kilka lat temu wygrałby konkurs ILSVRC, zajmuje zaledwie kilka dni. Tymczasem duże firmy trenują modele głębokiego uczenia na klastrach składających się z setek jednostek GPU, takich jak NVIDIA K80, opracowanych specjalnie na potrzeby głębokiego uczenia. Sama moc obliczeniowa takich klastrów jest czymś, co nigdy nie byłoby możliwe bez nowoczesnych procesorów graficznych.\nCo więcej, branża głębokiego uczenia zaczyna wychodzić poza procesory graficzne i inwestuje w coraz bardziej wyspecjalizowane, wydajne układy do głębokiego uczenia. W 2016 roku, na corocznej konwencji I/O, Google ujawniło swój projekt procesora tensorowego (TPU): nowy układ scalony opracowany od podstaw w celu uruchamiania głębokich sieci neuronowych, który jest podobno 10 razy szybszy i znacznie bardziej energooszczędny niż topowe układy GPU.\n\n\n10.3.2 Dane\nAI jest czasem zapowiadana jako nowa rewolucja przemysłowa. Jeśli głębokie uczenie jest maszyną parową tej rewolucji, to dane są jej węglem: surowcem, który zasila nasze inteligentne maszyny, bez którego nic nie byłoby możliwe. Jeśli chodzi o dane, to oprócz wykładniczego postępu w dziedzinie sprzętu do przechowywania danych w ciągu ostatnich 20 lat (zgodnie z prawem Moore’a2), kluczowym czynnikiem był rozwój Internetu, dzięki któremu możliwe stało się gromadzenie i rozpowszechnianie bardzo dużych zbiorów danych na potrzeby uczenia maszynowego. Obecnie duże firmy pracują z zestawami danych obrazowych, zestawami danych wideo i zestawami danych w języku naturalnym, które nie mogłyby zostać zebrane bez Internetu. Przykładowo, generowane przez użytkowników tagi do obrazów w serwisie Flickr są skarbnicą danych dla wizji komputerowej. Podobnie jest z filmami z YouTube. A Wikipedia jest kluczowym zbiorem danych dla przetwarzania języka naturalnego.2 mówi o tym, że liczba tranzystorów w procesorach rośnie wykładniczo\nJeśli jest jakiś zbiór danych, który stał się katalizatorem rozwoju głębokiego uczenia, to jest to zbiór danych ImageNet, składający się z 1,4 miliona obrazów, które zostały ręcznie przypisane do 1000 kategorii obrazów (1 kategoria na obraz). Jednak to, co czyni ImageNet wyjątkowym, to nie tylko jego duży rozmiar, ale także coroczny konkurs z nim związany. Jak pokazuje Kaggle od 2010 roku, publiczne konkursy są doskonałym sposobem motywowania naukowców i inżynierów do przekraczania granic. Posiadanie wspólnych benchmarków, które badacze starają się pokonać, bardzo pomogło w niedawnym rozwoju uczenia głębokiego.\n\n\n10.3.3 Algorytmy\nOprócz sprzętu i danych, aż do późnych lat 2000 brakowało nam niezawodnego sposobu trenowania bardzo głębokich sieci neuronowych. W rezultacie sieci neuronowe były wciąż dość płytkie, wykorzystując tylko jedną lub dwie warstwy reprezentacji; nie były więc w stanie zabłysnąć w porównaniu z bardziej wyrafinowanymi płytkimi metodami, takimi jak SVM czy lasy losowe. Kluczowym problemem była propagacja gradientu przez głębokie stosy warstw. Sygnał zwrotny używany do trenowania sieci neuronowych zanikał wraz ze wzrostem liczby warstw.\nZmieniło się to około 2009-2010 roku wraz z pojawieniem się kilku prostych, ale ważnych ulepszeń algorytmicznych, które pozwoliły na lepszą propagację gradientu:\n\nlepsze funkcje aktywacji dla warstw neuronowych;\nlepsze schematy inicjalizacji wag, począwszy od wstępnego szkolenia z podziałem na warstwy, które zostało szybko porzucone;\nlepsze schematy optymalizacji, takie jak RMSProp i Adam.\n\nDopiero gdy te ulepszenia zaczęły umożliwiać trenowanie modeli z 10 lub więcej warstwami, uczenie głębokie zaczęło błyszczeć. Wreszcie w latach 2014, 2015 i 2016 odkryto jeszcze bardziej zaawansowane sposoby wspomagania propagacji gradientu, takie jak normalizacja partii (ang. batch normalization), połączenia resztkowe (ang. residual connections) czy konwolucje separowalne w głąb (ang. depthwise separable convolutions). Dziś możemy trenować od podstaw modele, które mają tysiące warstw głębokości.\nCzy jest coś szczególnego w głębokich sieciach neuronowych, co sprawia, że są one “właściwym” podejściem dla firm, w które należy inwestować i dla naukowców, którzy chcą się nimi zainteresować? Czy może głębokie uczenie się jest tylko modą, która może nie przetrwać? Czy za 20 lat nadal będziemy używać głębokich sieci neuronowych?\nKrótka odpowiedź brzmi: tak 🙏 - głębokie uczenie ma kilka właściwości, które uzasadniają jego status jako rewolucji AI. Być może za dwie dekady nie będziemy używać sieci neuronowych, ale cokolwiek będziemy używać, będzie bezpośrednio dziedziczyć po nowoczesnym głębokim uczeniu i jego podstawowych koncepcjach. Najważniejsze właściwości można ogólnie podzielić na trzy kategorie:\n\nProstota - głębokie uczenie eliminuje potrzebę inżynierii cech, zastępując złożone, wrażliwe i wymagające inżynierii potoki prostymi, kompleksowo wytrenowanymi modelami, które są zazwyczaj budowane przy użyciu tylko pięciu lub sześciu różnych operacji na tensorach.\nSkalowalność - głębokie uczenie jest bardzo podatne na równoległe przetwarzanie na układach GPU lub TPU. Dodatkowo, modele głębokiego uczenia są trenowane poprzez iterację na małych partiach danych, co pozwala na ich trenowanie na zbiorach danych o dowolnym rozmiarze. (Jedynym wąskim gardłem jest ilość dostępnej mocy obliczeniowej).\nWszechstronność i możliwość ponownego wykorzystania - w przeciwieństwie do wielu wcześniejszych podejść do uczenia maszynowego, modele głębokiego uczenia mogą być trenowane na dodatkowych danych bez konieczności ponownego rozpoczynania od zera, co czyni je realnymi dla ciągłego uczenia się na bierząco - ważna właściwość dla bardzo dużych modeli produkcyjnych. Co więcej, wytrenowane modele głębokiego uczenia mogą być ponownie wykorzystane, na przykład, możliwe jest wzięcie modelu głębokiego uczenia wytrenowanego do klasyfikacji obrazów i wrzucenie go do potoku przetwarzania wideo."
  },
  {
    "objectID": "deeplearning.html#elementy-deep-learning",
    "href": "deeplearning.html#elementy-deep-learning",
    "title": "10  Deep learning",
    "section": "10.4 Elementy Deep Learning",
    "text": "10.4 Elementy Deep Learning\nZrozumienie głębokiego uczenia wymaga znajomości wielu prostych pojęć matematycznych: tensorów, operacji na tensorach, różniczkowania, spadku gradientu itp. Naszym celem w tym rozdziale będzie zbudowanie intuicji na temat tych pojęć bez nadmiernego zagłębiania się w technikę. W szczególności, będziemy unikać notacji matematycznej, która może być drażniąca dla osób nieposiadających żadnego wykształcenia matematycznego, a nie jest niezbędna do dobrego wytłumaczenia.\nAby dodać trochę kontekstu dla tensorów i spadku gradientu, rozpoczniemy podrozdział od praktycznego przykładu sieci neuronowej. Następnie przejdziemy przez każde nowe pojęcie, które zostało wprowadzone, punkt po punkcie. Pamiętaj, że pojęcia te będą niezbędne do zrozumienia praktycznych przykładów, które pojawią się w kolejnych rozdziałach!\nPrzyjrzyjmy się konkretnemu przykładowi sieci neuronowej, która wykorzystuje pakiet keras do nauki klasyfikacji ręcznie pisanych cyfr.\n\n\nKod\nlibrary(keras)\n\n\nProblemem, który postaramy się rozwiązać jest klasyfikacja obrazów pisma ręcznego w skali szarości (28 pikseli na 28 pikseli) do 10 kategorii (od 0 do 9). Użyjemy zestawu danych MNIST, klasycznego zestawu danych w społeczności ML, który istnieje prawie tak długo jak sama dziedzina i jest intensywnie badany. Jest to zestaw 60 000 obrazów treningowych oraz 10 000 obrazów testowych, zebranych przez National Institute of Standards and Technology (NIST w MNIST) w latach 80. Możesz myśleć o “rozwiązywaniu” MNIST jako o “Hello World” głębokiego uczenia - to jest to, co robisz, aby zweryfikować, że twoje algorytmy działają zgodnie z oczekiwaniami. Gdy staniesz się praktykiem uczenia maszynowego, zobaczysz, że MNIST pojawia się raz za razem, w pracach naukowych, wpisach na blogach i tak dalej. Kilka próbek MNIST można zobaczyć na Rysunek 10.7.\n\n\n\nRysunek 10.7: Kilka przykładów obrazów ze zbioru MNIST\n\n\n\n\n\n\n\n\nZagrożenie\n\n\n\nW uczeniu maszynowym kategoria w problemie klasyfikacyjnym nazywana jest klasą. Punkty danych są nazywane próbkami. Klasa związana z konkretną próbką nazywana jest etykietą (ang. label).\n\n\nZbiór danych MNIST jest wstępnie załadowany do keras w postaci list train i test, z których każda zawiera zestaw obrazów (x) i związanych z nimi etykiet (y):\n\n\nKod\nmnist &lt;- dataset_mnist()\ntrain_images &lt;- mnist$train$x\ntrain_labels &lt;- mnist$train$y\ntest_images &lt;- mnist$test$x\ntest_labels &lt;- mnist$test$y\n\n\ntrain_images i train_labels tworzą zbiór treningowy, czyli dane, na podstawie których model będzie się uczył. Model będzie następnie testowany na zbiorze testowym, test_images i test_labels . Obrazy są zakodowane jako tablice 3D, a etykiety to tablica 1D z cyframi od 0 do 9. Pomiędzy obrazami i etykietami istnieje korespondencja jeden do jednego.\n\n\nKod\nstr(train_images)\n\n\n int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nKod\nstr(train_labels)\n\n\n int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...\n\n\nKod\nstr(test_images)\n\n\n int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nKod\nstr(test_labels)\n\n\n int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...\n\n\nPrzepływ pracy (ang. workflow) będzie następujący: najpierw podamy sieci neuronowej dane treningowe, train_images i train_labels. Następnie sieć nauczy się kojarzyć obrazy i etykiety. Na koniec poprosimy sieć o stworzenie przewidywań dla test_images i sprawdzimy, czy te przewidywania pasują do etykiet z test_labels.\n\n\nKod\nnetwork &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 512, activation = \"relu\", input_shape = c(28 * 28)) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n\n\nPodstawowym elementem konstrukcyjnym sieci neuronowych jest warstwa, moduł przetwarzania danych, o którym można myśleć jak o filtrze dla danych. Niektóre dane przychodzą i wychodzą w bardziej użytecznej formie. W szczególności, warstwy wyodrębniają reprezentacje z danych, które są do nich wprowadzane - miejmy nadzieję, że reprezentacje, które są bardziej znaczące dla danego problemu. Większość głębokiego uczenia polega na łączeniu prostych warstw, które implementują formę stopniowej destylacji danych. Model głębokiego uczenia jest jak sito do przetwarzania danych, złożone z szeregu coraz bardziej wyrafinowanych filtrów danych - warstw.\n\n\n\n\n\nNasza sieć składa się z dwóch warstw, które są gęsto połączonymi (zwanymi też w pełni połączonymi - ang. fully connected) warstwami neuronowymi. Druga (i ostatnia) warstwa jest 10-kierunkową warstwą softmax, co oznacza, że zwróci tablicę 10 wyników prawdopodobieństwa (sumujących się do 1). Każdy wynik będzie oznaczał prawdopodobieństwo, że aktualny obraz cyfry należy do jednej z naszych 10 klas cyfr.\nAby sieć była gotowa do treningu, musimy wybrać jeszcze trzy rzeczy, w ramach kroku kompilacji:\n\nFunkcję straty - jak sieć będzie w stanie zmierzyć, jak dobrą pracę wykonuje na danych treningowych, a tym samym czy będzie w stanie kierować się we właściwym kierunku.\nOptymalizator - mechanizm, dzięki któremu sieć będzie się aktualizować w oparciu o dane, które widzi i swoją funkcję straty.\nMetryki, które należy monitorować podczas treningu i testów - tutaj zajmiemy się tylko dokładnością (frakcją obrazów, które zostały poprawnie sklasyfikowane - ang. accuracy).\n\nDokładne przeznaczenie funkcji straty i optymalizatora zostanie wyjaśnione w kolejnych rozdziałach.\n\n\nKod\nnetwork %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n\nZauważ, że funkcja compile() modyfikuje sieć na bieżąco (zamiast zwracać nowy obiekt sieci, co jest bardziej typowe dla R). Powód tego opiszemy, gdy powrócimy do przykładu w dalszej części rozdziału.\nPrzed treningiem wstępnie przetworzymy dane, zmieniając ich kształt na taki, jakiego oczekuje sieć, i skalując je tak, by wszystkie wartości mieściły się w przedziale [0, 1]. Poprzednio nasze obrazy treningowe, na przykład, były przechowywane w tablicy o kształcie (60000, 28, 28) typu integer z wartościami w przedziale [0, 255]. Przekształcamy go w podwójną tablicę kształtu (60000, 28 * 28) z wartościami w przedziale [0,1].\n\n\nKod\ntrain_images &lt;- array_reshape(train_images, c(60000, 28 * 28))\ntrain_images &lt;- train_images / 255\ntest_images &lt;- array_reshape(test_images, c(10000, 28 * 28))\ntest_images &lt;- test_images / 255\n\n\nZauważ, że używamy funkcji array_reshape() zamiast funkcji dim() do zmiany kształtu tablicy. Powód tego omówimy później, kiedy będziemy mówić o przekształcaniu tensorów. Musimy również zakodować etykiety w sposób kategoryczny.\n\n\nKod\ntrain_labels &lt;- to_categorical(train_labels)\ntest_labels &lt;- to_categorical(test_labels)\n\n\nTeraz jesteśmy gotowi do trenowania sieci, co w keras odbywa się poprzez wywołanie metody fit sieci - dopasowujemy model do danych treningowych:\n\n\nKod\nnetwork %&gt;% fit(train_images, train_labels, epochs = 5, batch_size = 128)\n\n\nPodczas treningu wyświetlane są dwie wielkości: strata sieci na danych treningowych oraz dokładność sieci na danych treningowych. Szybko osiągamy dokładność 0,989 (98,9%) na danych treningowych. Teraz sprawdźmy, czy model dobrze radzi sobie również na zbiorze testowym:\n\n\nKod\nmetrics &lt;- network %&gt;% evaluate(test_images, test_labels)\nmetrics\n\n\n      loss   accuracy \n0.07629414 0.97790003 \n\n\nDokładność zestawu testowego okazuje się wynosić 97,8% - to sporo mniej niż dokładność zestawu treningowego. Ta różnica między dokładnością treningu a dokładnością testu jest przykładem nadmiernego dopasowania (fakt, że modele uczenia maszynowego mają tendencję do osiągania gorszych wyników na nowych danych niż na danych treningowych). Wygenerujmy przewidywania dla pierwszych 10 próbek zbioru testowego:\n\n\nKod\nnetwork %&gt;% predict(test_images[1:10,]) |&gt; k_argmax()\n\n\ntf.Tensor([7 2 1 0 4 1 4 9 5 9], shape=(10), dtype=int64)\n\n\nNa tym kończymy nasz pierwszy przykład - właśnie zobaczyłeś, jak można zbudować i wytrenować sieć neuronową do klasyfikacji pisma ręcznego w mniej niż 20 liniach kodu R. W następnym rozdziale omówimy szczegółowo każdy element, który użyliśmy i wyjaśnimy, co on robi. Dowiesz się o tensorach, obiektach przechowujących dane w sieci; o operacjach na tensorach, z których składają się warstwy; oraz o spadku gradientu, który pozwala sieci uczyć się na przykładach treningowych.\n\n10.4.1 Operacje na danych\nW poprzednim przykładzie zaczynaliśmy od danych przechowywanych w wielowymiarowych tablicach, zwanych również tensorami. Wszystkie obecne systemy uczenia maszynowego używają tensorów jako podstawowej struktury danych. Tensory są fundamentalne dla tej dziedziny - tak fundamentalne, że Google’s TensorFlow został nazwany na ich cześć. Czym więc jest tensor?\nTensory są uogólnieniem wektorów i macierzy na dowolną liczbę wymiarów (zauważ, że w kontekście tensorów “wymiar” jest często nazywany “osią”). W R wektory są używane do tworzenia i manipulowania tensorami 1D, a matryce są używane do tensorów 2D. Dla wymiarów wyższego rzędu używane są obiekty tablicowe (które obsługują dowolną liczbę wymiarów).\n\n10.4.1.1 Skalary\nTensor, który zawiera tylko jedną liczbę nazywany jest skalarem (lub tensorem skalarnym, lub tensorem 0-wymiarowym, lub tensorem 0D). Chociaż R nie ma typu danych do reprezentowania skalarów (wszystkie obiekty numeryczne są wektorami, macierzami lub tablicami), wektor R, który ma zawsze długość 1, jest koncepcyjnie podobny do skalara.\n\n\n10.4.1.2 Wektory\nJednowymiarowa tablica liczb nazywana jest wektorem lub tensorem 1D. Mówi się, że tensor 1D ma dokładnie jedną oś. Możemy przekonwertować wektor R na obiekt tablicowy (array), aby sprawdzić jego wymiary:\n\n\nKod\nx &lt;- c(12, 3, 6, 14, 10)\nstr(x)\n\n\n num [1:5] 12 3 6 14 10\n\n\nKod\ndim(as.array(x))\n\n\n[1] 5\n\n\nWektor ten ma pięć wpisów i dlatego nazywany jest wektorem 5-wymiarowym. Nie należy mylić wektora 5D z tensorem 5D! Wektor 5D ma tylko jedną oś i ma pięć wymiarów wzdłuż swojej osi, podczas gdy tensor 5D ma pięć osi (i może mieć dowolną liczbę wymiarów wzdłuż każdej osi). Wymiarowość może oznaczać albo liczbę wpisów wzdłuż konkretnej osi (jak w przypadku naszego wektora 5D), albo liczbę osi w tensorze (jak tensor 5D), co może być czasem mylące. W tym drugim przypadku technicznie poprawniej jest mówić o tensorze rzędu 5 (ranga tensora to liczba osi), ale niejednoznaczny zapis tensor 5D jest powszechny niezależnie od tego.\n\n\n10.4.1.3 Macierze\nDwuwymiarowa tablica liczb to macierz, czyli tensor 2D. Macierz ma dwie osie (często nazywane wierszami i kolumnami). Możesz wizualnie zinterpretować macierz jako prostokątną siatkę liczb:\n\n\nKod\nx &lt;- matrix(rep(0, 3*5), nrow = 3, ncol = 5)\nx\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    0\n[3,]    0    0    0    0    0\n\n\nKod\ndim(x)\n\n\n[1] 3 5\n\n\n\n\n10.4.1.4 Tensory\nJeśli spakujemy takie macierze do nowej tablicy, otrzymamy tensor 3D, który możemy wizualnie zinterpretować jako sześcian liczb:\n\n\nKod\nx &lt;- array(rep(0, 2*3*2), dim = c(2,3,2))\nstr(x)\n\n\n num [1:2, 1:3, 1:2] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nKod\ndim(x)\n\n\n[1] 2 3 2\n\n\nPakując tensory 3D w tablicy, możesz stworzyć tensor 4D i tak dalej. W głębokim uczeniu się, generalnie będziesz manipulować tensorami, które są 0D do 4D, a tensory 5D pojawią się, jeśli będziesz przetwarzać dane wideo.\n\n\n10.4.1.5 Kluczowe własności\nTensor jest określony przez trzy kluczowe atrybuty:\n\nLiczba osi - np. tensor 3D ma trzy osie, a macierz dwie osie.\nKształt - to wektor liczb całkowitych, który opisuje, ile wymiarów ma tensor wzdłuż każdej osi. Na przykład poprzedni przykład macierzy ma kształt (3, 5), a przykład tensora 3D ma kształt (3, 3, 5). Wektor ma kształt z pojedynczym elementem, o wymiarze 5. Możesz sprawdzić wymiary dowolnej tablicy za pomocą funkcji dim().\nTyp danych - typ danych zawartych w tensorze; na przykład, typem tensora może być liczba całkowita lub podwójna3. W rzadkich przypadkach możesz zobaczyć tensor znakowy4. Ponieważ jednak tensory żyją we wstępnie przydzielonych segmentach pamięci, a łańcuchy znaków, będąc zmiennej długości, wykluczyłyby użycie tej implementacji, są one rzadziej używane.\n\n3 float - czyli liczba rzeczywista4 typu characterAby to skonkretyzować, spójrzmy na dane, które przetwarzaliśmy w przykładzie MNIST. Najpierw ładujemy zbiór danych MNIST:\n\n\nKod\ntrain_images &lt;- mnist$train$x\ntrain_labels &lt;- mnist$train$y\ntest_images &lt;- mnist$test$x\ntest_labels &lt;- mnist$test$y\nlength(dim(train_images)) # trzy osie\n\n\n[1] 3\n\n\nKod\ndim(train_images) # kształt\n\n\n[1] 60000    28    28\n\n\nKod\ntypeof(train_images) # typ danych\n\n\n[1] \"integer\"\n\n\nJak więc widać jest to tensor 3D liczb całkowitych. Dokładniej, jest to tablica 60000 macierzy 28 × 28 liczb całkowitych. Każda taka macierz jest obrazem w skali szarości, o współczynnikach od 0 do 255. Wykreślmy piątą cyfrę w tym tensorze 3D:\n\n\nKod\ndigit &lt;- train_images[5,,]\nplot(as.raster(digit, max = 255))\n\n\n\n\n\n\n\n10.4.1.6 Operacje na tensorach\nW poprzednim przykładzie wybraliśmy konkretną cyfrę wzdłuż pierwszej osi za pomocą składni train_images[i,,]. Wybieranie konkretnych elementów w tensorze nazywa się tensor slicing. Przyjrzyjmy się operacjom tensor slicing, które można wykonać na tablicach R. Poniżej wybrano cyfry od #10 do #99 i umieszczono je w tablicy o kształcie (90, 28, 28):\n\n\nKod\nmy_slice &lt;- train_images[10:99,,]\ndim(my_slice)\n\n\n[1] 90 28 28\n\n\nGeneralnie, możesz wybrać pomiędzy dwoma dowolnymi indeksami wzdłuż każdej osi tensora. Na przykład, aby wybrać 14 × 14 pikseli w prawym dolnym rogu wszystkich obrazów, użyj:\n\n\nKod\nmy_slice &lt;- train_images[, 15:28, 15:28]\n\n\nGeneralnie, pierwszą osią we wszystkich tensorach danych, z którymi zetkniesz się w głębokim uczeniu, będzie oś próbek (czasami nazywana wymiarem próbek). W przykładzie MNIST, próbki to obrazy cyfr. Ponadto, modele głębokiego uczenia nie przetwarzają całego zbioru danych na raz, ale raczej dzielą dane na małe partie (ang. batch). Konkretnie, oto jedna partia naszych cyfr MNIST, o rozmiarze partii 128:\n\n\nKod\nbatch &lt;- train_images[1:128,,]\n\n\nGdy rozważamy tensor partii, pierwsza oś nazywana jest osią partii lub wymiarem partii. Jest to termin, który często spotkasz podczas korzystania z keras i innych bibliotek uczenia głębokiego.\nUściślijmy tensory danych za pomocą kilku przykładów podobnych do tego, co napotkasz później. Dane, którymi będziesz manipulował, prawie zawsze będą należały do jednej z następujących kategorii:\n\nDane wektorowe-2D tensory kształtu (próbki, cechy);\nDane czasowe lub dane sekwencyjne-3D tensory kształtu (próbki, kroki czasowe, cechy);\nObrazy-4D tensory kształtu (próbki, wysokość, szerokość, kanały) lub (próbki, kanały, wysokość, szerokość);\nWideo-5D tensory kształtu (próbki, klatki, wysokość, szerokość, kanały) lub (próbki, klatki, kanały, wysokość, szerokość).\n\nDane wektorowe są najczęściej spotykanym przykładem formatu danych. W takim zbiorze danych każdy pojedynczy punkt danych może być zakodowany jako wektor, a więc partia danych będzie zakodowana jako tensor 2D (czyli tablica wektorów), gdzie pierwsza oś jest osią próbek, a druga osią cech. Przyjrzyjmy się dwóm przykładom:\n\nAktuarialny zbiór danych osób, gdzie rozpatrujemy wiek, kod i dochód każdej osoby. Każda osoba może być scharakteryzowana jako wektor 3 wartości, a zatem cały zbiór danych 100000 osób może być przechowywany w tensorze 2D o kształcie (100000, 3).\nZbiór dokumentów tekstowych, gdzie każdy dokument reprezentujemy poprzez zliczanie ile razy pojawia się w nim każde słowo (ze słownika 20000 słów). Każdy dokument można zakodować jako wektor 20000 wartości (po jednym zliczeniu na słowo w słowniku), a zatem cały zbiór 500 dokumentów można zapisać w tensorze kształtu (500, 20000).\n\nGdy w danych ważny się czas (lub kolejność sekwencji), sensowne jest przechowywanie ich w tensorze 3D z wyraźną osią czasu. Każda próbka może być zakodowana jako ciąg wektorów (tensor 2D), a zatem partia danych będzie zakodowana jako tensor 3D (patrz Rysunek 10.8).\n\n\n\nRysunek 10.8: Schemat zapisu danych sekwencyjnych\n\n\nOś czasu jest zawsze drugą osią, zgodnie z konwencją. Przyjrzyjmy się kilku przykładom:\n\nZbiór danych o cenach akcji. Co minutę zapisujemy aktualną cenę akcji, najwyższą cenę w danej minucie oraz najniższą cenę w danej minucie. Tak więc każda minuta jest zakodowana jako wektor 3D, cały dzień handlu jest zakodowany jako tensor 2D o kształcie (390, 3) (jest 390 minut w dniu handlu), a 250 dni danych może być przechowywanych w tensorze 3D o kształcie (250, 390, 3). W tym przypadku każda próbka to jeden dzień danych.\nZbiór danych tweetów, gdzie każdy tweet kodujemy jako ciąg 140 znaków z alfabetu 128 unikalnych znaków. W tym ustawieniu każdy znak może być zakodowany jako wektor binarny o rozmiarze 128 (wektor wszystkich zer, z wyjątkiem wpisu 1 w indeksie odpowiadającym znakowi). Następnie każdy tweet może być zakodowany jako tensor 2D o kształcie (140, 128), a zbiór danych 1 miliona tweetów może być przechowywany w tensorze o kształcie (1000000, 140, 128).\n\nObrazy mają zazwyczaj trzy wymiary: wysokość, szerokość i głębię koloru. Choć obrazy w skali szarości (jak nasze cyfry MNIST) mają tylko jeden kanał koloru i mogłyby być przechowywane w tensorach 2D, to konwencjonalnie tensory obrazów są zawsze trójwymiarowe, z jednowymiarowym kanałem koloru dla obrazów w skali szarości. Partia 128 obrazów w skali szarości o rozmiarach 256 × 256 mogłaby więc być przechowywana w tensorze o kształcie (128, 256, 256, 1), a partia 128 obrazów kolorowych - w tensorze o kształcie (128, 256, 256, 3) (patrz Rysunek 10.9).\n\n\n\nRysunek 10.9: Zapis kolorowego obrazu\n\n\nIstnieją dwie konwencje dla kształtów tensorów obrazów: konwencja channels-last (używana przez TensorFlow) i konwencja channels-first (używana przez Theano). Framework uczenia maszynowego TensorFlow, od Google, umieszcza oś głębokości koloru na końcu: (sample, height, width, color_depth). Tymczasem Theano umieszcza oś głębi koloru zaraz po osi wsadowej: (sample, color_depth, height, width). Keras zapewniają wsparcie dla obu formatów.\nDane wideo są jednym z niewielu typów danych ze świata rzeczywistego, dla których będziesz potrzebował tensorów 5D. Wideo może być rozumiane jako sekwencja klatek, z których każda jest obrazem kolorowym. Ponieważ każda klatka może być przechowywana w tensorze 3D (wysokość, szerokość, głębokość koloru), sekwencja klatek może być przechowywana w tensorze 4D (klatki, wysokość, szerokość, głębokość koloru), a zatem partia różnych filmów może być przechowywana w tensorze 5D o kształcie (próbki, klatki, wysokość, szerokość, głębokość koloru).\n\n\n\n\n\nNa przykład 60-sekundowy klip wideo YouTube o wymiarach 256 × 144, próbkowany z prędkością 4 klatek na sekundę, miałby 240 klatek. Partia czterech takich klipów wideo byłaby przechowywana w tensorze o kształcie (4, 240, 256, 144, 3). To w sumie 106168320 wartości! Jeśli typ danych tensora to double, to każda wartość jest przechowywana w 64 bitach, więc tensor reprezentowałby 810 MB. Sporo 😱! Filmy, które spotykasz w prawdziwym życiu są znacznie lżejsze, ponieważ nie są przechowywane w float32 i zazwyczaj są silnie skompresowane (jak w formacie MPEG).\nPodobnie jak każdy program komputerowy może być ostatecznie zredukowany do małego zestawu operacji binarnych na wejściach binarnych (AND, OR, NOR, i tak dalej), wszystkie transformacje uczone przez głębokie sieci neuronowe mogą być zredukowane do garści operacji tensorowych stosowanych do tensorów danych liczbowych. Na przykład, możliwe jest dodawanie tensorów, mnożenie tensorów i tak dalej. W naszym początkowym przykładzie budowaliśmy naszą sieć poprzez układanie gęstych warstw jedna na drugiej. Instancja warstwy wygląda tak:\n\n\nKod\nlayer_dense(units = 512, activation = \"relu\")\n\n\nWarstwa ta może być interpretowana jako funkcja, która przyjmuje na wejście tensor 2D i zwraca inny tensor 2D - nową reprezentację tensora wejściowego. Można ją też przedstawić inaczej (gdzie W jest tensorem 2D, a b jest wektorem):\n\n\nKod\n# nie wykonuj\noutput = relu(dot(W, input) + b)\n\n\nMamy tu trzy operacje na tensorach: iloczyn (dot) między tensorem wejściowym a tensorem W, dodawanie (+) między wynikowym tensorem 2D wektorem b i wreszcie operację relu, gdzie relu(x) to max(x, 0).\nOperacja relu i dodawanie są operacjami typu element-wise: operacjami, które są stosowane niezależnie do każdego wpisu w rozważanych tensorach. Oznacza to, że operacje te są bardzo podatne na wektoryzacje. W praktyce, gdy mamy do czynienia z tablicami R, operacje te są dostępne jako dobrze zoptymalizowane wbudowane funkcje R, które same delegują ciężką pracę do BLAS (Basic Linear Algebra Subprograms). BLAS to niskopoziomowe, wysoce wektoryzowalne, wydajne procedury manipulacji tensorami, zwykle zaimplementowane w Fortranie lub C.\nOperacje na tensorach można wykonywać również wtedy, gdy są one innych wymiarów. Przykładowo:\n\n\nKod\n# x is a tensor of random values with shape (64, 3, 32, 10)\nx &lt;- array(round(runif(1000, 0, 9)), dim = c(64, 3, 32, 10))\nstr(x)\n\n\n num [1:64, 1:3, 1:32, 1:10] 3 7 2 3 5 6 4 8 3 8 ...\n\n\nKod\n# y is a tensor of 5s of shape (32, 10)\ny &lt;- array(5, dim = c(32, 10))\nstr(y)\n\n\n num [1:32, 1:10] 5 5 5 5 5 5 5 5 5 5 ...\n\n\nKod\n# The output z has shape (64, 3, 32, 10) like x\nz &lt;- sweep(x, c(3, 4), y, pmax)\nstr(z)\n\n\n num [1:64, 1:3, 1:32, 1:10] 5 7 5 5 5 6 5 8 5 8 ...\n\n\nOperacja dot, zwana również iloczynem tensorowym jest najczęściej spotykaną, najbardziej użyteczną operacją na tensorach. W przeciwieństwie do operacji element-wise, łączy ona wpisy w tensorach wejściowych. Iloczyny tensorowe wykorzystują operator %*%. Zauważ, że gdy tylko jeden z dwóch tensorów ma więcej niż jeden wymiar, %*% nie jest symetryczny, czyli że x %*% y nie jest taki sam jak y %*% x.\n\n\n\nRysunek 10.10: Iloczyn tensorów\n\n\nTrzecim bardzo ważnym rodzajem operacji na tensorach jest przekształcanie tensorów. Chociaż nie było ono używane w gęstych warstwach w naszym pierwszym przykładzie sieci neuronowej, używaliśmy go, gdy wstępnie przetwarzaliśmy dane o cyfrach przed wprowadzeniem ich do naszej sieci:\n\n\nKod\ntrain_images &lt;- mnist$train$x\nstr(train_images)\n\n\n int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nKod\ntrain_images &lt;- array_reshape(train_images, c(60000, 28 * 28))\nstr(train_images)\n\n\n int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nPrzekształcenie tensora oznacza zmianę rozmieszczenia jego wierszy i kolumn tak, by pasowały do docelowego kształtu. Oczywiście, przekształcony tensor ma taką samą całkowitą liczbę współczynników jak tensor początkowy.\n\n\nKod\nx &lt;- matrix(c(0, 1,\n                2, 3,\n                4, 5),\n             nrow = 3, ncol = 2, byrow = TRUE)\nx\n\n\n     [,1] [,2]\n[1,]    0    1\n[2,]    2    3\n[3,]    4    5\n\n\nKod\nx &lt;- array_reshape(x, dim = c(6, 1)) \nx\n\n\n     [,1]\n[1,]    0\n[2,]    1\n[3,]    2\n[4,]    3\n[5,]    4\n[6,]    5\n\n\nKod\nx &lt;- array_reshape(x, dim = c(2, 3))\nx\n\n\n     [,1] [,2] [,3]\n[1,]    0    1    2\n[2,]    3    4    5\n\n\nSzczególnym przypadkiem przekształcenia, który jest powszechnie spotykany, jest transpozycja.\n\n\n\n10.4.2 Optymalizacja gradientowa\nJak widziałeś wyżej, każda warstwa neuronowa z naszego pierwszego przykładu sieci przekształca swoje dane wejściowe w następujący sposób:\n\n\nKod\noutput = relu(dot(input, W)+b)\n\n\nW tym wyrażeniu W i b są tensorami będącymi atrybutami warstwy. Nazywamy je wagami lub parametrami trenowanymi warstwy (odpowiednio atrybuty kernel i bias). Wagi te zawierają informacje wyuczone przez sieć w wyniku ekspozycji na dane treningowe.\nPoczątkowo te macierze wag są wypełnione małymi losowymi wartościami (krok zwany inicjalizacją losową). Oczywiście nie ma powodu, by oczekiwać, że relu(dot(W, input) + b), gdy W i b są losowe, da jakąkolwiek użyteczną reprezentację. Następnym krokiem jest stopniowe dostosowanie tych wag, w oparciu o sygnał zwrotny. To stopniowe dopasowywanie, zwane również treningiem, jest w zasadzie nauką, o którą chodzi w uczeniu modeli.\nDzieje się to w ramach tak zwanej pętli treningowej, która schematycznie wygląda następująco. Powtarzaj te kroki w pętli, tak długo jak to konieczne:\n\nWylosuj partię próbek treningowych x i odpowiadające im cele y.\nUruchom sieć na x (nazywane przejściem w przód), aby uzyskać predykcje y_pred.\nOblicz stratę sieci na partii, czyli miarę niedopasowania między y_pred a y.\nZaktualizuj wszystkie wagi sieci w taki sposób, aby nieznacznie zmniejszyć stratę na tej partii.\n\nW końcu otrzymamy sieć, która ma bardzo niską stratę na swoich danych treningowych: to znaczy, niskie rozbieżności między przewidywaniami y_pred i oczekiwanymi celami y. Ze wszystkich powyższych kroków pierwsze trzy wydają się oczywiste, natomiast 4 jest nieco trudniejszy - aktualizacja wag sieci. Biorąc pod uwagę indywidualny współczynnik wagowy w sieci, jak można obliczyć, czy współczynnik ten powinien być zwiększony czy zmniejszony i o ile?\nPonieważ wszystkie operacje używane w sieci są różniczkowalne, to można obliczyć gradient funkcji straty względem współczynników sieci. Następnie możesz skorygować współczynniki wagowe w kierunku przeciwnym do gradientu, zmniejszając w ten sposób stratę.\nBiorąc pod uwagę funkcję różniczkowalną, teoretycznie możliwe jest znalezienie jej minimum analitycznie: wiadomo, że minimum funkcji to punkt, w którym pochodna jest równa 0, więc wszystko, co musimy zrobić, to znaleźć wszystkie punkty, w których pochodna zmierza do 0 i sprawdzić, dla którego z tych punktów funkcja ma najmniejszą wartość.\nW przypadku sieci neuronowej oznacza to analityczne znalezienie kombinacji wartości wag, która daje najmniejszą możliwą funkcję straty. Można to zrobić rozwiązując równanie gradient(f)(W) = 0 dla W. Jest to równanie wielomianowe o \\(N\\) zmiennych, gdzie \\(N\\) jest liczbą współczynników w sieci. Chociaż możliwe byłoby rozwiązanie takiego równania dla dla \\(N = 2\\) lub \\(N = 3\\), to zrobienie tego jest niepraktyczne dla rzeczywistych sieci neuronowych, gdzie liczba parametrów nigdy nie jest mniejsza niż kilka tysięcy, a często może wynosić kilkadziesiąt milionów.\nZamiast tego możesz użyć czteroetapowego algorytmu przedstawionego na początku tej sekcji: modyfikujesz parametry po trochu w oparciu o bieżącą wartość straty na losowej partii danych. Ponieważ mamy do czynienia z funkcją różniczkowalną, możemy obliczyć jej gradient, co daje efektywny sposób implementacji kroku 4. Jeśli zaktualizujesz wagi w kierunku przeciwnym do gradientu, strata będzie za każdym razem nieco mniejsza:\n\nWylosuj partię próbek treningowych x i odpowiadające im cele y.\nUruchom sieć na x, aby uzyskać predykcje y_pred.\nOblicz stratę sieci na partii, czyli miarę niedopasowania między y_pred a y.\nOblicz gradient straty względem parametrów sieci (przejście wsteczne).\nSkoryguj (nieznacznie) parametry sieci w kierunku przeciwnym do gradientu - na przykład W = W - (krok * gradient) - tym samym zmniejszając nieco stratę na partii.\n\nTo, co właśnie opisaliśmy, nazywa się metodą minibatch stochastic gradient descent (minibatch SGD). Termin stochastyczny odnosi się do faktu, że każda partia danych jest losowana.\nJak widać, intuicyjnie ważne jest, aby wybrać rozsądną wartość współczynnika kroku. Jeśli jest on zbyt mały, zejście w dół krzywej zajmie wiele iteracji i może utknąć w lokalnym minimum. Jeśli krok jest zbyt duży, twoje aktualizacje mogą skończyć się zabraniem cię do całkowicie losowych miejsc na krzywej.\nZauważ, że wariant algorytmu mini-batch SGD polegający na losowaniu pojedynczej próbki i y w każdej iteracji, zamiast losowania partii danych. Wówczas byłby to oryginalny algorytm SGD, który jest mniej wydajny.\nOprócz wspomnianej metody mini-batch SGD istnieje wiele innych, dużo bardziej skutecznych metod optymalizacji parametrów wagowych modelu. Wśród nich należy wymienić:\n\nRMSprop\nAdagrad\nAdamax\nmini-batch SGD with Momentum\nmini-batch SGD with Nesterov Momentum\nAdam (chyba najpopularniejsza)\nNadam\n\nIstnieją również odmiany wspominanych wyżej metod z planami. Owe plany są przepisami na to jak współczynnik szybkości uczenia ma się zmieniać w kolejnych iteracjach.\n\n\n\nRysunek 10.11: Optymalizacja wag z wykorzystaniem różnych technik\n\n\n\n\n10.4.3 Wsteczna propagacja błędu\nWsteczna propagacja błędu (ang. backpropagation) jest metodą instruującą algorytm jak korygować wagi sieci. Korzysta ona z prawa znanego w matematyce jako pochodna funkcji złożonej (ang. chain rule)\n\\[\n[f(g(x))]'=f'(g(x))\\cdot g'(x).\n\\tag{10.1}\\]\nBackpropagation rozpoczyna się od końcowej wartości straty i działa wstecz od górnych warstw do dolnych, stosując regułę łańcuchową do obliczenia wkładu, jaki każdy parametr miał w wartości straty.\n\n\n\nRysunek 10.12: Zasada działania wstecznej propagacji błędu\n\n\nObecnie korzysta się z metod zdolnych do symbolicznego różniczkowania, takich jak TensorFlow. Oznacza to, że biorąc pod uwagę łańcuch operacji ze znaną pochodną, mogą obliczyć funkcję gradientu dla łańcucha (stosując regułę łańcucha), która mapuje wartości parametrów sieci do wartości gradientu. Kiedy masz dostęp do takiej funkcji, przejście wsteczne jest zredukowane do wywołania tej funkcji gradientu. Dzięki symbolicznemu różniczkowaniu nigdy nie będziesz musiał ręcznie implementować algorytmu wstecznej propagacji.\n\n\n10.4.4 Funkcje straty\nWe wcześniej prezentowanym przykładzie w funkcji compile() pojawiła się funkcja straty loss = 'categorical_crossentropy', która jest jedną z możliwych do zastosowania funkcji straty5. Funkcja straty jest używana jako sygnał zwrotny do uczenia tensorów wag i którą w fazie uczenia będzie monimalizowana. Odbywać się to będzie za pomocą mini-batch SGD. Dokładne zasady rządzące konkretną implementacją SGD są zdefiniowane przez optymalizator rmsprop przekazany jako pierwszy argument.5 o funkcjach straty będziemy jeszcze wspominać przy konkretnych przykładach\nUruchamiając\n\n\nKod\nnetwork |&gt; \n  fit(train_samples, train_labels, epochs = 5, batch_size = 128)\n\n\nrozpoczynamy iteracyjne uczenie przygotowanej sieci. Będzie ona realizowana w 5 epokach na partiach danych składających się ze 128 obserwacji.\n\n\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nBardzo pomocne w zrozumieniu zasady działania sieci neuronowych na przykładzie (bardzo podobnym do naszego) może być obejrzenie cyklu 4 filmów sieciach neuronowych kanału 3Blue1Brown\n\n\n\n\nKod\n# lista funkcji straty pakietu keras\n\nls(\"package:keras\") |&gt; \n  grep(x = _, pattern = \"^loss_\", value = T)\n\n\n [1] \"loss_binary_crossentropy\"            \n [2] \"loss_categorical_crossentropy\"       \n [3] \"loss_categorical_hinge\"              \n [4] \"loss_cosine_proximity\"               \n [5] \"loss_cosine_similarity\"              \n [6] \"loss_hinge\"                          \n [7] \"loss_huber\"                          \n [8] \"loss_kl_divergence\"                  \n [9] \"loss_kullback_leibler_divergence\"    \n[10] \"loss_logcosh\"                        \n[11] \"loss_mean_absolute_error\"            \n[12] \"loss_mean_absolute_percentage_error\" \n[13] \"loss_mean_squared_error\"             \n[14] \"loss_mean_squared_logarithmic_error\" \n[15] \"loss_poisson\"                        \n[16] \"loss_sparse_categorical_crossentropy\"\n[17] \"loss_squared_hinge\"                  \n\n\n\n\n\n\nCiresan, Dan C, Ueli Meier, Jonathan Masci, Luca M Gambardella, i Jurgen Schmidhuber. b.d. „Flexible, High Performance Convolutional Neural Networks for Image Classification”.\n\n\nKrizhevsky, Alex, Ilya Sutskever, i Geoffrey E. Hinton. 2017. „ImageNet Classification with Deep Convolutional Neural Networks”. Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386."
  },
  {
    "objectID": "fundamentals.html#podział-na-zbiór-uczący-walidacyjny-i-testowy",
    "href": "fundamentals.html#podział-na-zbiór-uczący-walidacyjny-i-testowy",
    "title": "11  Fundamenty DNN",
    "section": "11.1 Podział na zbiór uczący, walidacyjny i testowy",
    "text": "11.1 Podział na zbiór uczący, walidacyjny i testowy\nOcena modelu zawsze sprowadza się do podzielenia dostępnych danych na trzy zestawy: treningowy, walidacyjny i testowy. Trenujesz na danych treningowych i oceniasz swój model na danych walidacyjnych. Kiedy model jest już gotowy do użycia, testujesz go po raz ostatni na danych testowych.\n\n\n\n\n\nMożesz zapytać, dlaczego nie mieć dwóch zestawów: treningowego i testowego? Trenowałbyś na danych treningowych i oceniałbyś na danych testowych. To przecież o wiele prostsze!\nPowodem jest to, że opracowanie modelu zawsze wiąże się z dostrojeniem jego konfiguracji: na przykład wyborem liczby warstw lub rozmiaru warstw (zwanych hiperparametrami modelu, aby odróżnić je od parametrów, które są wagami sieci). Tuningu tego dokonasz, wykorzystując jako sygnał zwrotny wydajność modelu na danych walidacyjnych1. W istocie, to strojenie jest formą uczenia się: poszukiwaniem dobrej konfiguracji w pewnej przestrzeni parametrów. W rezultacie, dostrajanie hiperparametrów modelu na podstawie jego wydajności na zbiorze walidacyjnym może szybko doprowadzić do przeuczenia do zbioru walidacyjnego, nawet jeśli twój model nigdy nie jest bezpośrednio trenowany na nim.1 mierzoną najczęściej funkcją straty\nCentralnym punktem tego fenomenu jest pojęcie wycieku informacji. Za każdym razem, gdy dostrajasz hiperparametr swojego modelu w oparciu o jego wydajność na zbiorze walidacyjnym, niektóre informacje o danych walidacyjnych wyciekają do modelu. Jeśli zrobisz to tylko raz, dla jednego parametru, wtedy bardzo niewiele bitów informacji wycieknie, a twój zbiór walidacyjny pozostanie wiarygodny do oceny modelu. Ale jeśli powtórzysz to wiele razy - przeprowadzając jeden eksperyment, oceniając go na zbiorze walidacyjnym i modyfikując w rezultacie swój model - wtedy wycieknie coraz większa ilość informacji o zbiorze walidacyjnym do modelu.\nOstatecznie, skończysz z modelem, który działa podejrzanie dobrze na danych walidacyjnych, ponieważ właśnie po to go zoptymalizowałeś. Zależy Ci na wydajności na zupełnie nowych danych, a nie na danych walidacyjnych, więc musisz użyć zupełnie innego, nigdy wcześniej niewidzianego zbioru danych do oceny modelu: testowego zbioru danych. Twój model nie powinien mieć dostępu do żadnych informacji o zbiorze testowym, nawet pośrednio. Jeśli cokolwiek w modelu zostało dostrojone w oparciu o wydajność zbioru testowego, to Twoja miara uogólnienia będzie błędna.\nPodział danych na zbiory treningowe, walidacyjne i testowe może wydawać się prosty, ale istnieje kilka zaawansowanych sposobów, które mogą się przydać, gdy dostępnych jest niewiele danych. Przyjrzyjmy się trzem klasycznym przepisom oceny: zwykłej walidacji hold-out, walidacji krzyżowej K-krotnej oraz iterowanej krzyżowej walidacji K-krotnej z losowaniem.\n\n11.1.1 Hold-out\n\n\n\n\n\nWyodrębnij pewną część danych jako zbiór testowy. Trenuj na pozostałych danych i oceń na zbiorze testowym. Jak widziałeś w poprzednich rozdziałach, aby zapobiec wyciekowi informacji, nie powinieneś dostrajać swojego modelu na podstawie zbioru testowego, dlatego powinieneś również zarezerwować zbiór walidacyjny.\n\n\n\nRysunek 11.1: Walidacja hold-out\n\n\nSchematycznie, walidacja hold-out wygląda jak na Rysunek 11.1. Poniższy listing pokazuje prostą implementację.\n#| eval: false\n\n1indices &lt;- sample(1:nrow(data), size = 0.80 * nrow(data))\n2evaluation_data  &lt;- data[-indices, ]\n3training_data &lt;- data[indices, ]\n4model &lt;- get_model()\nmodel %&gt;% train(training_data)\nvalidation_score &lt;- model %&gt;% evaluate(validation_data)\n5model &lt;- get_model()\nmodel %&gt;% train(data)\ntest_score &lt;- model %&gt;% evaluate(test_data)\n\n1\n\nwylosuj indeksy zbioru uczącego;\n\n2\n\nzdefiniuj zbiór walidacyjny;\n\n3\n\nzdefiniuj zbiór uczący;\n\n4\n\nucz model na zbiorze uczący i sprawdzaj dopasowanie na walidacyjnym;\n\n5\n\nnaucz model na pełnym zestawie danych uczących (na połączonym zbiorze uczącym i walidacyjnym).\n\n\nJest to najprostszy protokół oceny, ale ma jedną wadę: jeśli dostępnych danych jest mało, wtedy twoje zestawy walidacyjne i testowe mogą zawierać zbyt mało próbek, aby być statystycznie reprezentatywne dla danych. Łatwo to rozpoznać: jeśli różne losowe rundy tasowania danych przed podziałem kończą się uzyskaniem bardzo różnych miar wydajności modelu, to masz ten problem.\n\n\n11.1.2 Walidacja krzyżowa K-krotna\nW tym podejściu dzielimy dane na K podzbiorów (foldów) o (w miarę) równym rozmiarze.Trenuj swój model na K-1 foldach, a na jednym pozostałym foldzie oceń jego jakość. Twój ostateczny wynik jest średnią z K uzyskanych wyników. Podobnie jak w przypadku walidacji typu hold-out, metoda ta nie zwalnia z używania odrębnego zbioru walidacyjnego do kalibracji modelu.\n\n\n\nRysunek 11.2: Walidacja krzyżowa K-krotna\n\n\nSchematycznie, K-krotna walidacja krzyżowa wygląda jak na Rysunek 11.2. Oto prosta implementacja pseudokodu w R.\n#| eval: false\n\nk &lt;- 4\nindices &lt;- sample(1:nrow(data))\nfolds &lt;- cut(indices, breaks = k, labels = FALSE)\nvalidation_scores &lt;- c()\n\nfor (i in 1:k) {\n  validation_indices &lt;- which(folds == i, arr.ind = TRUE)\n1  validation_data &lt;- data[validation_indices,]\n2  training_data &lt;- data[-validation_indices,]\n  \n3  model &lt;- get_model()\n  model %&gt;% train(training_data)\n  results &lt;- model %&gt;% evaluate(validation_data)\n  validation_scores &lt;- c(validation_scores, results$accuracy)\n}\n\n4validation_score &lt;- mean(validation_scores)\n\n5model &lt;- get_model()\nmodel %&gt;% train(data) \nresults &lt;- model %&gt;% evaluate(test_data)\n\n1\n\nwybierz obserwacje do zbioru walidacyjnego;\n\n2\n\nużyj pozostałych danych jako zbioru uczącego (fold);\n\n3\n\nstwórz model i ucz go na zbiorze uczącym (foldzie);\n\n4\n\noceń dopasowanie na zbiorze walidacyjnym (uśrednione z foldów);\n\n5\n\nucz model na pełnym zestawie uczącym.\n\n\n\n\n11.1.3 Iterowana metoda walidacji krzyżowej z losowaniem\n\n\n\n\n\nPrzeznaczona jest dla sytuacji, w których masz stosunkowo mało dostępnych danych i musisz jak najdokładniej ocenić swój model. Polega ona na wielokrotnym zastosowaniu K-krotnej walidacji, tasując dane za każdym razem przed podzieleniem ich na K sposobów. Końcowy wynik jest średnią z wyników uzyskanych w każdym przebiegu K-krotnej walidacji. Zauważ, że kończysz szkolenie i ocenę P * K modeli (gdzie P to liczba iteracji, których używasz), co może być bardzo kosztowne.\nW literaturze tematu metoda ta występuje również pod nazwą K-krotnego sprawdzianu krzyżowego z powtórzeniami.\n\n\n11.1.4 Uwagi do resamplingu\n\nReprezentatywność danych - chcesz, aby zarówno zbiór treningowy, jak i testowy były reprezentatywne dla danych. Na przykład, jeśli próbujesz sklasyfikować obrazy cyfr i zaczynasz od tablicy próbek, gdzie próbki są uporządkowane według ich klasy, to biorąc pierwsze 80% tablicy jako zbiór treningowy, a pozostałe 20% jako testowy, twój zbiór treningowy będzie zawierał tylko klasy 0-7, podczas gdy twój zbiór testowy zawiera tylko klasy 8-9. Wydaje się to niedorzecznym błędem, ale jest zaskakująco powszechne. Z tego powodu, zazwyczaj powinieneś losowo przetasować swoje dane przed podzieleniem ich na zestawy treningowe i testowe.\nOś czasu - jeśli próbujesz przewidzieć przyszłe wartości biorąc pod uwagę przeszłość (na przykład, pogoda jutro, zmiany cen akcji, i tak dalej), nie powinieneś losowo tasować swoich danych przed ich podziałem, ponieważ robiąc to, stworzysz czasowy wyciek danych: twój model będzie skutecznie trenowany na danych z przyszłości. W takich sytuacjach zawsze powinieneś upewnić się, że wszystkie dane w twoim zestawie testowym są potomne w stosunku do danych w zestawie treningowym.\nRedundancja w danych - Jeśli niektóre obserwacje pojawiają się dwukrotnie (co jest dość powszechne w przypadku danych ze świata rzeczywistego), to przetasowanie danych i podzielenie ich na zbiór treningowy i walidacyjny spowoduje redundancję pomiędzy zbiorem treningowym i walidacyjnym. W efekcie, będziesz testował na części danych treningowych, co jest najgorszą rzeczą jaką możesz zrobić! Upewnij się, że twój zestaw treningowy i zestaw walidacyjny są rozłączne."
  },
  {
    "objectID": "fundamentals.html#przygotowanie-danych",
    "href": "fundamentals.html#przygotowanie-danych",
    "title": "11  Fundamenty DNN",
    "section": "11.2 Przygotowanie danych",
    "text": "11.2 Przygotowanie danych\nWiele technik wstępnego przetwarzania danych i inżynierii cech jest specyficznych dla danej dziedziny (np. specyficznych dla danych tekstowych lub danych obrazowych); omówimy je w kolejnych rozdziałach, gdy napotkamy je w praktycznych przykładach. Na razie zajmiemy się podstawami, które są wspólne dla wszystkich domen danych. Skupimy się na najważniejszych.\n\n11.2.1 Wektoryzacja\nWszystkie wejścia i cele w sieci neuronowej muszą być tensorami danych zmiennoprzecinkowych (lub, w szczególnych przypadkach, tensorami liczb całkowitych). Jakiekolwiek dane, które chcesz przetworzyć - dźwięk, obraz, tekst - musisz najpierw zamienić na tensory, co nazywamy wektoryzacją danych. Na przykład, jeśli zajmujemy się automatyczna analizą tekstu, to konieczna jest transformacja wyrazów metodą one-hot encoding, która zamieni nam wyrazy na zestaw zmiennych zmiennoprzecinkowych. W przykładach klasyfikacji cyfr dane były już w postaci wektorowej, więc można było pominąć ten krok.\n\n\n11.2.2 Normalizacja\nW przykładzie klasyfikacji cyfr, zacząłeś od danych obrazu zakodowanych jako liczby całkowite z zakresu 0-255, kodujące wartości w skali szarości. Zanim wprowadziłeś te dane do sieci, musiałeś podzielić je przez 255, aby uzyskać wartości zmiennoprzecinkowe z zakresu 0-1. Podobnie, przy przewidywaniu cen domów, zacząłeś od cech, które miały różne zakresy - niektóre cechy miały małe wartości zmiennoprzecinkowe, inne miały dość duże wartości całkowite. Zanim wprowadziłeś te dane do swojej sieci, musiałeś znormalizować każdą cechę niezależnie, tak aby jej odchylenie standardowe wynosiło 1, a średnia 0.\n\n\n\n\n\nOgólnie rzecz biorąc, nie jest bezpiecznie podawać do sieci neuronowej danych, które przyjmują stosunkowo duże wartości (na przykład wielocyfrowe liczby całkowite, które są znacznie większe niż wartości początkowe przyjmowane przez wagi sieci) lub dane, które są niejednorodne (na przykład dane, w których jedna cecha jest w zakresie 0-1, a inna w zakresie 100-200). Takie postępowanie może wywołać duże aktualizacje gradientu, które uniemożliwią sieci osiągnięcie zbieżności. Aby ułatwić uczenie się sieci, twoje dane powinny mieć następujące cechy:\n\nPrzyjmuj małe wartości - większość wartości powinna być w zakresie 0-1.\nZachowaj jednorodność - to znaczy, że wszystkie cechy powinny przyjmować wartości w mniej więcej tym samym zakresie.\n\nZazwyczaj będziesz normalizował cechy zarówno w danych treningowych, jak i testowych. W tym przypadku będziesz chciał obliczyć średnią i odchylenie standardowe tylko na danych treningowych, a następnie zastosować je zarówno do danych treningowych, jak i testowych.\n#| eval: false\n\n1mean &lt;- apply(train_data, 2, mean)\nstd &lt;- apply(train_data, 2, sd)\n\n2train_data &lt;- scale(train_data, center = mean, scale = std)\ntest_data &lt;- scale(test_data, center = mean, scale = std)\n\n1\n\noblicz średnią i odchylenie standardowe na zbiorze uczącym;\n\n2\n\nzastosuj obliczone parametry do zbioru uczącego i testowego.\n\n\n\n\n11.2.3 Usuń braki danych\nCzasami w danych mogą pojawić się brakujące wartości. Genralnie, w sieciach neuronowych bezpiecznie jest wprowadzić brakujące wartości jako 0, pod warunkiem, że 0 nie jest już wartością znaczącą. Sieć nauczy się z ekspozycji na dane, że wartość 0 oznacza brakujące dane i zacznie ignorować tę wartość.\nZauważ, że jeśli spodziewasz się brakujących wartości w danych testowych, ale sieć była trenowana na danych bez żadnych brakujących wartości, sieć nie nauczy się ignorować brakujących wartości! W tej sytuacji powinieneś sztucznie wygenerować próbki treningowe z brakującymi wpisami: skopiuj kilka próbek treningowych kilka razy i opuść niektóre cechy, które spodziewasz się, że prawdopodobnie będą brakować w danych testowych."
  },
  {
    "objectID": "fundamentals.html#nadmierne-dopasowanie-i-niedopasowanie",
    "href": "fundamentals.html#nadmierne-dopasowanie-i-niedopasowanie",
    "title": "11  Fundamenty DNN",
    "section": "11.3 Nadmierne dopasowanie i niedopasowanie",
    "text": "11.3 Nadmierne dopasowanie i niedopasowanie\nZacznijmy od nauczenia sieci neuronowej do rozpoznawania cyfr na podstawie obrazów prezentowany wcześniej.\n\n\nKod\nlibrary(keras)\nmnist &lt;- dataset_mnist()\ntrain_images &lt;- mnist$train$x\ntrain_labels &lt;- mnist$train$y\ntest_images &lt;- mnist$test$x\ntest_labels &lt;- mnist$test$y\n\ntrain_images &lt;- array_reshape(train_images, c(60000, 28 * 28))\ntrain_images &lt;- train_images / 255\ntest_images &lt;- array_reshape(test_images, c(10000, 28 * 28))\ntest_images &lt;- test_images / 255\n\ntrain_labels &lt;- to_categorical(train_labels)\ntest_labels &lt;- to_categorical(test_labels)\n\nnetwork &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 512, activation = \"relu\", input_shape = c(28 * 28)) %&gt;%\n  layer_dense(units = 256, activation = \"relu\") |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 10, activation = \"softmax\")\n\nnetwork %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nnetwork\n\nhistory &lt;- network %&gt;% \n  fit(train_images, \n      train_labels, \n      epochs = 20, \n      batch_size = 128,\n      validation_split = 0.2)\n\nplot(history)\n\n\n\n\nModel: \"sequential_3\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_9 (Dense)                    (None, 512)                     401920      \n dense_8 (Dense)                    (None, 256)                     131328      \n dense_7 (Dense)                    (None, 64)                      16448       \n dense_6 (Dense)                    (None, 10)                      650         \n================================================================================\nTotal params: 550,346\nTrainable params: 550,346\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\n\nNa podstawie powyższego modelu możemy stwierdzić, że wydajność modelu na odłożonych danych walidacyjnych zawsze osiągała szczyt po kilku epokach, a następnie zaczynała się pogarszać. Overfitting zdarza się w każdym problemie uczenia maszynowego. Nauka radzenia sobie z nadmiernym dopasowaniem jest niezbędna do opanowania uczenia maszynowego.\nPodstawowym problemem w uczeniu maszynowym jest konflikt pomiędzy optymalizacją a generalizacją. Optymalizacja odnosi się do procesu dostosowywania modelu w celu uzyskania jak najlepszej wydajności na danych treningowych, podczas gdy generalizacja odnosi się do tego, jak dobrze wyszkolony model radzi sobie na danych, których nigdy wcześniej nie widział.\nNa początku treningu optymalizacja i generalizacja są zbieżne: im mniejsza strata na danych treningowych, tym mniejsza strata na danych testowych. Gdy tak się dzieje, mówi się, że twój model jest niedopasowany: wciąż jest postęp do zrobienia; sieć nie jest nauczona jeszcze wszystkich istotnych wzorców w danych treningowych. Jednak po pewnej liczbie iteracji na danych treningowych, generalizacja przestaje się poprawiać, a metryki dla zbioru walidacyjnego nie poprawiają się, a nawet zaczynają się pogarszać: model zaczyna być nadmiernie dopasowany. Oznacza to, że model zaczyna się uczyć wzorców, które są specyficzne dla danych treningowych, ale które są mylące lub nieistotne, gdy chodzi o nowe dane.\n\n\n\n\n\nAby zapobiec uczeniu się przez model błędnych lub nieistotnych wzorców występujących w danych treningowych, najlepszym rozwiązaniem jest uzyskanie większej ilości danych treningowych. Model wytrenowany na większej ilości danych będzie naturalnie lepiej generalizował. Jeśli nie jest to możliwe, innym rozwiązaniem jest modulowanie ilości informacji, które model może przechowywać, lub dodanie ograniczeń na informacje, które może przechowywać. Jeśli sieć może sobie pozwolić na zapamiętanie tylko niewielkiej liczby wzorców, proces optymalizacji zmusi ją do skupienia się na najbardziej widocznych wzorcach, które mają większą szansę na dobrą generalizację.\nProces walki z overfittingiem w ten sposób nazywany jest regularyzacją. Zapoznajmy się z kilkoma najpopularniejszymi technikami regularyzacji i zastosujmy je w praktyce, aby poprawić model klasyfikacji.\n\n11.3.1 Redukcja wielkości sieci\nNajprostszym sposobem zapobiegania overfittingowi jest zmniejszenie rozmiaru modelu: czyli liczby możliwych do nauczenia się parametrów w modelu (która jest określona przez liczbę warstw i liczbę neuronów na warstwę). W uczeniu głębokim liczba możliwych do nauczenia się parametrów w modelu jest często określana jako pojemność modelu. Intuicyjnie, model z większą liczbą parametrów ma większą zdolność zapamiętywania i dlatego może łatwo nauczyć się idealnego odwzorowania między próbkami treningowymi a ich celami - odwzorowania bez żadnej mocy generalizacji. Na przykład, model z 500 000 parametrów binarnych mógłby z łatwością nauczyć się każdej cyfry w zbiorze treningowym MNIST: potrzebowalibyśmy tylko 10 parametrów binarnych dla każdej z 50 000 cyfr. Ale taki model byłby bezużyteczny do klasyfikowania nowych próbek. Zawsze należy o tym pamiętać: modele głębokiego uczenia mają tendencję do dobrego dopasowania do danych treningowych, ale prawdziwym wyzwaniem jest generalizacja, a nie dopasowanie.\nZ drugiej strony, jeśli sieć ma ograniczone zasoby pamięci, nie będzie w stanie nauczyć się tego odwzorowania tak łatwo; dlatego, aby zminimalizować straty, będzie musiała uciec się do uczenia skompresowanych reprezentacji, które mają moc predykcyjną w odniesieniu do celów - dokładnie ten typ reprezentacji nas interesuje. Jednocześnie pamiętaj, że powinieneś używać modeli, które mają wystarczająco dużo parametrów, aby nie były niedostosowane: twój model nie powinien być ograniczany ze względu na zasoby pamięciowe. Trzeba znaleźć kompromis między zbyt dużą pojemnością a niewystarczającą.\nNiestety, nie ma magicznej formuły, aby określić właściwą liczbę warstw lub właściwy rozmiar dla każdej warstwy. Musisz ocenić szereg różnych architektur (oczywiście na zbiorze walidacyjnym, nie na zbiorze testowym), aby znaleźć właściwy rozmiar modelu dla twoich danych. Ogólny tok postępowania w celu znalezienia odpowiedniego rozmiaru modelu polega na rozpoczęciu od stosunkowo niewielkiej liczby warstw i parametrów, a następnie zwiększaniu rozmiaru warstw lub dodawaniu nowych warstw, dopóki nie zobaczysz malejących zwrotów w odniesieniu do utraty walidacji.\n\n\nKod\nnetwork &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 128, activation = \"relu\", input_shape = c(28 * 28)) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n\nnetwork %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory &lt;- network %&gt;% \n  fit(train_images, \n      train_labels, \n      epochs = 20, \n      batch_size = 128,\n      validation_split = 0.2)\n\n\n\n\nModel: \"sequential_4\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_11 (Dense)                   (None, 128)                     100480      \n dense_10 (Dense)                   (None, 10)                      1290        \n================================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\n\nJak widać z powyższej symulacji dla prostszej sieci zjawisko overffitingu pojawia się później i nie ma tak wyraźnego charakteru.\n\n\n11.3.2 Regularyzacja za pomocą kar\nPodobnie jak w modelach liniowych prostsze modele są mniej podatne na overfitting, również i sieci głębokiego uczenia również podlegają tej zasadzie - jak się mogliśmy przekonać na podstawie powyższego przykładu. Powyższy przykład ową prostotę modelu realizował poprzez prostą strukturę sieci (uboższy model) ale można go “upraszczać” również inaczej.\nProsty model to model, w którym rozkład wartości parametrów ma mniejszą entropię (lub model z mniejszą liczbą parametrów, jak widzieliśmy w poprzedniej sekcji). Dlatego powszechnym sposobem łagodzenia overfittingu jest nałożenie ograniczeń na złożoność sieci poprzez zmuszenie jej wag do przyjmowania tylko małych wartości, co czyni rozkład wartości wag bardziej regularnym. Nazywa się to regularyzacją wag i odbywa się poprzez dodanie do funkcji straty sieci kosztu związanego z posiadaniem dużych wag. Koszt ten występuje w dwóch postaciach:\n\nL1 - dodany koszt jest proporcjonalny do wartości bezwzględnej współczynników wagowych (norma L1 wag).\nL2 - koszt dodany jest proporcjonalny do kwadratu wartości współczynników wagowych (norma L2 wag).\n\nW keras, regularyzacja wagowa jest dodawana poprzez dodanie instancji regularyzatora wagowego do warstw. Należy pamiętać, że regularyzacja jest stosowana do sieci tylko na etapie uczenia, dlatego na zbiorze testowym strata będzie mniejsza niż na treningowym.\n\n\n\n\n\n\n\nKod\nnetwork &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 256, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l2(0.001),\n              input_shape = c(28 * 28)) |&gt; \n  layer_dense(units = 10, activation = \"softmax\")\n\nnetwork %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory &lt;- network %&gt;% \n  fit(train_images, \n      train_labels, \n      epochs = 20, \n      batch_size = 128,\n      validation_split = 0.2)\n\n\n\n\nModel: \"sequential_6\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_15 (Dense)                   (None, 256)                     200960      \n dense_14 (Dense)                   (None, 10)                      2570        \n================================================================================\nTotal params: 203,530\nTrainable params: 203,530\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\n\nJak widać z powyższego wykresu przez zastosowanie regularyzacji L2 otrzymaliśmy większą zbieżność na obu próbach.\n\n\n11.3.3 Dropout\nDropout to jedna z najskuteczniejszych i najczęściej stosowanych technik regularizacji dla sieci neuronowych, opracowana przez Geoffa Hintona i jego studentów z Uniwersytetu w Toronto. Dropout, zastosowany do warstwy, polega na losowym wyrzuceniu (ustawieniu na zero) pewnej liczby cech wyjściowych warstwy podczas treningu. Powiedzmy, że dana warstwa normalnie zwróciłaby podczas treningu wektor [0.2, 0.5, 1.3, 0.8, 1.1] dla danej próbki wejściowej. Po zastosowaniu dropoutu, wektor ten będzie miał kilka zerowych wpisów rozmieszczonych losowo: na przykład [0, 0.5, 1.3, 0, 1.1].\n\n\n\n\n\nWspółczynnik dropoutu to frakcja cech, które mają być wyzerowane; zwykle jest ustawiony między 0,2 a 0,5. W czasie ewaluacji modelu na zbiorze testowym żadne jednostki nie są usuwane; zamiast tego wartości wyjściowe warstwy są skalowane w dół o czynnik równy współczynnikowi usuwania, aby zrównoważyć fakt, że więcej jednostek jest aktywnych niż w czasie treningu. Czyli przykładowo jeśli warstwa miała współczynnik dropout 0,25, to w czasie ewaluacji na zbiorze testowym wartości wyjściowe tej warstwy są mnożone przez 0,75.\n\n\n\nRysunek 11.3: Zasada działania dropout\n\n\nTa technika może wydawać się dziwna i przypadkowa. Dlaczego miałoby to pomóc w redukcji overfitting? Hinton mówi, że zainspirował go między innymi mechanizm zapobiegania oszustwom stosowany przez banki. Mówiąc jego własnymi słowami: “Poszedłem do swojego banku. Kasjerzy ciągle się zmieniali i zapytałem jednego z nich, dlaczego. Powiedział, że nie wie, ale są często przenoszeni. Uznałem, że musi to być spowodowane tym, że skuteczne oszukanie banku wymagałoby współpracy między pracownikami. To uświadomiło mi, że losowe usuwanie różnych podzbiorów neuronów na każdym przykładzie zapobiegnie spiskom, a tym samym zmniejszy overfitting.” Główna idea polega na tym, że wprowadzenie szumu do wartości wyjściowych warstwy może rozbić przypadkowe wzorce, które nie są istotne (co Hinton określa jako spiski), które sieć zacznie zapamiętywać, jeśli nie będzie w niej szumu.\n\n\nKod\nnetwork &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 512, \n              activation = \"relu\",\n              input_shape = c(28 * 28)) |&gt; \n  layer_dropout(0.5) |&gt; \n  layer_dense(units = 10, activation = \"softmax\")\n\nnetwork %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory &lt;- network %&gt;% \n  fit(train_images, \n      train_labels, \n      epochs = 20, \n      batch_size = 128,\n      validation_split = 0.2)\n\n\n\n\nModel: \"sequential_7\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_17 (Dense)                   (None, 512)                     401920      \n dropout (Dropout)                  (None, 512)                     0           \n dense_16 (Dense)                   (None, 10)                      5130        \n================================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n________________________________________________________________________________"
  },
  {
    "objectID": "fundamentals.html#reguła-postępowania-w-czasie-budowy-modelu",
    "href": "fundamentals.html#reguła-postępowania-w-czasie-budowy-modelu",
    "title": "11  Fundamenty DNN",
    "section": "11.4 Reguła postępowania w czasie budowy modelu",
    "text": "11.4 Reguła postępowania w czasie budowy modelu\nW tym rozdziale przedstawimy uniwersalny schemat, który można wykorzystać do rozwiązania każdego problemu uczenia maszynowego. Dyskusja łączy koncepcje, które poznałeś w tym rozdziale: definicję problemu, ocenę, inżynierię cech i walkę z overfittingiem.\n\n11.4.1 Definiowanie problemu i przygotowanie zbioru danych\nPo pierwsze, musisz zdefiniować problem:\n\nJakie będą twoje dane wejściowe? Co próbujesz przewidzieć? Możesz nauczyć się przewidywać coś tylko wtedy, gdy masz dostępne dane treningowe: na przykład, możesz nauczyć się klasyfikować sentyment2 recenzji filmowych tylko wtedy, gdy masz dostępne zarówno recenzje filmów, jak i adnotacje sentymentu. W związku z tym, dostępność danych jest zwykle czynnikiem ograniczającym na tym etapie (chyba, że masz środki, aby zapłacić ludziom, aby zebrać dane dla Ciebie).\nZ jakim typem problemu masz do czynienia? Czy jest to klasyfikacja binarna? Klasyfikacja wieloklasowa? Regresja skalarna? Regresja wektorowa? Klasyfikacja wieloklasowa, wieloetykietowa? Coś innego, jak klastrowanie, generowanie lub uczenie wzmacniające3? Określenie typu problemu pozwoli Ci na wybór architektury modelu, funkcji straty i tak dalej.\n\n2 nastawienie3 ang. reinforcement learningNie możesz przejść do następnego etapu, dopóki nie wiesz, jakie są twoje wejścia i wyjścia oraz z jakich danych będziesz korzystać. Bądź świadomy hipotez, które stawiasz. Dopóki nie masz działającego modelu, są to jedynie hipotezy, czekające na potwierdzenie lub unieważnienie. Nie wszystkie problemy można rozwiązać; tylko dlatego, że zebrałeś przykłady wejść X i celów Y, nie oznacza to bowiem, że X zawiera wystarczająco dużo informacji, aby przewidzieć Y. Na przykład, jeśli próbujesz przewidzieć ruchy akcji na giełdzie, biorąc pod uwagę jej niedawną historię cenową, raczej ci się to nie uda, ponieważ historia cenowa nie zawiera zbyt wielu informacji prognostycznych.\nJedną z klas nierozwiązywalnych problemów, o których powinieneś wiedzieć, są problemy niestacjonarne. Załóżmy, że próbujesz zbudować silnik rekomendacji do sprzedaży ubrań, trenujesz go na jednym miesiącu danych (sierpień) i chcesz zacząć generować rekomendacje w zimie. Jednym z poważnych problemów jest to, że rodzaje ubrań, które ludzie kupują, zmieniają się z sezonu na sezon: kupowanie ubrań jest zjawiskiem niestacjonarnym w skali kilku miesięcy. To, co próbujesz modelować, zmienia się w czasie. W tym przypadku, właściwym posunięciem jest ciągłe doszkalanie modelu na danych z niedawnej przeszłości lub zbieranie danych w skali czasowej, w której problem jest stacjonarny. Dla problemu cyklicznego, takiego jak kupowanie ubrań, wystarczy kilka lat danych, aby uchwycić sezonową zmienność - ale pamiętaj, aby czas roku był wejściem do modelu!\nPamiętaj, że uczenie maszynowe może być użyte tylko do zapamiętania wzorców, które są obecne w danych treningowych. Możesz rozpoznać tylko to, co widziałeś wcześniej. Używając uczenia maszynowego wyszkolonego na danych z przeszłości do przewidywania przyszłości, przyjmujesz założenie, że przyszłość będzie zachowywać się jak przeszłość, co nie zawsze jest prawdą.\n\n\n11.4.2 Określenie miary do oceny jakości dopasowania modelu\nAby coś kontrolować, musisz być w stanie to obserwować. Aby osiągnąć sukces, musisz zdefiniować, co rozumiesz przez sukces: dokładność? Twoja miara sukcesu będzie kierowała wyborem funkcji straty: czyli tego, co twój model będzie optymalizował.\nDla problemów klasyfikacji zrównoważonej, gdzie każda klasa jest mniej więcej równie prawdopodobna, dokładność4 i obszar pod krzywą ROC są powszechnymi metrykami. W przypadku problemów związanych z niezbalansowaną klasyfikacją, można użyć precision-recall. Dla problemów rankingowych lub klasyfikacji wieloznakowej można użyć średniej średniej precyzji5 . Nierzadko trzeba też zdefiniować własną, niestandardową metrykę, za pomocą której mierzy się sukces . Aby uzyskać poczucie różnorodności metryk dopasowania uczenia maszynowego i jak odnoszą się one do różnych domen problemowych, warto przejrzeć konkursy na Kaggle (kaggle.com); pokazują one szeroki zakres problemów i metryk .4 accuracy5 mean average precision\n\n\n11.4.3 Określenie techniki oceny wydajności modelu\nKiedy już wiesz, do czego dążysz, musisz ustalić, jak będziesz mierzyć swoje bieżące postępy. Wcześniej omówiliśmy trzy popularne protokoły oceny:\n\nUtrzymywanie zbioru walidacyjnego typu hold-out - dobry sposób, gdy masz dużo danych;\nPrzeprowadzanie K-krotnej walidacji krzyżowej - właściwy wybór, gdy masz zbyt mało próbek, aby walidacja była wiarygodna.\nPrzeprowadzanie iterowanej walidacji K-krotnej z losowaniem - bardzo dokładna oceny modelu, gdy dostępnych jest niewiele danych.\n\nW większości przypadków pierwsza będzie działać wystarczająco dobrze.\n\n\n11.4.4 Przygotuj dane\nKiedy już wiesz, na czym trenujesz, co optymalizujesz i jak ocenić swoje rozwiązanie, jesteś prawie gotowy do rozpoczęcia treningu modeli. Najpierw jednak należy sformatować dane w taki sposób, aby można je było wprowadzić do modelu uczenia maszynowego - tutaj założymy głęboką sieć neuronową:\n\nJak widziałeś wcześniej, twoje dane powinny być sformatowane jako tensory.\nWartości przyjmowane przez te tensory powinny być zazwyczaj skalowane do małych wartości: na przykład w zakresie [-1, 1] lub [0, 1].\nJeśli różne cechy przyjmują wartości w różnych zakresach (dane heterogeniczne), to dane powinny być znormalizowane.\nMożesz dokonać inżynierii cech, szczególnie dla problemów z małą liczbą danych.\n\nGdy tensory danych wejściowych i danych docelowych są gotowe, możesz rozpocząć trenowanie modeli.\n\n\n11.4.5 Porównaj model z modelem bazowym\nTwoim celem na tym etapie jest osiągnięcie mocy statystycznej: to znaczy opracowanie małego modelu, który jest w stanie pokonać model bazowy. W przykładzie klasyfikacji cyfr MNIST, wszystko co osiąga dokładność większą niż 0,16 można powiedzieć, że ma moc statystyczną.6 zwykła heurystyka polegająca na wylosowaniu wynikowej cyfry ma prawdopodobieństwo powodzenia właśnie równe 0,1\nZauważ, że nie zawsze jest możliwe osiągnięcie takiej mocy statystycznej. Jeśli nie możesz pokonać przyjętej linii bazowej po wypróbowaniu wielu rozsądnych architektur, może się okazać, że odpowiedź na pytanie, które zadajesz, nie jest dostępna na podstawie danych wejściowych. Pamiętaj, że stawiasz dwie hipotezy:\n\nzmienną wynikową można przewidzieć, na postawie danych wejściowych;\ndane zawierają wystarczająco dużo informacji, aby poznać związek pomiędzy wejściami i wyjściami.\n\nMoże się okazać, że te hipotezy są fałszywe. Zakładając, że wszystko idzie dobrze, musisz dokonać trzech kluczowych wyborów, aby zbudować swój pierwszy działający model:\n\nfunkcja aktywacji ostatniej warstwy - ustanawia ona praktyczne ograniczenia na wyjściu sieci. Na przykład w sieci ze zmienną wynikową dwuwartościową (dwie kategorie) powinieneś ustawić aktywację sigmoid.\nfunkcja straty - powinna odpowiadać rodzajowi problemu, który próbujesz rozwiązać. Na przykład w przykładzie MNIST użyto categorical_crossentropy.\nkonfiguracja procedury optymalizacji - jakiego optymalizatora użyjesz? Jaki będzie jego współczynnik szybkości uczenia? W większości przypadków bezpiecznie jest użyć rmsprop lub adam i jego domyślnego współczynnika uczenia.\n\nJeżeli chodzi o wybór funkcji straty, zauważ, że nie zawsze jest możliwa bezpośrednia optymalizacja dla metryki, która mierzy dopasowanie w danym problemie. Czasami nie ma łatwego sposobu na przekształcenie metryki w funkcję straty; funkcje straty, w końcu, muszą być obliczalne biorąc pod uwagę tylko partię danych (idealnie, funkcja straty powinna być obliczalna dla zaledwie jednego punktu danych) i musi być różniczkowalna (w przeciwnym razie nie można użyć wstecznej propagacji do trenowania sieci). Na przykład, szeroko stosowana metryka klasyfikacyjna ROC AUC nie może być bezpośrednio optymalizowana. Dlatego w zadaniach klasyfikacyjnych w jej miejsce używa się entropii krzyżowej (ang. cross-entropy). Można mieć nadzieję, że im niższa będzie entropia krzyżowa, tym wyższy będzie ROC-AUC.\n\n\n11.4.6 Skalowanie w górę\nKiedy już uzyskasz model, który ma moc statystyczną, pojawia się pytanie, czy twój model jest wystarczająco skuteczny? Czy ma on wystarczająco dużo warstw i parametrów, aby prawidłowo modelować dany problem? Na przykład, sieć z pojedynczą warstwą ukrytą z dwoma neuronami miałaby moc statystyczną dla zbioru MNIST, ale nie byłaby wystarczająca do dobrego rozwiązania problemu. Pamiętaj jednak, że w uczeniu maszynowym stale występuje “walka” między optymalizacją a generalizacją; idealny model to model pomiędzy niedostatecznym dopasowaniem a nadmiernym dopasowaniem. Aby dowiedzieć się, gdzie leży ta granica, najpierw trzeba ją przekroczyć 😎.\nAby dowiedzieć się, jak duży model będzie potrzebny, musisz opracować model, który jest nadmiernie dopasowany. Można to łatwo osiągnąć, realizują następujące kroki:\n\nDodaj warstwy.\nSpraw, by warstwy były większe (więcej neuronów).\nDłużej trenuj sieć (więcej epok).\n\nZawsze monitoruj stratę na zbiorze treningowym i walidacyjnym, jak również wartości wszystkich metryk, na których Ci zależy. Kiedy widzisz, że wydajność modelu na danych walidacyjnych zaczyna się pogarszać, osiągnąłeś nadmierne dopasowanie. Następnym etapem jest rozpoczęcie regularyzacji i dostrajania modelu, aby zbliżyć się jak najbardziej do idealnego modelu, który nie jest ani niedopasowany, ani nadmiernie dopasowany.\n\n\n\n\n\n\n\n11.4.7 Regularyzacja modelu\nTen krok zajmie najwięcej czasu: będziesz wielokrotnie modyfikował swój model, trenował go, oceniał na danych walidacyjnych (w tym momencie nie na danych testowych), ponownie go modyfikował i powtarzał, aż model będzie tak dobry, jak to tylko możliwe. Oto kilka rzeczy, które powinieneś wypróbować:\n\ndodaj dropout;\nspróbuj różnych architektur - dodaj lub usuń warstwy;\ndodaj regularyzację L1 i/lub L2.\nwypróbuj różne hiperparametry (takie jak liczba jednostek na warstwę lub szybkość uczenia optymalizatora), aby znaleźć optymalną konfigurację;\nopcjonalnie, wykonaj inżynierię cech - dodaj nowe cechy lub usuń cechy, które nie wydają się być informacyjne.\n\nNależy pamiętać o tym, że za każdym razem, gdy używamy informacji zwrotnej z procesu walidacji do dostrojenia modelu, do modelu wyciekają informacje o procesie walidacji. Powtarzając to tylko kilka razy, nie jest to wielki problem; ale robiąc to systematycznie przez wiele iteracji, w końcu spowoduje to, że Twój model będzie nadmiernie dopasowany do walidacji (nawet jeśli żaden model nie jest trenowany bezpośrednio na żadnych danych walidacyjnych). To sprawia, że proces oceny jest mniej wiarygodny.\nPo opracowaniu wystarczająco dobrej konfiguracji modelu, możesz wytrenować swój ostateczny model na wszystkich dostępnych danych (treningowych i walidacyjnych) i ocenić go po raz ostatni na zbiorze testowym. Jeśli okaże się, że wydajność na zestawie testowym jest znacznie gorsza niż wydajność zmierzona na danych walidacyjnych, może to oznaczać, że albo twoja procedura walidacji nie była wiarygodna, albo wcześniej pojawiło się zjawisko nadmiernego dopasowania do danych walidacyjnych podczas dostrajania parametrów modelu. W tym przypadku możesz zmienić procedurę oceny modelu na bardziej wiarygodną (jak np. iterowana walidacja K-krotna)."
  },
  {
    "objectID": "convolution.html#działanie-sieci-splotowej",
    "href": "convolution.html#działanie-sieci-splotowej",
    "title": "12  Sieci splotowe",
    "section": "12.1 Działanie sieci splotowej",
    "text": "12.1 Działanie sieci splotowej\nPodstawową różnicą pomiędzy warstwą gęstych połączeń a siecią splotową jest to, że warstwy dense uczą się cech parametrów globalnych w swoich wejściowych przestrzeniach (w przypadku cyfr MNIST są to wzorce związane ze wszystkimi pikselami), a warstwy konwolucyjne uczą się lokalnych wzorców (patrz rys) - w przypadku obrazów wzorce są znajdowane w małych dwuwymiarowych oknach danych wejściowych. W zaprezentowanym przykładzie wszystkie te okna charakteryzowały się wymiarami 3x3.\n\n\n\nRysunek 12.1: Rozbicie obrazu na lokalne wzorce\n\n\nDzięki tej kluczowej charakterystyce sieci konwolucyjne mają dwie ciekawe własności:\n\nwzorce rozpoznawane przez sieć są niezależne od przesunięcia. Sieć konwolucyjna po rozpoznaniu określonego wzoru w prawym dolnym rogu obrazu może rozpoznać go np. w lewym górnym rogu obrazu. Sieć gęsta w celu rozpoznania wzorca znajdującego się w innym miejscu musi nauczyć się go na nowo. W związku z tym sieci konwolucyjne charakteryzują się dużą wydajnością podczas przetwarzania obrazów. Sieci splotowe mogą skutecznie tworzyć uogólnienia po przetworzeniu mniejszego zbioru testowego.\nsieci splotowe mogą uczyć się przestrzennej hierarchii wzorców (patrz Rysunek 12.5). Pierwsza warstwa uczy się rozpoznania położenia kluczowych obiektów przez zmianę konturów i kontrastu. Druga warstwa (pooling) redukuje najważniejsze informacje do prostszej postaci (zmniejszenie rozdzielczości). Kolejna warstwa stara się wyciągnąć kluczowe elementy (wzorce) występujące w obiekcie, jak linie proste, ukośne, okręgi, łuki, itp. Kolejne dwie warstwy ponownie redukują rozdzielczość wyciągając kluczowe elementy obrazu. Ostatecznie wartości wyjściowe z ostatniej warstwy konwolucyjnej przekazują kluczowe informacje do sieci gęstej, a ta ostatecznie zamienia je za pomocą funkcji softmax na przewidywane cyfry.\n\nSieci konwolucyjne działają na trójwymiarowych tensorach określanych mianem map cech, zawierających dwie przestrzenne osie definiujące wysokość i szerokość. Trzecią osią jest oś głębi, nazywana również osią kanałów. W przypadku obrazu RGB oś głębi ma trzy wymiary (po jednym dla każdego koloru). Obrazy monochromatyczne (takie jak MNIST), mają jeden wymiar głębi (kolor opisuje tylko skalę nasycenia szarości). Operacja konwolucji wyodrębnia fragmenty z wejściowej mapy cech i stosuje to samo przekształcenie do wszystkich tych fragmentów, dając wyjściową mapę cech. Ta wyjściowa mapa cech jest nadal tensorem 3D: ma szerokość i wysokość. Jej głębokość może być dowolna, ponieważ głębokość wyjściowa jest parametrem warstwy, a różne kanały w tej osi głębokości nie oznaczają już konkretnych kolorów, jak w przypadku wejścia RGB; oznaczają one raczej filtry. Filtry kodują specyficzne aspekty danych wejściowych: na wysokim poziomie pojedynczy filtr może kodować na przykład pojęcie “obecności twarzy na wejściu”.\n\n\n\nRysunek 12.2: Procedura filtrowania obrazu\n\n\nW przykładzie MNIST, pierwsza warstwa konwolucji pobiera mapę cech o rozmiarze (28, 28, 1) i wyprowadza mapę cech o rozmiarze (26, 26, 32): oblicza 32 filtry na danych wejściowych. Każdy z tych 32 filtrów wyjściowych zawiera siatkę wartości 26 × 26, która jest mapą odpowiedzi filtra, wskazującą odpowiedź tego filtra w różnych miejscach wejścia (patrz Rysunek 12.2). To właśnie oznacza termin mapa cech: każdy wymiar na osi głębokości jest cechą (lub filtrem), a tensor 2D output[:, :, n] jest przestrzenną mapą 2D odpowiedzi tego filtra na wejście.\nKonwolucje są definiowane przez dwa kluczowe parametry:\n\nRozmiar filtrów wyodrębnionych z wejść - są to zwykle 3 × 3 lub 5 × 5. W przykładzie były to 3 × 3, co jest częstym wyborem.\nGłębokość wyjściowej mapy cech - czyli liczba filtrów obliczonych przez konwolucję. Przykład rozpoczął się z głębokością 32, a zakończył z głębokością 64.\n\nW keras parametry te są pierwszymi argumentami przekazywanymi do warstwy: layer_conv_2d(output_depth, c(window_height, window_width)).\nKonwolucja działa poprzez przesuwanie tych okien o rozmiarze 3 × 3 lub 5 × 5 po wejściowej mapie cech 3D, zatrzymując się w każdym możliwym miejscu, i wyodrębniając trójwymiarową łatę otaczających cech (kształt (window_height, window_width, input_depth)). Każda taka paczka 3D jest następnie przekształcana (poprzez iloczyn tensorowy z tą samą uczoną macierzą wag, zwaną jądrem konwolucji) w 1D wektor kształtu (output_depth). Wszystkie te wektory są następnie przestrzennie składane w trójwymiarową wyjściową mapę kształtu (wysokość, szerokość, głębokość wyjściowa). Każde miejsce w wyjściowej mapie cech odpowiada temu samemu miejscu w wejściowej mapie cech (na przykład prawy dolny róg wyjścia zawiera informacje o prawym dolnym rogu wejścia). Na przykład, przy oknach 3 × 3, wektor output[i, j, ] pochodzi z wejściowej mapy 3D input[i-1:i+1, j-1:j+1, ]. Pełny proces został szczegółowo przedstawiony na Rysunek 12.3.\n\n\n\nRysunek 12.3: Zasada działania filtrów w sieci splotowej\n\n\nZauważ, że szerokość i wysokość wyjściowa może się różnić od szerokości i wysokości wejściowej. Mogą się one różnić z dwóch powodów:\n\nEfekty brzegowe, którym można przeciwdziałać poprzez padding wejściowej mapy funkcji\nUżycie pasków (ang. strides), które zdefiniujemy za chwilę.\n\n\n12.1.1 Efekty brzegowe - padding\nRozważmy mapę cech 5 × 5 (łącznie 25 kwadratów). Jest tylko 9 kwadratów, wokół których można wyśrodkować okno 3 × 3, tworząc siatkę 3 × 3. Dlatego też wyjściowa mapa cech zmniejsza się nieco: w tym przypadku dokładnie o dwa kwadraty wzdłuż każdego wymiaru. Ten efekt brzegowy można zobaczyć w działaniu we wcześniejszym przykładzie: zaczynasz z 28 × 28 na danych wejściowych, które po pierwszej warstwie konwolucji stają się 26 × 26.\nJeśli chcesz uzyskać wyjściową mapę cech o takich samych wymiarach przestrzennych jak wejściowa, możesz użyć paddingu. Padding polega na dodaniu odpowiedniej liczby wierszy i kolumn po każdej stronie wejściowej mapy cech, tak aby umożliwić dopasowanie środkowych okien konwolucji wokół każdego kafelka wejściowego. Dla okna 3 × 3 dodasz jedną kolumnę po prawej, jedną kolumnę po lewej, jeden rząd na górze i jeden rząd na dole. Dla okna 5 × 5 dodałbyś dwa rzędy (patrz Rysunek 12.3).\nW warstwach layer_conv_2d padding jest konfigurowalny poprzez argument padding, który przyjmuje dwie wartości: “valid”, co oznacza brak paddingu (zostaną użyte tylko poprawne lokalizacje okien); oraz “same”, co oznacza “rozszerz wejście w taki sposób, aby mieć wyjście o takiej samej szerokości i wysokości jak wejście”. Argument padding domyślnie przyjmuje wartość “valid”.\n\n\n\nRysunek 12.4: Przykłady paddingu i konwolucji kroczącej\n\n\n\n\n12.1.2 Efekty brzegowe - stirdes\nInnym czynnikiem, który może wpływać na wielkość wyjścia jest pojęcie kroku (ang. strides). Dotychczasowy opis konwolucji zakładał, że wszystkie środkowe kwadraty okien konwolucji są przylegające. Jednak odległość między dwoma kolejnymi oknami jest parametrem konwolucji, zwanym jej krokiem, który domyślnie wynosi 1. Możliwe jest istnienie konwolucji kroczących: konwolucji o kroku większym niż jeden. Na Rysunek 12.4 widać części wyekstrahowane przez konwolucję 3 x 3 z rozstępem 2 na wejściu 5 × 5 (bez wypełnienia). Użycie kroku 2 oznacza, że szerokość i wysokość mapy cech są pomniejszane o współczynnik 2 (oprócz zmian wywołanych przez efekty brzegowe). Konwersje z przesunięciem są rzadko używane w praktyce, choć mogą być przydatne w niektórych typach modeli; dobrze jest zapoznać się z tą koncepcją. Do downsamplingu map cech, zamiast kroków, używamy zwykle operacji max pooling, którą zastosowaliśmy w sieci do przykładu MNIST. Przyjrzyjmy się jej bardziej szczegółowo."
  },
  {
    "objectID": "convolution.html#max-pooling",
    "href": "convolution.html#max-pooling",
    "title": "12  Sieci splotowe",
    "section": "12.2 Max pooling",
    "text": "12.2 Max pooling\nW przykładzie MNIST mogłeś zauważyć, że rozmiar map cech jest zmniejszany o połowę po każdej operacji layer_max_pooling_2d. Na przykład przed pierwszą layer_max_pooling_2d mapa cech ma rozmiar 26 × 26, ale operacja max poolingu zmniejsza ją o połowę do 13 × 13. Taka jest właśnie rola max poolingu: agresywne zmniejszanie próbkowania map cech, podobnie jak w przypadku konwolucji krokowych.\nOperacja max pooling polega na wyodrębnieniu okien z wejściowych map cech i wyprowadzeniu maksymalnej wartości każdego filtra. Koncepcyjnie jest to podobne do konwolucji, z tą różnicą, że zamiast przekształcać lokalne plamy poprzez wyuczone przekształcenie liniowe (jądro konwolucji), są one przekształcane poprzez zakodowaną operację max tensora. Dużą różnicą w stosunku do konwolucji jest to, że max pooling jest zwykle wykonywany z oknami 2 × 2 i krokiem 2, w celu zmniejszenia próbkowania map cech o współczynnik 2. Z drugiej strony, konwolucja jest zwykle wykonywana z oknami 3 × 3 i bez kroku (stride 1).\nDlaczego obniżamy rozmiar mapy cech w ten sposób? Dlaczego nie usunąć warstw max pooling i zachować dość duże mapy funkcji przez całą sieć? Przyjrzyjmy się tej opcji. Konwolucyjna baza modelu wyglądałaby wtedy tak:\n\n\nKod\nmodel_no_max_pool &lt;- keras_model_sequential() %&gt;%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\",\n                input_shape = c(28, 28, 1)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\")\n\nmodel_no_max_pool\n\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_5 (Conv2D)                  (None, 26, 26, 32)              320         \n conv2d_4 (Conv2D)                  (None, 24, 24, 64)              18496       \n conv2d_3 (Conv2D)                  (None, 22, 22, 64)              36928       \n================================================================================\nTotal params: 55,744\nTrainable params: 55,744\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nCo jest nie tak z tą architekturą? Dwie rzeczy:\n\nNie sprzyja uczeniu się przestrzennej hierarchii cech. Okna 3 × 3 w trzecich warstwach będą zawierały jedynie informacje pochodzące z okien 7 × 7 wejścia początkowego. Wzorce wysokopoziomowe wyuczone przez sieć splotową będą nadal bardzo małe w stosunku do początkowego wejścia, co może nie wystarczyć do nauki klasyfikacji cyfr (spróbuj rozpoznać cyfrę, patrząc na nią tylko przez okna o wymiarach 7 × 7 pikseli!). Potrzebujemy, aby cechy z ostatniej warstwy konwolucji zawierały informacje o całości danych wejściowych.\nOstateczna mapa cech ma 22 * 22 * 64 = 30 976 wszystkich współczynników na próbkę. Jest to ogromna ilość. Jeśli miałbyś ją spłaszczyć, aby dołaczyć gęstą warstwę o rozmiarze 512, ta warstwa miałaby 15,8 miliona parametrów. Jest to zdecydowanie zbyt dużo dla tak małego modelu i spowodowałoby przeuczenie.\n\nW skrócie, powodem użycia redukcji wymiaru jest zmniejszenie liczby współczynników mapy cech do przetworzenia, jak również wywołanie hierarchii filtrów przestrzennych poprzez sprawienie, że kolejne warstwy konwolucji będą patrzyły na coraz większe okna (w sensie ułamka oryginalnego wejścia, które obejmują).\nZauważ, że max pooling nie jest jedynym sposobem, w jaki możesz osiągnąć taki redukcję wymiaru. Jak już wiesz, możesz również użyć kroków w poprzedniej warstwie konwolucji. I możesz użyć average pooling zamiast max pooling, gdzie każdy lokalny fragment wejściowy jest przekształcany przez użycie średniej wartości każdego filtra w tym fragmencie, a nie maksimum. Mimo to, max pooling jest preferowanym rozwiązaniem ponieważ często daje lepsze rezultaty. W skrócie, powodem jest to, że cechy mają tendencję do kodowania przestrzennej obecności jakiegoś wzoru lub koncepcji w różnych kaflach mapy cech (stąd termin mapa cech), a bardziej informatywne jest spojrzenie na maksymalną obecność różnych cech niż na ich średnią obecność. Tak więc najrozsądniejszą strategią redukcji wymiaru jest najpierw wytworzenie map cech, a następnie spojrzenie na maksymalną aktywację cech w małych fragmentach, a nie patrzenie na rzadsze okna wejść lub uśrednianie fragmentów wejściowych, co może spowodować przegapienie lub rozmycie informacji o obecności cech.\nPoniżej zaprezentowane są wyniki działania poszczególnych warstw sieci konwolucyjnej.\n#| label: fig-conv1\n#| fig-cap: Wyniki filtracji pierwszą warstwą splotową (wybrano filtry 4 i 10)\n#| layout-ncol: 2\n1img &lt;- train_images[44,,,] |&gt; as.raster()\nplot(img, interp=F)\n\n2img_tensor &lt;- train_images[44,,,]\ndim(img_tensor)\n\n3img_tensor &lt;- array_reshape(img_tensor, c(1, 28, 28, 1))\ndim(img_tensor)\n\n4layer_outputs &lt;- lapply(model$layers[1:5], function(layer) layer$output)\nlayer_outputs\n\n5activation_model &lt;- keras_model(inputs = model$input, outputs = layer_outputs)\nactivations &lt;- activation_model %&gt;% predict(img_tensor)\n\n6first_layer_activation &lt;- activations[[1]]\ndim(first_layer_activation)\n\n7plot_channel &lt;- function(channel) {\n  rotate &lt;- function(x) t(apply(x, 2, rev))\n  image(rotate(channel), axes = FALSE, asp = 1, \n        col = gray.colors(20))\n}\n\n8plot_channel(first_layer_activation[1,,,4])\nplot_channel(first_layer_activation[1,,,10])\n\n1\n\nwybierz obraz\n\n2\n\nzamień go na tensor\n\n3\n\ndostosuj rozmiar tensora do wejścia do sieci\n\n4\n\nwylistuj wszystkie wyjścia z sieci splotowych\n\n5\n\nstwórz model pomocniczy składający się z wejścia i warstw splotowych\n\n6\n\nwybierz warstwę do wizualizacji\n\n7\n\nnapisz funkcję do wyświetlania obrazów\n\n8\n\nrysuj obrazy\n\n\nTak wygląda wynik pierwszej warstwy splotowej dla wybranych dwóch filtrów (kanałów), a jakby to wyglądało gdyby wyświetlić wyniki wszystkich warstw i kanałów.\n#| eval: false\n1dir.create(\"nine_activations\")\n2image_size &lt;- 58\n3images_per_row &lt;- 16\n4for (i in 1:5) {\n  \n5  layer_activation &lt;- activations[[i]]\n6  layer_name &lt;- model$layers[[i]]$name $\n\n7  n_features &lt;- dim(layer_activation)[[4]]\n8  n_cols &lt;- n_features %/% images_per_row\n \n9  png(paste0(\"nine_activations/\", i, \"_\", layer_name, \".png\"),\n      width = image_size * images_per_row, \n      height = image_size * n_cols)\n10  op &lt;- par(mfrow = c(n_cols, images_per_row), mai = rep_len(0.02, 4))\n  \n11  for (col in 0:(n_cols-1)) {\n12    for (row in 0:(images_per_row-1)) {\n13      channel_image &lt;- layer_activation[1,,,(col*images_per_row) + row + 1]\n14      plot_channel(channel_image)\n    }\n  }\n  \n  par(op)\n15  dev.off()\n}\n\n1\n\nstwórz katalog na obrazy\n\n2\n\nwybierz wielkość obrazu w px\n\n3\n\nwybierz ile obrazów ma się mieścić w wierszu\n\n4\n\nrozpocznij pętlę po wszystkich nr warstw splowych\n\n5\n\nprzypisz i-tą warstwę\n\n6\n\nzapisz nazwę warstwy\n\n7\n\nwyciągnij liczbę filtrów\n\n8\n\noblicz liczbę obrazów na wiersz\n\n9\n\nstwórz plik png o wymiarach zgodnych z liczbą obrazów w wierszu i liczbą wierszy\n\n10\n\nokreśl parametry obrazu (zmiana layout i marginesów)\n\n11\n\nrozpocznij pętlę po kolumnach\n\n12\n\nrozpocznij pętlę po wierszach\n\n13\n\nwybierz filtr do obrazowania\n\n14\n\nnarysuj obraz filtra\n\n15\n\nzapisz plik\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRysunek 12.5: Wyniki wszystkich warstw splotowych i wszystkich filtrów"
  },
  {
    "objectID": "example.html#pobieranie-danych",
    "href": "example.html#pobieranie-danych",
    "title": "13  Przykład uczenia sieci splotowej",
    "section": "13.1 Pobieranie danych",
    "text": "13.1 Pobieranie danych\nZestaw danych Dogs vs. Cats, z którego będziesz korzystać, nie jest dołączony do keras. Został udostępniony przez Kaggle w ramach konkursu wizji komputerowej pod koniec 2013 roku, jeszcze w czasach, gdy sieci splotowe nie były głównym nurtem. Oryginalny zbiór danych można pobrać ze strony https://www.kaggle.com/competitions/dogs-vs-cats/data1.Zdjęcia to kolorowe JPEG-i o średniej rozdzielczości. Rysunek 5.8 pokazuje kilka przykładów.1 konieczne będzie założenie konta w Kaggle, jeśli jeszcze go nie masz - bez obaw, proces jest bezbolesny\nNie jest zaskoczeniem, że konkurs Kaggle cat-versus-dogs w 2013 roku został wygrany przez uczestników, którzy użyli sieci konwolucyjnych. Najlepsze struktury osiągnęły do 95% dokładności. W tym przykładzie uzyskasz wynik bliski tej wartości, mimo że będziesz trenował swoje modele na mniej niż 10% danych, które były dostępne dla konkurencji.\n\n\n\nRysunek 13.1: Kilka przykładowych obrazów ze zbioru\n\n\nTen zbiór danych zawiera 25000 obrazów psów i kotów (12500 z każdej klasy) i ma rozmiar 569 MB (skompresowany)2. Po pobraniu i rozpakowaniu, utworzysz nowy zbiór danych zawierający trzy podzbiory: zbiór treningowy z 1000 próbek każdej klasy, zbiór walidacyjny z 500 próbkami każdej klasy oraz zbiór testowy z 500 próbkami każdej klasy. Poniżej znajduje się kod do wykonania tego zadania.2 w ściągniętym pliku dogs-vs-cats.zip są zawarte trzy pliki, a skupiamy się na train.zip i to ten rozpakowujemy\n#| eval: false\n1original_dataset_dir &lt;- \"/Users/majerek/Downloads/dogs-vs-cats/train\"\n2base_dir &lt;- \"/Users/majerek/Downloads/cats_and_dogs_small\"\ndir.create(base_dir)\ntrain_dir &lt;- file.path(base_dir, \"train\")\ndir.create(train_dir)\nvalidation_dir &lt;- file.path(base_dir, \"validation\")\ndir.create(validation_dir)\ntest_dir &lt;- file.path(base_dir, \"test\")\ndir.create(test_dir)\ntrain_cats_dir &lt;- file.path(train_dir, \"cats\")\ndir.create(train_cats_dir)\ntrain_dogs_dir &lt;- file.path(train_dir, \"dogs\")\ndir.create(train_dogs_dir)\nvalidation_cats_dir &lt;- file.path(validation_dir, \"cats\")\ndir.create(validation_cats_dir)\nvalidation_dogs_dir &lt;- file.path(validation_dir, \"dogs\")\ndir.create(validation_dogs_dir)\ntest_cats_dir &lt;- file.path(test_dir, \"cats\")\ndir.create(test_cats_dir)\ntest_dogs_dir &lt;- file.path(test_dir, \"dogs\")\ndir.create(test_dogs_dir)\nfnames &lt;- paste0(\"cat.\", 1:1000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(train_cats_dir))\nfnames &lt;- paste0(\"cat.\", 1001:1500, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(validation_cats_dir))\nfnames &lt;- paste0(\"cat.\", 1501:2000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(test_cats_dir))\nfnames &lt;- paste0(\"dog.\", 1:1000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(train_dogs_dir))\nfnames &lt;- paste0(\"dog.\", 1001:1500, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(validation_dogs_dir))\nfnames &lt;- paste0(\"dog.\", 1501:2000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(test_dogs_dir))\n\n1\n\ntu ustaw swoją ścieżkę do katalogu z rozpakowanym katalogiem train\n\n2\n\nustaw ścieżkę gdzie chcesz zapisywać obrazy do uczenia sieci\n\n\nJako sprawdzenie poprawności, policzmy, ile zdjęć jest w każdym podziale treningowym (train/validation/test):\n\n\nKod\ncat(\"total training cat images:\", length(list.files(train_cats_dir)), \"\\n\")\n\n\ntotal training cat images: 1000 \n\n\nKod\ncat(\"total training dog images:\", length(list.files(train_dogs_dir)), \"\\n\")\n\n\ntotal training dog images: 1000 \n\n\nKod\ncat(\"total validation cat images:\", length(list.files(validation_cats_dir)), \"\\n\")\n\n\ntotal validation cat images: 500 \n\n\nKod\ncat(\"total validation dog images:\", length(list.files(validation_dogs_dir)), \"\\n\")\n\n\ntotal validation dog images: 500 \n\n\nKod\ncat(\"total test cat images:\", length(list.files(test_cats_dir)), \"\\n\")\n\n\ntotal test cat images: 500 \n\n\nKod\ncat(\"total test dog images:\", length(list.files(test_dogs_dir)), \"\\n\")\n\n\ntotal test dog images: 500 \n\n\nTak więc masz rzeczywiście 2000 obrazów treningowych, 1000 obrazów walidacyjnych i 1000 obrazów testowych. Każdy podział zawiera taką samą liczbę próbek z każdej klasy: jest to zrównoważony problem klasyfikacji binarnej, co oznacza, że dokładność klasyfikacji będzie odpowiednią miarą dopasowania."
  },
  {
    "objectID": "example.html#budowa-sieci",
    "href": "example.html#budowa-sieci",
    "title": "13  Przykład uczenia sieci splotowej",
    "section": "13.2 Budowa sieci",
    "text": "13.2 Budowa sieci\nW poprzednim przykładzie zbudowaliśmy małą sieć splotową dla MNIST. Ponownie użyjemy tej samej ogólnej struktury: sieć będzie stosem naprzemiennych warstw layer_conv_2d (z aktywacją relu) i layer_max_pooling_2d.\nAle ponieważ mamy do czynienia z większymi obrazami i bardziej złożonym problemem, sprawimy, że nasza sieć będzie odpowiednio większa: będzie miała jeszcze jedną kombinację layer_conv_2d + layer_max_pooling_2d. Służy to zarówno zwiększeniu pojemności sieci, jak i dalszemu zmniejszeniu rozmiaru map funkcji, aby nie były zbyt duże, gdy dojdziemy do layer_flatten. Tutaj, ponieważ zaczynamy od wejść o rozmiarze 150 × 150 (nieco arbitralny wybór), kończymy z mapami cech o rozmiarze 7 × 7 tuż przed layer_flatten.\n\n\n\n\n\n\nWażne\n\n\n\nGłębokość map cech stopniowo zwiększa się w sieci (od 32 do 128), natomiast rozmiar map cech maleje (od 148 × 148 do 7 × 7). Jest to wzór, który zobaczysz w prawie wszystkich sieciach splotowych.\n\n\nPonieważ zajmujemy się problemem klasyfikacji binarnej, zakończymy sieć pojedynczą warstwą (layer_dense o rozmiarze 1) i sigmoidalną aktywacją. Ta warstwa będzie kodować prawdopodobieństwo wystąpienia jednej lub drugiej klasy.\n\n\nKod\nlibrary(keras)\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\",\n                input_shape = c(150, 150, 3)) %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(units = 512, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel\n\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_3 (Conv2D)                  (None, 148, 148, 32)            896         \n max_pooling2d_3 (MaxPooling2D)     (None, 74, 74, 32)              0           \n conv2d_2 (Conv2D)                  (None, 72, 72, 64)              18496       \n max_pooling2d_2 (MaxPooling2D)     (None, 36, 36, 64)              0           \n conv2d_1 (Conv2D)                  (None, 34, 34, 128)             73856       \n max_pooling2d_1 (MaxPooling2D)     (None, 17, 17, 128)             0           \n conv2d (Conv2D)                    (None, 15, 15, 128)             147584      \n max_pooling2d (MaxPooling2D)       (None, 7, 7, 128)               0           \n flatten (Flatten)                  (None, 6272)                    0           \n dense_1 (Dense)                    (None, 512)                     3211776     \n dense (Dense)                      (None, 1)                       513         \n================================================================================\nTotal params: 3,453,121\nTrainable params: 3,453,121\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nW kroku kompilacji, użyjemy optymalizatora RMSprop. Ponieważ sieć kończy się pojedynczą jednostką sigmoidalną, użyjemy binarnej entropii krzyżowej jako funkcji straty.\n\n\nKod\nmodel %&gt;% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(learning_rate = 1e-4),\n  metrics = c(\"acc\")\n)"
  },
  {
    "objectID": "example.html#przygotowanie-danych",
    "href": "example.html#przygotowanie-danych",
    "title": "13  Przykład uczenia sieci splotowej",
    "section": "13.3 Przygotowanie danych",
    "text": "13.3 Przygotowanie danych\nJak już wiemy, dane przed wprowadzeniem do sieci powinny być sformatowane w odpowiednio wstępnie przetworzone tensory zmiennoprzecinkowe. Obecnie dane są zapisane na dysku w postaci plików JPEG, więc kroki w celu wprowadzenia ich do sieci wyglądają mniej więcej tak:\n\nOdczytaj pliki z obrazkami.\nZdekoduj zawartość JPEG na siatki pikseli RGB.\nPrzekształć je na tensory zmiennoprzecinkowe.\nPrzeskaluj wartości pikseli (między 0 a 255) do przedziału [0, 1] (jak wiadomo, sieci neuronowe wolą mieć do czynienia z małymi wartościami wejściowymi).\n\nTo może wydawać się nieco zniechęcające, ale na szczęście keras ma narzędzia, które zajmują się tymi krokami automatycznie. keras zawiera wiele narzędzi pomocniczych do przetwarzania obrazów. W szczególności, zawiera funkcję image_data_generator(), która może automatycznie przekształcić pliki graficzne na dysku w partie wstępnie przetworzonych tensorów. To właśnie z niej będziemy tutaj korzystać.\n\n\n\n\n\n\n\nKod\ntrain_datagen &lt;- image_data_generator(rescale = 1/255)\nvalidation_datagen &lt;- image_data_generator(rescale = 1/255)\n\ntrain_generator &lt;- flow_images_from_directory(\n  train_dir,\n  train_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nvalidation_generator &lt;- flow_images_from_directory(\n  validation_dir,\n  validation_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\n\nPrzyjrzyjmy się wyjściu jednego z takich generatorów: daje on partie obrazów RGB o wymiarach 150 × 150 (kształt (20, 150, 150, 3)) oraz binarne etykiety (kształt (20)). W każdej partii znajduje się 20 próbek (rozmiar partii). Zauważ, że generator tworzy te partie w nieskończoność: zapętla się bez końca nad obrazami w folderze docelowym.\n\n\nKod\nbatch &lt;- generator_next(train_generator)\nstr(batch)\n\n\nList of 2\n $ : num [1:20, 1:150, 1:150, 1:3] 0.1529 0.4471 0.0314 0.7804 0.6706 ...\n $ : num [1:20(1d)] 1 0 1 0 1 1 1 0 0 0 ...\n\n\nDopasujmy model do danych uzyskanych za pomocą generatora. Robimy to za pomocą funkcji fit. Jako pierwszy argument oczekuje ona generatora, który będzie generował partie danych wejściowych i docelowych. Ponieważ dane są generowane w nieskończoność, generator musi wiedzieć, ile próbek pobrać z generatora, zanim zadeklaruje koniec epoki. Taką rolę pełni argument steps_per_epoch: po pobraniu z generatora partii próbek - czyli po wykonaniu kroków spadku gradientu - proces dopasowania przejdzie do następnej epoki. W tym przypadku, partie są 20-próbkowe, więc zajmie to 100 partii, aż do osiągnięcia celu 2000 próbek.\nKiedy używasz fit, możesz przekazać też argument validation_data. Ważne jest, aby zauważyć, że ten argument może być zbiorem danych, ale może to być również lista tablic. Jeśli przekażesz generator jako validation_data, to oczekuje się, że ten generator będzie dawał partie danych walidacyjnych w nieskończoność; dlatego powinieneś również określić argument validation_steps, który mówi procesowi ile partii ma pobrać z generatora walidacji do oceny3.3 ponieważ liczebnośc tej próby wynosi 1000 obserwacji, to z prostego rachunku # validation_data / batch_size = 1000/20 = 50\n\n\nKod\nhistory &lt;- model %&gt;% fit(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 30,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\n# można też zapisać model\nsave_model_hdf5(model, filepath = \"models/mod_conv.h5\")\n\n\n\n\nKod\nplot(history)\n\n\n\n\n\nTe wykresy są charakterystyczne dla nadmiernego dopasowania. Dokładność szkolenia rośnie liniowo w czasie, aż osiąga prawie 100%, natomiast dokładność walidacji zatrzymuje się na poziomie 70-74%. Strata związana z walidacją osiąga swoje minimum już po pięciu epokach, a następnie zatrzymuje się, podczas gdy strata związana z treningiem zmniejsza się liniowo, aż osiągnie prawie 0.\nPonieważ mamy stosunkowo mało próbek treningowych (2000), overfitting będzie naszym najczęstszym problemem. Wiemy już o kilku technikach, które mogą pomóc złagodzić overfitting, takich jak dropout i regularyzacje L1 i L2. Teraz wprowadzimy nową, specyficzną dla wizji komputerowej i używaną niemal powszechnie podczas przetwarzania obrazów za pomocą modeli głębokiego uczenia: augmentację danych."
  },
  {
    "objectID": "example.html#augmentacja-obrazów",
    "href": "example.html#augmentacja-obrazów",
    "title": "13  Przykład uczenia sieci splotowej",
    "section": "13.4 Augmentacja obrazów",
    "text": "13.4 Augmentacja obrazów\nNadmierne dopasowanie jest spowodowany przez posiadanie zbyt małej ilości próbek do nauki, co powoduje, że nie można wytrenować modelu, który może uogólniać się na nowe dane. Przy nieskończonej ilości danych, twój model byłby wystawiony na każdy możliwy rozkład danych: overfitting nigdy nie byłby możliwy. Augmentacja danych polega na generowaniu większej ilości danych treningowych z istniejących próbek treningowych, poprzez szereg losowych przekształceń, które dają wiarygodnie wyglądające obrazy. Celem jest, aby w czasie treningu model nigdy nie widział dokładnie tego samego obrazu dwa razy. To pomaga wystawić model na więcej aspektów danych i lepiej generalizować.\n\n\n\n\n\nW keras można to zrobić poprzez skonfigurowanie szeregu losowych przekształceń, które mają być wykonywane na obrazach odczytywanych przez image_data_generator.\n\n\nKod\ndatagen &lt;- image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE,\n  fill_mode = \"nearest\"\n)\n\n\nTo tylko kilka z dostępnych opcji. Prześledźmy szybko ten kod:\n\nrotation_range to wartość podana w stopniach (0-180), czyli zakres, w którym można losowo obracać zdjęcia.\nwidth_shift i height_shift to zakresy (jako ułamek całkowitej szerokości lub wysokości), w których można losowo przesuwać obrazy w pionie lub poziomie.\nshear_range służy do losowego stosowania transformacji ścinających.\nzoom_range służy do losowego powiększania obrazów.\nhorizontal_flip służy do losowego obracania obrazów w poziomie4 - ma to znaczenie, gdy nie ma założeń asymetrii poziomej (np. obrazy z prawdziwego świata).\nfill_mode to strategia używana do wypełniania nowo utworzonych pikseli, które mogą pojawić się po obrocie lub zmianie szerokości/wysokości.\n\n4 obrócona zostanie połowa obrazówfnames &lt;- list.files(train_cats_dir, full.names = TRUE)\n1img_path &lt;- fnames[[3]]\n2img &lt;- image_load(img_path, target_size = c(150, 150))\n3img_array &lt;- image_to_array(img)\n4img_array &lt;- array_reshape(img_array, c(1, 150, 150, 3))\n\n5augmentation_generator &lt;- flow_images_from_data(\n  img_array,\n  generator = datagen,\n  batch_size = 1\n)\n\n6op &lt;- par(mfrow = c(2, 2), pty = \"s\", mar = c(1, 0, 1, 0))\nfor (i in 1:4) { \n  batch &lt;- generator_next(augmentation_generator)\n  plot(as.raster(batch[1,,,]))\n}\npar(op)\n\n1\n\nwybierz obraz do augmentacji\n\n2\n\nwczytaj obraz i zmień rozmiar na 150 x 150\n\n3\n\nzamień obraz do formatu (150, 150, 3)\n\n4\n\nprzekształć na tensor (1, 150, 150, 3)\n\n5\n\ngeneruj partie losowo przekształconych obrazów\n\n6\n\nwyświetl obrazy\n\n\nJeśli wytrenujesz nową sieć używając tej konfiguracji augmentacji danych, sieć nigdy nie zobaczy tego samego wejścia dwa razy. Dane wejściowe, które widzi, są nadal silnie powiązane, ponieważ pochodzą z niewielkiej liczby oryginalnych obrazów - nie możesz wytworzyć nowych informacji, możesz jedynie zmiksować istniejące. Jako takie, może to nie wystarczyć, aby całkowicie pozbyć się nadmiernego dopasowania. Aby dalej walczyć z nadmiernym dopasowanie, dodajemy do modelu warstwę dropout, tuż przed klasyfikatorem gęsto połączonym.\n\n\nKod\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\",\n                input_shape = c(150, 150, 3)) %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_flatten() %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = 512, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %&gt;% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(learning_rate = 1e-4),\n  metrics = c(\"acc\")\n)\n\ndatagen &lt;- image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE\n)\n\ntest_datagen &lt;- image_data_generator(rescale = 1/255)\n\ntrain_generator &lt;- flow_images_from_directory(\n  train_dir,\n  datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nvalidation_generator &lt;- flow_images_from_directory(\n  validation_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nhistory &lt;- model %&gt;% fit(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 100,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\n\n\n\n\n\n\nZanim przejdziemy do oceny dopasowania modelu wspomnijmy najważniejsze parametry w ustawieniu uczenia:\n\nbatch_size - określa liczbę próbek w każdej partii. Jego maksimum to liczba wszystkich próbek, co sprawia, że spadek gradientu jest dokładny, strata zmniejszy się w kierunku minimum, jeśli współczynnik uczenia jest wystarczająco mały, ale iteracje są wolniejsze. Z drugiej strony jego minimum to 1, co powoduje stochastyczny spadek gradientu - szybki, ale kierunek kroku gradientu jest oparty tylko na jednym przykładzie, dlatego może błądzić. batch_size pozwala na kompromis między dwoma skrajnościami: dokładnym kierunkiem gradientu i szybką iteracją. Maksymalna wartość dla batch_size może być ograniczona, jeśli twój model + zestaw danych nie mieści się w dostępnej (GPU) pamięci.\nsteps_per_epoch - liczba iteracji partii, zanim epoka treningowa zostanie zakończona. Jeśli masz zestaw treningowy o stałym rozmiarze, możesz go zignorować, ale może być przydatny, jeśli masz ogromny zestaw danych lub jeśli generujesz losowe augmentacje danych w locie, tj. jeśli twój zestaw treningowy ma (wygenerowany) nieskończony rozmiar.\nvalidation_steps - podobne do steps_per_epoch ale na zestawie danych walidacyjnych.\n\nWracając do podsumowania dopasowania sieci, widzimy, że augmentacja oraz dodanie jednej warstwy regularyzacyjnej znacznie poprawiło wynik dopasowania (83%). Stosując więcej warstw regularyzacyjnych można osiągnąć jeszcze lepszy wynik 86-87%."
  },
  {
    "objectID": "example.html#wykorzystanie-sieci-wstępnie-wytrenowanej",
    "href": "example.html#wykorzystanie-sieci-wstępnie-wytrenowanej",
    "title": "13  Przykład uczenia sieci splotowej",
    "section": "13.5 Wykorzystanie sieci wstępnie wytrenowanej",
    "text": "13.5 Wykorzystanie sieci wstępnie wytrenowanej\nPowszechnym i wysoce efektywnym podejściem do głębokiego uczenia na małych zbiorach danych obrazów jest użycie wstępnie wytrenowanej sieci. Sieć wstępnie wytrenowana to sieć zapisana, która została wcześniej wytrenowana na dużym zbiorze danych, zazwyczaj na zadaniu klasyfikacji obrazów na dużą skalę. Jeśli ten oryginalny zbiór danych jest wystarczająco duży i ogólny, to hierarchia cech przestrzennych wyuczona przez wstępnie wytrenowaną sieć może efektywnie działać jako ogólny model świata wizualnego, a zatem jej cechy mogą okazać się użyteczne dla wielu różnych problemów związanych z widzeniem komputerowym, nawet jeśli te nowe problemy mogą dotyczyć zupełnie innych klas niż te z oryginalnego zadania. Na przykład, można wytrenować sieć na ImageNet (gdzie klasy to głównie zwierzęta i przedmioty codziennego użytku), a następnie wykorzystać ją do czegoś tak odległego jak identyfikacja mebli na obrazach. Taka możliwość przenoszenia wyuczonych cech na różne problemy jest kluczową zaletą głębokiego uczenia w porównaniu do wielu starszych podejść płytkiego uczenia i sprawia, że głębokie uczenie jest bardzo efektywne w przypadku problemów z małymi danymi. W tym przypadku, rozważmy dużą sieć splotową wytrenowaną na zbiorze danych ImageNet (1,4 miliona oznaczonych obrazów i 1000 różnych klas). ImageNet zawiera wiele klas zwierząt, w tym różne gatunki kotów i psów, można więc oczekiwać dobrych wyników w problemie klasyfikacji koty kontra psy. Będziemy używać architektury VGG16, opracowanej przez Simonyan i Zisserman (2014); jest to prosta i szeroko stosowana architektura sieci konwolucyjnej dla ImageNet. Chociaż jest to starszy model, daleki od obecnego stanu wiedzy i nieco cięższy niż wiele innych najnowszych modeli, wybraliśmy go, ponieważ jego architektura jest podobna do tego, co już znamy i jest łatwa do zrozumienia bez wprowadzania nowych koncepcji. To może być pierwsze spotkanie z jedną z tych uroczych nazw modeli-VGG, ResNet, Inception, Inception-ResNet, Xception i tak dalej.\nIstnieją dwa sposoby wykorzystania wstępnie wytrenowanej sieci: ekstrakcja cech i dostrajanie. Omówimy oba z nich. Zacznijmy od ekstrakcji cech.\n\n13.5.1 Ekstrakcja cech\nEkstrakcja cech polega na wykorzystaniu reprezentacji wyuczonych przez poprzednią sieć do wyodrębnienia interesujących cech z nowych próbek. Cechy te są następnie przepuszczane przez nowy klasyfikator, który jest trenowany od podstaw.\nJak widzieliśmy wcześniej, sieci konwolucyjne używane do klasyfikacji obrazów składają się z dwóch części: zaczynają się od serii warstw pooling i konwolucji, a kończą na gęsto połączonym klasyfikatorze. Pierwsza część nazywana jest bazą konwolucyjną modelu. W przypadku sieci konwolucyjnych ekstrakcja cech polega na wzięciu bazy konwolucyjnej wcześniej wytrenowanej sieci, przepuszczeniu przez nią nowych danych i wytrenowaniu na jej wyjściu nowego klasyfikatora (patrz Rysunek 13.2).\n\n\n\nRysunek 13.2: Sposób zastosowania wyuczonej sieci\n\n\nDlaczego warto ponownie wykorzystać tylko bazę konwolucyjną? Czy mógłbyś ponownie użyć również gęsto połączonego klasyfikatora? Ogólnie rzecz biorąc, należy tego unikać. Powodem jest to, że reprezentacje nauczone przez bazę konwolucyjną będą prawdopodobnie bardziej ogólne, a zatem bardziej użyteczne: mapy cech sieci konwolucyjnej są mapami obecności ogólnych wzorców na obrazie, co prawdopodobnie będzie przydatne niezależnie od problemu widzenia komputerowego. Natomiast reprezentacje wyuczone przez klasyfikator będą z konieczności specyficzne dla zbioru klas, na których model został wytrenowany - będą zawierać jedynie informacje o prawdopodobieństwie obecności tej czy innej klasy na całym obrazie. Dodatkowo, reprezentacje znajdujące się w gęsto połączonych warstwach nie zawierają już żadnej informacji o tym, gdzie w obrazie wejściowym znajdują się obiekty: warstwy te pozbywają się pojęcia przestrzeni, lokalizacja obiektów jest opisywana tylko przez konwencjonalne mapy cech. Dla problemów, w których lokalizacja obiektów ma znaczenie, gęsto połączone cechy są w dużej mierze bezużyteczne.\nZauważmy, że poziom ogólności (a więc i możliwości ponownego wykorzystania) reprezentacji wyodrębnionych przez konkretne warstwy konwolucyjne zależy od głębokości warstwy w modelu. Warstwy znajdujące się wcześniej w modelu ekstrahują lokalne, bardzo ogólne mapy cech (takie jak krawędzie, kolory i tekstury), podczas gdy warstwy znajdujące się wyżej ekstrahują bardziej abstrakcyjne pojęcia (takie jak “kocie ucho” lub “psie oko”). Jeśli więc nowy zestaw danych różni się znacznie od zestawu danych, na którym był trenowany oryginalny model, lepiej będzie użyć tylko kilku pierwszych warstw modelu do ekstrakcji cech, niż używać całej bazy konwolucyjnej.\nW tym przypadku, ponieważ zbiór klas ImageNet zawiera wiele klas psów i kotów, prawdopodobnie korzystne byłoby ponowne wykorzystanie informacji zawartych w gęsto połączonych warstwach oryginalnego modelu. Nie zdecydujemy się jednak na to, aby uwzględnić bardziej ogólny przypadek, gdy zbiór klas nowego problemu nie pokrywa się ze zbiorem klas oryginalnego modelu. Przedstawmy to w praktyce, wykorzystując bazę konwolucyjną sieci VGG16, wytrenowaną na ImageNet, do wyodrębnienia interesujących cech z obrazów kotów i psów, a następnie wytrenowania klasyfikatora koty vs psy na tych cechach.\nModel VGG16, między innymi, jest dostarczany w pakiecie z keras. Oto lista modeli klasyfikacji obrazów (wszystkie wytrenowane na zbiorze danych ImageNet), które są dostępne jako część keras:\n\nXception\nInceptionV3\nResNet50\nVGG16\nVGG19\nMobileNet.\n\n\n\nKod\nconv_base &lt;- application_vgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(150, 150, 3)\n)\n\n\nDo funkcji przekazywane są trzy argumenty:\n\nweights - określa punkt startowy wag, z którego należy zainicjalizować model.\ninclude_top - odnosi się do włączenia (lub nie) gęsto połączonego klasyfikatora na szczycie sieci. Domyślnie, ten gęsto połączony klasyfikator odpowiada 1000 klas z ImageNet. Ponieważ zamierzasz użyć własnego gęsto połączonego klasyfikatora (z tylko dwoma klasami: kot i pies), nie musisz go włączać.\ninput_shape - to kształt tensorów obrazów, które podasz sieci. Ten argument jest czysto opcjonalny: jeśli go nie podasz, sieć będzie w stanie przetwarzać dane wejściowe o dowolnym rozmiarze.\n\n1conv_base\n\n2plot(conv_base)\n\n1\n\nsieć można przestawić jako podsumowanie tekstowe\n\n2\n\nale również jako wykres\n\n\nOstateczna mapa cech ma kształt (4, 4, 512). To jest ta warstwa, do której będziemy dołączać sieć gęstą. W tym momencie możemy postąpić na dwa sposoby:\n\nUruchomienie bazy konwolucyjnej na zbiorze danych, następnie zapisanie jej wyjść do pliku na dysku, a następnie użycie tych danych jako danych wejściowych do własnego, gęsto połączonego klasyfikatora. To rozwiązanie jest szybkie i tanie5 w wykonaniu, ponieważ wymaga uruchomienia bazy konwolucyjnej tylko raz dla każdego obrazu wejściowego, a baza konwolucyjna jest zdecydowanie najdroższą częścią potoku. Ale z tego samego powodu ta technika nie pozwoli ci użyć augmentacji danych.\nRozszerzenie modelu, który mamy (conv_base), dodając gęste warstwy na górze i uruchamiając całą sieć na danych wejściowych. To pozwoli ci użyć augmentacji danych, ponieważ każdy obraz wejściowy przechodzi przez bazę konwolucyjną za każdym razem, gdy jest widziany przez model. Ale z tego samego powodu ta technika jest znacznie droższa od pierwszej. Zajmiemy się obiema technikami.\n\n5 obliczeniowo\n13.5.1.1 Ekstrakcja cech bez augmentacji\nZacznijmy od uruchomienia instancji wcześniej wprowadzonego image_data_generator, aby wyodrębnić obrazy jako tablice, jak również ich etykiety. Wyodrębnimy cechy z tych obrazów poprzez wywołanie metody predict na modelu.\n#| eval: false\ndatagen &lt;- image_data_generator(rescale = 1/255)\n\nbatch_size &lt;- 20\n\nextract_features &lt;- function(directory, sample_count) {\n  features &lt;- array(0, dim = c(sample_count, 4, 4, 512))\n  labels &lt;- array(0, dim = c(sample_count))\n  generator &lt;- flow_images_from_directory(\n    directory = directory,\n    generator = datagen,\n    target_size = c(150, 150),\n    batch_size = batch_size,\n    class_mode = \"binary\"\n  )\n  i &lt;- 0\n  while(TRUE) {\n    batch &lt;- generator_next(generator)\n    inputs_batch &lt;- batch[[1]]\n    labels_batch &lt;- batch[[2]]\n    features_batch &lt;- conv_base %&gt;% predict(inputs_batch)\n    index_range &lt;- ((i * batch_size)+1):((i + 1) * batch_size)\n    features[index_range,,,] &lt;- features_batch\n    labels[index_range] &lt;- labels_batch\n    i &lt;- i + 1\n    if (i * batch_size &gt;= sample_count)\n1    break }\n  list(\n    features = features,\n    labels = labels\n) }\n\ntrain &lt;- extract_features(train_dir, 2000)\n\nvalidation &lt;- extract_features(validation_dir, 1000)\n\ntest &lt;- extract_features(test_dir, 1000)\n\n1\n\nponieważ generator obrazów tworzy je w nieskończoność, to musimy go zatrzymać po tym jak każdy obraz zostanie obejrzany raz\n\n\nWyodrębnione cechy mają obecnie kształt (samples, 4, 4, 512). Będziemy je przekazywali do gęsto połączonego klasyfikatora, więc najpierw musisz je spłaszczyć do postaci (samples, 8192):\n\n\nKod\nreshape_features &lt;- function(features) {\n  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))\n}\ntrain$features &lt;- reshape_features(train$features)\nvalidation$features &lt;- reshape_features(validation$features)\ntest$features &lt;- reshape_features(test$features)\n\n\nTeraz możemy przejść do uczenia sieci gęstej na wstępnie przetworzonych danych przez sieć konwolucyjną.\n\n\nKod\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 256, activation = \"relu\",\n              input_shape = 4 * 4 * 512) %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %&gt;% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 2e-5),\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory &lt;- model %&gt;% fit(\n  train$features, train$labels,\n  epochs = 30,\n  batch_size = 20,\n  validation_data = list(validation$features, validation$labels)\n)\n\n\n\n\nKod\nmodel &lt;- load_model_tf(\"models/exm_conv3/\")\nload(\"models/exm_hist3.rda\")\nplot(history)\n\n\n\n\n\nOsiągnięty poziom dopasowania na zbiorze walidacyjnym bliski 90% jest imponujący. Jednak wykres uczenia pokazuje, że wystąpiło zjawisko nadmiernego dopasowania. Jest to spowodowane brakiem augmentacji danych w tym podejściu.\n\n\n13.5.1.2 Ekstrakcja cech z augmentacją\nPonieważ modele zachowują się tak samo jak warstwy, możemy dodać model (taki jak conv_base) do modelu sekwencyjnego tak samo jak dodalibyśmy warstwę.\n\n\nKod\nmodel &lt;- keras_model_sequential() %&gt;%\n  conv_base %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(units = 256, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel\n\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n vgg16 (Functional)                 (None, 4, 4, 512)               14714688    \n flatten_1 (Flatten)                (None, 8192)                    0           \n dense_3 (Dense)                    (None, 256)                     2097408     \n dense_2 (Dense)                    (None, 1)                       257         \n================================================================================\nTotal params: 16,812,353\nTrainable params: 16,812,353\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nJak widać, baza konwolucyjna VGG16 ma 14714688 parametrów. Klasyfikator, który dodajesz na końcu, ma 2 miliony parametrów. Zanim skompilujemy i wytrenujemy model, bardzo ważne jest zamrożenie bazy konwolucyjnej. Zamrożenie warstwy lub zestawu warstw oznacza uniemożliwienie aktualizacji ich wag podczas treningu. Jeśli tego nie zrobisz, to reprezentacje, które zostały wcześniej wyuczone przez bazę konwolucyjną, zostaną zmodyfikowane podczas treningu. Ponieważ gęste warstwy na górze są losowo inicjalizowane, bardzo duże aktualizacje wag byłyby propagowane przez sieć, skutecznie niszcząc reprezentacje wcześniej nauczone. W keras zamrażamy sieć za pomocą funkcji freeze_weights().\n\n\nKod\ncat(\"Liczba tensorów poddawanych uczeniu przez zamrożeniem:\", length(model$trainable_weights), \"\\n\")\n\n\nLiczba tensorów poddawanych uczeniu przez zamrożeniem: 30 \n\n\nKod\nfreeze_weights(conv_base)\n\ncat(\"Liczba tensorów poddawanych ucznieu po zamrożeniu wag:\", length(model$trainable_weights), \"\\n\")\n\n\nLiczba tensorów poddawanych ucznieu po zamrożeniu wag: 4 \n\n\nPrzy takiej konfiguracji trenowane będą tylko wagi z dwóch gęstych warstw, które dodaliśmy. W sumie są to cztery tensory wag: dwa na warstwę (główna macierz wag i wektor bias). Zauważmy, że aby te zmiany zaczęły obowiązywać, musimy najpierw skompilować model. Jeśli kiedykolwiek zmodyfikujemy możliwość trenowania wag po kompilacji, powinniśmy ponownie skompilować model, w przeciwnym razie zmiany te zostaną zignorowane.\n\n\nKod\ntrain_datagen = image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE,\n  fill_mode = \"nearest\"\n)\n\ntest_datagen &lt;- image_data_generator(rescale = 1/255)\n\ntrain_generator &lt;- flow_images_from_directory(\n  train_dir,\n  train_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nvalidation_generator &lt;- flow_images_from_directory(\n  validation_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nmodel %&gt;% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(learning_rate = 2e-05),\n  metrics = c(\"accuracy\")\n)\n\nhistory &lt;- model %&gt;% fit(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 30,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\n\n\n\nKod\nmodel &lt;- load_model_tf(\"models/exm_conv4/\")\nload(\"models/exm_hist4.rda\")\nplot(history)\n\n\n\n\n\nJak widać, osiągnęliśmy dokładność walidacji na poziomie ponad 90%. Jest to znacznie lepszy wynik niż ten, który osiągnęliśmy z małą siecią konwolucyjną wytrenowaną od zera.\n\n\n\n13.5.2 Dostrajanie sieci\nInną szeroko stosowaną techniką ponownego wykorzystania modelu, uzupełniającą ekstrakcję cech, jest dostrajanie. Dostrajanie polega na odmrożeniu kilku górnych warstw zamrożonej bazy modelu użytej do ekstrakcji cech i wspólnym treningu zarówno nowo dodanej części modelu (w tym przypadku w pełni połączonego klasyfikatora), jak i tych górnych warstw. Nazywa się to dostrajaniem, ponieważ nieznacznie dostosowuje bardziej abstrakcyjne reprezentacje modelu, który jest ponownie wykorzystywany, aby uczynić je bardziej odpowiednimi dla danego problemu.\nStwierdziliśmy wcześniej, że konieczne jest zamrożenie bazy konwolucyjnej VGG16, aby móc trenować losowo zainicjowany klasyfikator. Z tego samego powodu możliwe jest dostrojenie górnych warstw bazy konwolucyjnej tylko wtedy, gdy klasyfikator na górze został już wytrenowany. Jeśli klasyfikator nie był już wytrenowany, to sygnał błędu propagujący się przez sieć podczas treningu byłby zbyt duży, a reprezentacje wyuczone wcześniej przez dostrajane warstwy zostałyby zniszczone. Zatem kroki dostrajania sieci są następujące:\n\nDodaj swoją własną sieć na wierzchu już wytrenowanej sieci bazowej.\nZamroź sieć bazową.\nWytrenuj dodaną część.\nOdmroź niektóre warstwy w sieci bazowej.\nWspólnie wytrenuj obie te warstwy i część, którą dodałeś.\n\nWykonaliśmy już pierwsze trzy kroki podczas wykonywania ekstrakcji cech. Przejdźmy do kroku 4: odmrozimy sieć conv_base, a następnie zamrozimy poszczególne warstwy wewnątrz niej. Dostroimy trzy ostatnie warstwy konwolucyjne, co oznacza, że wszystkie warstwy aż do block4_pool powinny być zamrożone, a warstwy block5_conv1, block5_conv2 i block5_conv3 powinny być trenowane. Dlaczego nie dostroić więcej warstw? Dlaczego nie dostroić całej bazy konwolucyjnej? Można. Ale musisz wziąć pod uwagę następujące kwestie:\n\nWcześniejsze warstwy w bazie konwolucyjnej kodują bardziej ogólne, możliwe do ponownego wykorzystania cechy, podczas gdy warstwy wyżej kodują bardziej wyspecjalizowane cechy. Bardziej przydatne jest dostrojenie bardziej wyspecjalizowanych cech, ponieważ są to te, które muszą być ponownie wykorzystane w naszym nowym problemie.\nIm więcej parametrów trenujesz, tym bardziej ryzykujesz nadmierne dopasowanie. Baza konwencjonalna ma ~15 milionów parametrów, więc ryzykowna byłaby próba wytrenowania jej na twoim małym zbiorze danych.\n\nDlatego w tej sytuacji dobrą strategią jest dostrojenie tylko dwóch lub trzech górnych warstw w bazie konwolucyjnej. Ustawmy to, zaczynając od miejsca, w którym zakończyliśmy pracę w poprzednim przykładzie.\n\n\nKod\nunfreeze_weights(conv_base, from = \"block5_conv1\")\n\n\nTeraz możemy zacząć dostrajać sieć. Zrobimy to za pomocą optymalizatora RMSProp, używając bardzo niskiego współczynnika uczenia. Powodem użycia niskiego współczynnika uczenia jest to, że chcemy ograniczyć wielkość modyfikacji, które wprowadzasz do reprezentacji trzech warstw, które dostrajasz. Zbyt duże aktualizacje mogą zaszkodzić tym reprezentacjom. Dokonamy jeszcze jednej zmiany. Ponieważ odmrożenie ostatnich warstw bazy konwolucyjnej może spowodować przeuczenie sieci, to dodamy warstwę dropout.\n\n\nKod\nmodel &lt;- keras_model_sequential() %&gt;%\n  conv_base %&gt;%\n  layer_flatten() %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = 256, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %&gt;% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(learning_rate = 1e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory &lt;- model %&gt;% fit(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 100,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\n\n\n\n\n\n\nDopasowanie tej sieci okazało się najlepsze6 pomimo, jak widać z rysunku, nadmiernego dopasowania. Być może dołożenie jeszcze jednej warstwy regularyzacji zaradziłoby temu problemowi. Możemy też sprawdzić jak ta sieć radzi sobie z zupełnie nowymi danymi, zbioru testowego.6 blisko 95% na zbiorze walidacyjnym\n\n\nKod\ntest_generator &lt;- flow_images_from_directory(\n  test_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nmodel %&gt;% evaluate(test_generator, steps = 50)\n\n\n    loss accuracy \n0.394595 0.940000 \n\n\nJak widać również na zbiorze testowym sieć jest dobrze dopasowana.\n\n\n\n\nSimonyan, Karen, i Andrew Zisserman. 2014. „Very Deep Convolutional Networks for Large-Scale Image Recognition”. https://doi.org/10.48550/ARXIV.1409.1556."
  },
  {
    "objectID": "segmentation.html#nowoczesne-architektury-sieci-splotowych",
    "href": "segmentation.html#nowoczesne-architektury-sieci-splotowych",
    "title": "14  Segmentacja obrazów",
    "section": "14.1 Nowoczesne architektury sieci splotowych",
    "text": "14.1 Nowoczesne architektury sieci splotowych\nArchitektura modelu jest sumą wyborów, które zostały dokonane przy jego tworzeniu: jakich warstw użyć, jak je skonfigurować i w jakim układzie je połączyć. Te wybory definiują przestrzeń hipotez twojego modelu: przestrzeń możliwych funkcji, które mogą być przeszukiwane przez spadek gradientu, parametryzowane przez wagi modelu. Podobnie jak w przypadku inżynierii cech, dobra przestrzeń hipotez koduje wcześniejszą wiedzę, którą posiadasz na temat danego problemu i jego rozwiązania. Na przykład, użycie warstw konwolucji oznacza, że z góry wiemy, że istotne wzory obecne na obrazach wejściowych są niezmienne względem translacji. Aby skutecznie uczyć się z danych, musisz przyjąć założenia dotyczące tego, czego szukasz.\nArchitektura modelu jest często balansem pomiędzy sukcesem a porażką. Jeśli dokonasz niewłaściwego wyboru architektury, Twój model może utknąć z nieoptymalnymi metrykami i żadna ilość danych treningowych go nie uratuje. I odwrotnie, dobra architektura modelu przyspieszy uczenie i umożliwi efektywne wykorzystanie dostępnych danych treningowych, zmniejszając zapotrzebowanie na duże zbiory danych. Dobra architektura modelu to taka, która zmniejsza rozmiar przestrzeni wyszukiwania lub w inny sposób ułatwia konwergencję do dobrego punktu przestrzeni wyszukiwania. Podobnie jak w przypadku inżynierii cech, architektura modelu polega na uproszczeniu problemu do rozwiązania przez spadek gradientu.\n\n\n\n\n\nArchitektura modelu jest bardziej sztuką niż nauką. Doświadczeni inżynierowie uczenia maszynowego są w stanie intuicyjnie poskładać wydajne modele przy pierwszej próbie, podczas gdy początkujący często mają problemy ze stworzeniem modelu, który w ogóle się uczy. Kluczowym słowem jest tu intuicja: nikt nie jest w stanie podać jasnego wyjaśnienia, co działa, a co nie. Eksperci polegają na kojarzeniu wzorców, umiejętności, którą nabywają poprzez bogate doświadczenie praktyczne.\nW dalszej części omówimy kilka podstawowych praktyk architektury sieci konwolucyjnych: w szczególności połączenia resztkowe (ang. residual connection), normalizację partii (ang. batch normalization) i konwolucje separowalne (ang. separable convolutions).\nWiększość sieci splotowych często charakteryzuje się strukturami przypominającymi piramidy (hierarchie cech). Przypomnij sobie na przykład progresję w liczbie filtrów konwolucyjnych, których użyliśmy w pierwszej sieci splotowej: 32, 64, 128. Liczba filtrów rośnie wraz z głębokością warstw, podczas gdy rozmiar map cech odpowiednio się kurczy. Ten sam wzór zauważysz w blokach modelu VGG16 (patrz Rysunek 14.6).\n\n\n\nRysunek 14.6: Architektura sieci VGG16\n\n\nGłębsze hierarchie są z natury dobre, ponieważ zachęcają do ponownego użycia funkcji, a zatem abstrakcji. Ogólnie rzecz biorąc, głęboki stos wąskich warstw działa lepiej niż płytki stos dużych warstw. Istnieje jednak granica, jak głęboko można układać warstwy, ze względu na problem znikających gradientów. To prowadzi nas do naszego pierwszego istotnego wzorca architektury modelu: połączeń szczątkowych.\n\n14.1.1 Połączenia resztowe\nWsteczna propagacja używania do uczenia modeli głębokich przypomina grę w “głuchy telefon”, gdzie kolejne osoby szepczą sobie do ucha przekazjuąc pewną - wymyśloną przez pierwszego gracza - informację. Po kilku przejściach informacja ta jest znacznie zniekształcona. Podobnie jest w uczeniu sieci, gdzie propagacją błędów wstecz powoduje wprowadzenie pewnych błędów w kolejnych warstwach sieci. Każda kolejna funkcja w łańcuchu wprowadza pewną ilość szumu. Jeśli łańcuch funkcji jest zbyt głęboki, szum ten zaczyna przytłaczać informacje o gradiencie i wsteczna propagacja przestaje działać. Twój model nie będzie w ogóle trenowany. Jest to problem znikających gradientów.\nRozwiązanie jest proste: wystarczy wymusić, aby każda funkcja w łańcuchu była nieniszcząca - zachowywała bezszumową wersję informacji zawartej w poprzednim wejściu. Najłatwiejszym sposobem wdrożenia tego jest użycie połączenia rezydualnego.\n\n\n\n\n\nJest to bardzo proste: wystarczy dodać wejście warstwy lub bloku warstw z powrotem do jej wyjścia (patrz Rysunek 14.7). Połączenie rezydualne działa jak skrót informacyjny wokół destrukcyjnych lub zaszumiających bloków (takich jak bloki zawierające aktywacje relu lub warstwy dropout), przekazując informację o gradiencie błędu z wczesnych warstw, tak aby propagować przez głęboką sieć. Technika ta została wprowadzona w 2015 roku wraz z rodziną modeli ResNet (opracowanych przez He i in. (2015)).\n\n\n\nRysunek 14.7: Połączenie resztowe\n\n\nZauważ, że dodanie wejścia z powrotem do wyjścia bloku sugeruje, że wyjście powinno mieć taki sam kształt jak wejście. Jednak tak nie jest, jeśli twój blok zawiera warstwy konwolucyjne ze zwiększoną liczbą filtrów lub warstwę max-pooling. W takich przypadkach użyj warstwy 1x1 layer_conv_2d() bez aktywacji, aby liniowo rzutować resztę na pożądany kształt wyjścia. Zazwyczaj używamy paddingu = \"same\" w warstwach konwolucji w bloku docelowym, aby uniknąć przestrzennego downsamplingu spowodowanego paddingiem, a w projekcji resztkowej używamy strides.\n\n\nKod\n# klasyczne podejście\ninputs &lt;- layer_input(shape = c(32, 32, 3))\nx &lt;- inputs |&gt; layer_conv_2d(32, 3, activation = \"relu\")\nresidual &lt;- x\nx &lt;- x |&gt; layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\")\nresidual &lt;- residual |&gt; layer_conv_2d(64, 1)\nx &lt;- layer_add(c(x, residual))\n\n\n\n\nKod\n# podejście wykorzystujące stride\ninputs &lt;- layer_input(shape = c(32, 32, 3))\nx &lt;- inputs |&gt; layer_conv_2d(32, 3, activation = \"relu\")\nresidual &lt;- x\nx &lt;- x |&gt;\n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") |&gt;\n  layer_max_pooling_2d(2, padding = \"same\")\nresidual &lt;- residual |&gt;\n  layer_conv_2d(64, 1, strides = 2)\nx &lt;- layer_add(list(x, residual))\n\n\nAby uczynić te pomysły bardziej konkretnymi, oto przykład prostego sieci splotowej zorganizowanej w serię bloków, z których każdy składa się z dwóch warstw konwolucji i jednej opcjonalnej warstwy max-pooling, z połączeniem szczątkowym wokół każdego bloku:\n\n\nKod\ninputs &lt;- layer_input(shape = c(32, 32, 3))\nx &lt;- layer_rescaling(inputs, scale = 1/255)\n\nresidual_block &lt;- function(x, filters, \n                           pooling = FALSE) {\n  residual &lt;- x\n  x &lt;- x |&gt;\n    layer_conv_2d(filters, 3, activation = \"relu\", padding = \"same\") |&gt;\n    layer_conv_2d(filters, 3, activation = \"relu\", padding = \"same\")\n\n  if (pooling) {\n    x &lt;- x |&gt; layer_max_pooling_2d(pool_size = 2, padding = \"same\")\n    residual &lt;- residual |&gt; layer_conv_2d(filters, 1, strides = 2)\n  } else if (filters != dim(residual)[4]) {\n    residual &lt;- residual |&gt; layer_conv_2d(filters, 1)\n  }\n\n  layer_add(list(x, residual))\n}\n\noutputs &lt;- x |&gt;\n  residual_block(filters = 32, pooling = TRUE) |&gt;\n  residual_block(filters = 64, pooling = TRUE) |&gt;\n  residual_block(filters = 128, pooling = FALSE) |&gt;\n  layer_global_average_pooling_2d() |&gt;\n  layer_dense(units = 1, activation = \"sigmoid\")\nmodel &lt;- keras_model(inputs = inputs, outputs = outputs)\nmodel\n\n\nModel: \"model_1\"\n________________________________________________________________________________\n Layer (type)             Output Shape      Param #  Connected to               \n================================================================================\n input_4 (InputLayer)     [(None, 32, 32,   0        []                         \n                          3)]                                                   \n rescaling_1 (Rescaling)  (None, 32, 32, 3  0        ['input_4[0][0]']          \n                          )                                                     \n conv2d_14 (Conv2D)       (None, 32, 32, 3  896      ['rescaling_1[0][0]']      \n                          2)                                                    \n conv2d_13 (Conv2D)       (None, 32, 32, 3  9248     ['conv2d_14[0][0]']        \n                          2)                                                    \n max_pooling2d_1 (MaxPool  (None, 16, 16, 3  0       ['conv2d_13[0][0]']        \n ing2D)                   2)                                                    \n conv2d_15 (Conv2D)       (None, 16, 16, 3  128      ['rescaling_1[0][0]']      \n                          2)                                                    \n add_2 (Add)              (None, 16, 16, 3  0        ['max_pooling2d_1[0][0]',  \n                          2)                          'conv2d_15[0][0]']        \n conv2d_17 (Conv2D)       (None, 16, 16, 6  18496    ['add_2[0][0]']            \n                          4)                                                    \n conv2d_16 (Conv2D)       (None, 16, 16, 6  36928    ['conv2d_17[0][0]']        \n                          4)                                                    \n max_pooling2d_2 (MaxPool  (None, 8, 8, 64)  0       ['conv2d_16[0][0]']        \n ing2D)                                                                         \n conv2d_18 (Conv2D)       (None, 8, 8, 64)  2112     ['add_2[0][0]']            \n add_3 (Add)              (None, 8, 8, 64)  0        ['max_pooling2d_2[0][0]',  \n                                                      'conv2d_18[0][0]']        \n conv2d_20 (Conv2D)       (None, 8, 8, 128  73856    ['add_3[0][0]']            \n                          )                                                     \n conv2d_19 (Conv2D)       (None, 8, 8, 128  147584   ['conv2d_20[0][0]']        \n                          )                                                     \n conv2d_21 (Conv2D)       (None, 8, 8, 128  8320     ['add_3[0][0]']            \n                          )                                                     \n add_4 (Add)              (None, 8, 8, 128  0        ['conv2d_19[0][0]',        \n                          )                           'conv2d_21[0][0]']        \n global_average_pooling2d  (None, 128)      0        ['add_4[0][0]']            \n  (GlobalAveragePooling2D                                                       \n )                                                                              \n dense (Dense)            (None, 1)         129      ['global_average_pooling2d[\n                                                     0][0]']                    \n================================================================================\nTotal params: 297,697\nTrainable params: 297,697\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nDzięki połączeniom rezydualnym można budować sieci o dowolnej głębokości, bez konieczności martwienia się o znikające gradienty.\n\n\n14.1.2 Normalizacja partii\nNormalizacja jest szeroką kategorią metod, które mają na celu uczynienie różnych próbek widzianych przez model uczenia maszynowego bardziej podobnymi do siebie, co pomaga modelowi w uczeniu się i generalizacji na nowych danych. Najbardziej powszechną formą normalizacji danych jest ta, którą już widziałeś kilka razy w tej książce: wyśrodkowanie danych na zero poprzez odjęcie średniej od danych i nadanie danym jednostkowego odchylenia standardowego poprzez podzielenie danych przez ich odchylenie standardowe. W efekcie przyjmuje się założenie, że dane mają rozkład normalny (lub gaussowski) i upewnia się, że rozkład ten jest wyśrodkowany i przeskalowany do jednostkowej wariancji:\n\n\nKod\nnormalize_data &lt;- apply(data, &lt;axis&gt;, function(x) (x - mean(x)) / sd(x))\n\n\nPoprzednie przykłady w tej książce normalizowały dane przed wprowadzeniem ich do modeli. Ale normalizacja danych może być interesująca po każdym przekształceniu dokonanym przez sieć: nawet jeśli dane wchodzące do sieci Dense lub Conv2D mają średnią 0 i jednostkową wariancję, nie ma powodu, by a priori oczekiwać, że tak będzie w przypadku danych wychodzących. Czy normalizacja aktywacji pośrednich mogłaby pomóc?\n\n\n\n\n\nNormalizacja partii danych właśnie to robi. Jest to rodzaj warstwy (layer_batch_normalization() w keras) wprowadzonej w 2015 roku przez Ioffe i Szegedy (2015); może ona adaptacyjnie normalizować dane, nawet gdy średnia i wariancja zmieniają się w czasie treningu. Podczas szkolenia używa średniej i wariancji bieżącej partii danych do normalizacji próbek, a podczas wnioskowania (gdy wystarczająco duża partia reprezentatywnych danych może nie być dostępna), używa wykładniczej średniej ruchomej i wariancji danych widzianych podczas szkolenia.\nChociaż w oryginalnym artykule stwierdzono, że normalizacja partii działa poprzez “redukcję wewnętrznego przesunięcia kowariancji”, nikt tak naprawdę nie wie na pewno, dlaczego normalizacja partii pomaga. Różne hipotezy istnieją, ale nie ma pewności.\nW praktyce, głównym efektem normalizacji partii wydaje się być to, że pomaga ona w propagacji gradientu - podobnie jak połączenia resztkowe - i tym samym pozwala na tworzenie głębszych sieci. Niektóre bardzo głębokie sieci mogą być trenowane tylko wtedy, gdy zawierają wiele warstw BatchNormalization. Na przykład, normalizacja partii jest szeroko stosowana w wielu zaawansowanych architekturach sieci konwolucyjnych, które są dostarczane z keras, takich jak ResNet50, EfficientNet i Xception.\nZarówno layer_dense() jak i layer_conv_2d() wykorzystują wektor obciążeń (ang. bias), wyuczoną zmienną, której celem jest uczynienie warstwy afiniczną, a nie czysto liniową. Na przykład, funkcja layer_ conv_2d() zwraca, y = conv(x, kernel) + bias, a layer_dense() zwraca y = dot(x, kernel) + bias. Ponieważ krok normalizacji zajmie się wyśrodkowaniem wyjścia warstwy na zero, wektor obciążenia nie jest już potrzebny w użyciu funkcji layer_batch_normalization(), a warstwę można utworzyć bez niego za pomocą opcji use_bias = FALSE.\nCo ważne, generalnie zalecałbym umieszczanie aktywacji poprzedniej warstwy po warstwie normalizacji wsadowej (choć to wciąż temat do dyskusji).\n\n\nKod\nx |&gt;\n layer_conv_2d(32, 3, activation = \"relu\") |&gt;\n layer_batch_normalization()\n\n# lub\n\nx |&gt;\n  layer_conv_2d(32, 3, use_bias = FALSE) |&gt;\n  layer_batch_normalization() |&gt;\n  layer_activation(\"relu\")\n\n\nIntuicyjnym powodem tego podejścia jest to, że normalizacja partii skupi twoje wejścia na zerze, podczas gdy twoja aktywacja relu używa zera jako punktu zwrotnego dla utrzymania lub porzucenia aktywowanych kanałów: robienie normalizacji przed aktywacją maksymalizuje wykorzystanie relu.\nNormalizacja wsadowa ma wiele pułapek. Jedena z głównych dotyczy dostrajania: podczas dostrajania modelu, który zawiera warstwy BatchNormalization, zaleca się pozostawienie tych warstw zamrożonymi (wywołaj freeze_weights(), aby ustawić ich atrybut trainable na FALSE). W przeciwnym razie, będą one aktualizować swoją wewnętrzną średnią i wariancję, co może kolidować z bardzo małymi aktualizacjami zastosowanymi w otaczających warstwach Conv2D:\n\n\nKod\nbatch_norm_layer_s3_classname &lt;- class(layer_batch_normalization())[1]\nbatch_norm_layer_s3_classname\n\n\n[1] \"keras.layers.normalization.batch_normalization.BatchNormalization\"\n\n\nKod\nis_batch_norm_layer &lt;- function(x) inherits(x, batch_norm_layer_s3_classname)\n\nmodel &lt;- application_efficientnet_b0()\n\nfor (layer in model$layers) {\n  if (is_batch_norm_layer(layer)) {\n    layer$trainable &lt;- FALSE\n  }\n}\n\nmodel\n\n\nModel: \"efficientnetb0\"\n________________________________________________________________________________\n Layer (type)         Output Shape   Param #  Connected to           Trainable  \n================================================================================\n input_5 (InputLayer)  [(None, 224,   0       []                     Y          \n                      224, 3)]                                                  \n rescaling_2 (Rescali  (None, 224, 2  0       ['input_5[0][0]']      Y          \n ng)                  24, 3)                                                    \n normalization (Norma  (None, 224, 2  7       ['rescaling_2[0][0]']  Y          \n lization)            24, 3)                                                    \n rescaling_3 (Rescali  (None, 224, 2  0       ['normalization[0][0]  Y          \n ng)                  24, 3)                  ']                                \n stem_conv_pad (ZeroP  (None, 225, 2  0       ['rescaling_3[0][0]']  Y          \n adding2D)            25, 3)                                                    \n stem_conv (Conv2D)   (None, 112, 1  864      ['stem_conv_pad[0][0]  Y          \n                      12, 32)                 ']                                \n stem_bn (BatchNormal  (None, 112, 1  128     ['stem_conv[0][0]']    N          \n ization)             12, 32)                                                   \n stem_activation (Act  (None, 112, 1  0       ['stem_bn[0][0]']      Y          \n ivation)             12, 32)                                                   \n block1a_dwconv (Dept  (None, 112, 1  288     ['stem_activation[0][  Y          \n hwiseConv2D)         12, 32)                 0]']                              \n block1a_bn (BatchNor  (None, 112, 1  128     ['block1a_dwconv[0][0  N          \n malization)          12, 32)                 ]']                               \n block1a_activation (  (None, 112, 1  0       ['block1a_bn[0][0]']   Y          \n Activation)          12, 32)                                                   \n block1a_se_squeeze (  (None, 32)    0        ['block1a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block1a_se_reshape (  (None, 1, 1,   0       ['block1a_se_squeeze[  Y          \n Reshape)             32)                     0][0]']                           \n block1a_se_reduce (C  (None, 1, 1,   264     ['block1a_se_reshape[  Y          \n onv2D)               8)                      0][0]']                           \n block1a_se_expand (C  (None, 1, 1,   288     ['block1a_se_reduce[0  Y          \n onv2D)               32)                     ][0]']                            \n block1a_se_excite (M  (None, 112, 1  0       ['block1a_activation[  Y          \n ultiply)             12, 32)                 0][0]',                           \n                                               'block1a_se_expand[0             \n                                              ][0]']                            \n block1a_project_conv  (None, 112, 1  512     ['block1a_se_excite[0  Y          \n  (Conv2D)            12, 16)                 ][0]']                            \n block1a_project_bn (  (None, 112, 1  64      ['block1a_project_con  N          \n BatchNormalization)  12, 16)                 v[0][0]']                         \n block2a_expand_conv   (None, 112, 1  1536    ['block1a_project_bn[  Y          \n (Conv2D)             12, 96)                 0][0]']                           \n block2a_expand_bn (B  (None, 112, 1  384     ['block2a_expand_conv  N          \n atchNormalization)   12, 96)                 [0][0]']                          \n block2a_expand_activ  (None, 112, 1  0       ['block2a_expand_bn[0  Y          \n ation (Activation)   12, 96)                 ][0]']                            \n block2a_dwconv_pad (  (None, 113, 1  0       ['block2a_expand_acti  Y          \n ZeroPadding2D)       13, 96)                 vation[0][0]']                    \n block2a_dwconv (Dept  (None, 56, 56  864     ['block2a_dwconv_pad[  Y          \n hwiseConv2D)         , 96)                   0][0]']                           \n block2a_bn (BatchNor  (None, 56, 56  384     ['block2a_dwconv[0][0  N          \n malization)          , 96)                   ]']                               \n block2a_activation (  (None, 56, 56  0       ['block2a_bn[0][0]']   Y          \n Activation)          , 96)                                                     \n block2a_se_squeeze (  (None, 96)    0        ['block2a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block2a_se_reshape (  (None, 1, 1,   0       ['block2a_se_squeeze[  Y          \n Reshape)             96)                     0][0]']                           \n block2a_se_reduce (C  (None, 1, 1,   388     ['block2a_se_reshape[  Y          \n onv2D)               4)                      0][0]']                           \n block2a_se_expand (C  (None, 1, 1,   480     ['block2a_se_reduce[0  Y          \n onv2D)               96)                     ][0]']                            \n block2a_se_excite (M  (None, 56, 56  0       ['block2a_activation[  Y          \n ultiply)             , 96)                   0][0]',                           \n                                               'block2a_se_expand[0             \n                                              ][0]']                            \n block2a_project_conv  (None, 56, 56  2304    ['block2a_se_excite[0  Y          \n  (Conv2D)            , 24)                   ][0]']                            \n block2a_project_bn (  (None, 56, 56  96      ['block2a_project_con  N          \n BatchNormalization)  , 24)                   v[0][0]']                         \n block2b_expand_conv   (None, 56, 56  3456    ['block2a_project_bn[  Y          \n (Conv2D)             , 144)                  0][0]']                           \n block2b_expand_bn (B  (None, 56, 56  576     ['block2b_expand_conv  N          \n atchNormalization)   , 144)                  [0][0]']                          \n block2b_expand_activ  (None, 56, 56  0       ['block2b_expand_bn[0  Y          \n ation (Activation)   , 144)                  ][0]']                            \n block2b_dwconv (Dept  (None, 56, 56  1296    ['block2b_expand_acti  Y          \n hwiseConv2D)         , 144)                  vation[0][0]']                    \n block2b_bn (BatchNor  (None, 56, 56  576     ['block2b_dwconv[0][0  N          \n malization)          , 144)                  ]']                               \n block2b_activation (  (None, 56, 56  0       ['block2b_bn[0][0]']   Y          \n Activation)          , 144)                                                    \n block2b_se_squeeze (  (None, 144)   0        ['block2b_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block2b_se_reshape (  (None, 1, 1,   0       ['block2b_se_squeeze[  Y          \n Reshape)             144)                    0][0]']                           \n block2b_se_reduce (C  (None, 1, 1,   870     ['block2b_se_reshape[  Y          \n onv2D)               6)                      0][0]']                           \n block2b_se_expand (C  (None, 1, 1,   1008    ['block2b_se_reduce[0  Y          \n onv2D)               144)                    ][0]']                            \n block2b_se_excite (M  (None, 56, 56  0       ['block2b_activation[  Y          \n ultiply)             , 144)                  0][0]',                           \n                                               'block2b_se_expand[0             \n                                              ][0]']                            \n block2b_project_conv  (None, 56, 56  3456    ['block2b_se_excite[0  Y          \n  (Conv2D)            , 24)                   ][0]']                            \n block2b_project_bn (  (None, 56, 56  96      ['block2b_project_con  N          \n BatchNormalization)  , 24)                   v[0][0]']                         \n block2b_drop (Dropou  (None, 56, 56  0       ['block2b_project_bn[  Y          \n t)                   , 24)                   0][0]']                           \n block2b_add (Add)    (None, 56, 56  0        ['block2b_drop[0][0]'  Y          \n                      , 24)                   , 'block2a_project_bn             \n                                              [0][0]']                          \n block3a_expand_conv   (None, 56, 56  3456    ['block2b_add[0][0]']  Y          \n (Conv2D)             , 144)                                                    \n block3a_expand_bn (B  (None, 56, 56  576     ['block3a_expand_conv  N          \n atchNormalization)   , 144)                  [0][0]']                          \n block3a_expand_activ  (None, 56, 56  0       ['block3a_expand_bn[0  Y          \n ation (Activation)   , 144)                  ][0]']                            \n block3a_dwconv_pad (  (None, 59, 59  0       ['block3a_expand_acti  Y          \n ZeroPadding2D)       , 144)                  vation[0][0]']                    \n block3a_dwconv (Dept  (None, 28, 28  3600    ['block3a_dwconv_pad[  Y          \n hwiseConv2D)         , 144)                  0][0]']                           \n block3a_bn (BatchNor  (None, 28, 28  576     ['block3a_dwconv[0][0  N          \n malization)          , 144)                  ]']                               \n block3a_activation (  (None, 28, 28  0       ['block3a_bn[0][0]']   Y          \n Activation)          , 144)                                                    \n block3a_se_squeeze (  (None, 144)   0        ['block3a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block3a_se_reshape (  (None, 1, 1,   0       ['block3a_se_squeeze[  Y          \n Reshape)             144)                    0][0]']                           \n block3a_se_reduce (C  (None, 1, 1,   870     ['block3a_se_reshape[  Y          \n onv2D)               6)                      0][0]']                           \n block3a_se_expand (C  (None, 1, 1,   1008    ['block3a_se_reduce[0  Y          \n onv2D)               144)                    ][0]']                            \n block3a_se_excite (M  (None, 28, 28  0       ['block3a_activation[  Y          \n ultiply)             , 144)                  0][0]',                           \n                                               'block3a_se_expand[0             \n                                              ][0]']                            \n block3a_project_conv  (None, 28, 28  5760    ['block3a_se_excite[0  Y          \n  (Conv2D)            , 40)                   ][0]']                            \n block3a_project_bn (  (None, 28, 28  160     ['block3a_project_con  N          \n BatchNormalization)  , 40)                   v[0][0]']                         \n block3b_expand_conv   (None, 28, 28  9600    ['block3a_project_bn[  Y          \n (Conv2D)             , 240)                  0][0]']                           \n block3b_expand_bn (B  (None, 28, 28  960     ['block3b_expand_conv  N          \n atchNormalization)   , 240)                  [0][0]']                          \n block3b_expand_activ  (None, 28, 28  0       ['block3b_expand_bn[0  Y          \n ation (Activation)   , 240)                  ][0]']                            \n block3b_dwconv (Dept  (None, 28, 28  6000    ['block3b_expand_acti  Y          \n hwiseConv2D)         , 240)                  vation[0][0]']                    \n block3b_bn (BatchNor  (None, 28, 28  960     ['block3b_dwconv[0][0  N          \n malization)          , 240)                  ]']                               \n block3b_activation (  (None, 28, 28  0       ['block3b_bn[0][0]']   Y          \n Activation)          , 240)                                                    \n block3b_se_squeeze (  (None, 240)   0        ['block3b_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block3b_se_reshape (  (None, 1, 1,   0       ['block3b_se_squeeze[  Y          \n Reshape)             240)                    0][0]']                           \n block3b_se_reduce (C  (None, 1, 1,   2410    ['block3b_se_reshape[  Y          \n onv2D)               10)                     0][0]']                           \n block3b_se_expand (C  (None, 1, 1,   2640    ['block3b_se_reduce[0  Y          \n onv2D)               240)                    ][0]']                            \n block3b_se_excite (M  (None, 28, 28  0       ['block3b_activation[  Y          \n ultiply)             , 240)                  0][0]',                           \n                                               'block3b_se_expand[0             \n                                              ][0]']                            \n block3b_project_conv  (None, 28, 28  9600    ['block3b_se_excite[0  Y          \n  (Conv2D)            , 40)                   ][0]']                            \n block3b_project_bn (  (None, 28, 28  160     ['block3b_project_con  N          \n BatchNormalization)  , 40)                   v[0][0]']                         \n block3b_drop (Dropou  (None, 28, 28  0       ['block3b_project_bn[  Y          \n t)                   , 40)                   0][0]']                           \n block3b_add (Add)    (None, 28, 28  0        ['block3b_drop[0][0]'  Y          \n                      , 40)                   , 'block3a_project_bn             \n                                              [0][0]']                          \n block4a_expand_conv   (None, 28, 28  9600    ['block3b_add[0][0]']  Y          \n (Conv2D)             , 240)                                                    \n block4a_expand_bn (B  (None, 28, 28  960     ['block4a_expand_conv  N          \n atchNormalization)   , 240)                  [0][0]']                          \n block4a_expand_activ  (None, 28, 28  0       ['block4a_expand_bn[0  Y          \n ation (Activation)   , 240)                  ][0]']                            \n block4a_dwconv_pad (  (None, 29, 29  0       ['block4a_expand_acti  Y          \n ZeroPadding2D)       , 240)                  vation[0][0]']                    \n block4a_dwconv (Dept  (None, 14, 14  2160    ['block4a_dwconv_pad[  Y          \n hwiseConv2D)         , 240)                  0][0]']                           \n block4a_bn (BatchNor  (None, 14, 14  960     ['block4a_dwconv[0][0  N          \n malization)          , 240)                  ]']                               \n block4a_activation (  (None, 14, 14  0       ['block4a_bn[0][0]']   Y          \n Activation)          , 240)                                                    \n block4a_se_squeeze (  (None, 240)   0        ['block4a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block4a_se_reshape (  (None, 1, 1,   0       ['block4a_se_squeeze[  Y          \n Reshape)             240)                    0][0]']                           \n block4a_se_reduce (C  (None, 1, 1,   2410    ['block4a_se_reshape[  Y          \n onv2D)               10)                     0][0]']                           \n block4a_se_expand (C  (None, 1, 1,   2640    ['block4a_se_reduce[0  Y          \n onv2D)               240)                    ][0]']                            \n block4a_se_excite (M  (None, 14, 14  0       ['block4a_activation[  Y          \n ultiply)             , 240)                  0][0]',                           \n                                               'block4a_se_expand[0             \n                                              ][0]']                            \n block4a_project_conv  (None, 14, 14  19200   ['block4a_se_excite[0  Y          \n  (Conv2D)            , 80)                   ][0]']                            \n block4a_project_bn (  (None, 14, 14  320     ['block4a_project_con  N          \n BatchNormalization)  , 80)                   v[0][0]']                         \n block4b_expand_conv   (None, 14, 14  38400   ['block4a_project_bn[  Y          \n (Conv2D)             , 480)                  0][0]']                           \n block4b_expand_bn (B  (None, 14, 14  1920    ['block4b_expand_conv  N          \n atchNormalization)   , 480)                  [0][0]']                          \n block4b_expand_activ  (None, 14, 14  0       ['block4b_expand_bn[0  Y          \n ation (Activation)   , 480)                  ][0]']                            \n block4b_dwconv (Dept  (None, 14, 14  4320    ['block4b_expand_acti  Y          \n hwiseConv2D)         , 480)                  vation[0][0]']                    \n block4b_bn (BatchNor  (None, 14, 14  1920    ['block4b_dwconv[0][0  N          \n malization)          , 480)                  ]']                               \n block4b_activation (  (None, 14, 14  0       ['block4b_bn[0][0]']   Y          \n Activation)          , 480)                                                    \n block4b_se_squeeze (  (None, 480)   0        ['block4b_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block4b_se_reshape (  (None, 1, 1,   0       ['block4b_se_squeeze[  Y          \n Reshape)             480)                    0][0]']                           \n block4b_se_reduce (C  (None, 1, 1,   9620    ['block4b_se_reshape[  Y          \n onv2D)               20)                     0][0]']                           \n block4b_se_expand (C  (None, 1, 1,   10080   ['block4b_se_reduce[0  Y          \n onv2D)               480)                    ][0]']                            \n block4b_se_excite (M  (None, 14, 14  0       ['block4b_activation[  Y          \n ultiply)             , 480)                  0][0]',                           \n                                               'block4b_se_expand[0             \n                                              ][0]']                            \n block4b_project_conv  (None, 14, 14  38400   ['block4b_se_excite[0  Y          \n  (Conv2D)            , 80)                   ][0]']                            \n block4b_project_bn (  (None, 14, 14  320     ['block4b_project_con  N          \n BatchNormalization)  , 80)                   v[0][0]']                         \n block4b_drop (Dropou  (None, 14, 14  0       ['block4b_project_bn[  Y          \n t)                   , 80)                   0][0]']                           \n block4b_add (Add)    (None, 14, 14  0        ['block4b_drop[0][0]'  Y          \n                      , 80)                   , 'block4a_project_bn             \n                                              [0][0]']                          \n block4c_expand_conv   (None, 14, 14  38400   ['block4b_add[0][0]']  Y          \n (Conv2D)             , 480)                                                    \n block4c_expand_bn (B  (None, 14, 14  1920    ['block4c_expand_conv  N          \n atchNormalization)   , 480)                  [0][0]']                          \n block4c_expand_activ  (None, 14, 14  0       ['block4c_expand_bn[0  Y          \n ation (Activation)   , 480)                  ][0]']                            \n block4c_dwconv (Dept  (None, 14, 14  4320    ['block4c_expand_acti  Y          \n hwiseConv2D)         , 480)                  vation[0][0]']                    \n block4c_bn (BatchNor  (None, 14, 14  1920    ['block4c_dwconv[0][0  N          \n malization)          , 480)                  ]']                               \n block4c_activation (  (None, 14, 14  0       ['block4c_bn[0][0]']   Y          \n Activation)          , 480)                                                    \n block4c_se_squeeze (  (None, 480)   0        ['block4c_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block4c_se_reshape (  (None, 1, 1,   0       ['block4c_se_squeeze[  Y          \n Reshape)             480)                    0][0]']                           \n block4c_se_reduce (C  (None, 1, 1,   9620    ['block4c_se_reshape[  Y          \n onv2D)               20)                     0][0]']                           \n block4c_se_expand (C  (None, 1, 1,   10080   ['block4c_se_reduce[0  Y          \n onv2D)               480)                    ][0]']                            \n block4c_se_excite (M  (None, 14, 14  0       ['block4c_activation[  Y          \n ultiply)             , 480)                  0][0]',                           \n                                               'block4c_se_expand[0             \n                                              ][0]']                            \n block4c_project_conv  (None, 14, 14  38400   ['block4c_se_excite[0  Y          \n  (Conv2D)            , 80)                   ][0]']                            \n block4c_project_bn (  (None, 14, 14  320     ['block4c_project_con  N          \n BatchNormalization)  , 80)                   v[0][0]']                         \n block4c_drop (Dropou  (None, 14, 14  0       ['block4c_project_bn[  Y          \n t)                   , 80)                   0][0]']                           \n block4c_add (Add)    (None, 14, 14  0        ['block4c_drop[0][0]'  Y          \n                      , 80)                   , 'block4b_add[0][0]'             \n                                              ]                                 \n block5a_expand_conv   (None, 14, 14  38400   ['block4c_add[0][0]']  Y          \n (Conv2D)             , 480)                                                    \n block5a_expand_bn (B  (None, 14, 14  1920    ['block5a_expand_conv  N          \n atchNormalization)   , 480)                  [0][0]']                          \n block5a_expand_activ  (None, 14, 14  0       ['block5a_expand_bn[0  Y          \n ation (Activation)   , 480)                  ][0]']                            \n block5a_dwconv (Dept  (None, 14, 14  12000   ['block5a_expand_acti  Y          \n hwiseConv2D)         , 480)                  vation[0][0]']                    \n block5a_bn (BatchNor  (None, 14, 14  1920    ['block5a_dwconv[0][0  N          \n malization)          , 480)                  ]']                               \n block5a_activation (  (None, 14, 14  0       ['block5a_bn[0][0]']   Y          \n Activation)          , 480)                                                    \n block5a_se_squeeze (  (None, 480)   0        ['block5a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block5a_se_reshape (  (None, 1, 1,   0       ['block5a_se_squeeze[  Y          \n Reshape)             480)                    0][0]']                           \n block5a_se_reduce (C  (None, 1, 1,   9620    ['block5a_se_reshape[  Y          \n onv2D)               20)                     0][0]']                           \n block5a_se_expand (C  (None, 1, 1,   10080   ['block5a_se_reduce[0  Y          \n onv2D)               480)                    ][0]']                            \n block5a_se_excite (M  (None, 14, 14  0       ['block5a_activation[  Y          \n ultiply)             , 480)                  0][0]',                           \n                                               'block5a_se_expand[0             \n                                              ][0]']                            \n block5a_project_conv  (None, 14, 14  53760   ['block5a_se_excite[0  Y          \n  (Conv2D)            , 112)                  ][0]']                            \n block5a_project_bn (  (None, 14, 14  448     ['block5a_project_con  N          \n BatchNormalization)  , 112)                  v[0][0]']                         \n block5b_expand_conv   (None, 14, 14  75264   ['block5a_project_bn[  Y          \n (Conv2D)             , 672)                  0][0]']                           \n block5b_expand_bn (B  (None, 14, 14  2688    ['block5b_expand_conv  N          \n atchNormalization)   , 672)                  [0][0]']                          \n block5b_expand_activ  (None, 14, 14  0       ['block5b_expand_bn[0  Y          \n ation (Activation)   , 672)                  ][0]']                            \n block5b_dwconv (Dept  (None, 14, 14  16800   ['block5b_expand_acti  Y          \n hwiseConv2D)         , 672)                  vation[0][0]']                    \n block5b_bn (BatchNor  (None, 14, 14  2688    ['block5b_dwconv[0][0  N          \n malization)          , 672)                  ]']                               \n block5b_activation (  (None, 14, 14  0       ['block5b_bn[0][0]']   Y          \n Activation)          , 672)                                                    \n block5b_se_squeeze (  (None, 672)   0        ['block5b_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block5b_se_reshape (  (None, 1, 1,   0       ['block5b_se_squeeze[  Y          \n Reshape)             672)                    0][0]']                           \n block5b_se_reduce (C  (None, 1, 1,   18844   ['block5b_se_reshape[  Y          \n onv2D)               28)                     0][0]']                           \n block5b_se_expand (C  (None, 1, 1,   19488   ['block5b_se_reduce[0  Y          \n onv2D)               672)                    ][0]']                            \n block5b_se_excite (M  (None, 14, 14  0       ['block5b_activation[  Y          \n ultiply)             , 672)                  0][0]',                           \n                                               'block5b_se_expand[0             \n                                              ][0]']                            \n block5b_project_conv  (None, 14, 14  75264   ['block5b_se_excite[0  Y          \n  (Conv2D)            , 112)                  ][0]']                            \n block5b_project_bn (  (None, 14, 14  448     ['block5b_project_con  N          \n BatchNormalization)  , 112)                  v[0][0]']                         \n block5b_drop (Dropou  (None, 14, 14  0       ['block5b_project_bn[  Y          \n t)                   , 112)                  0][0]']                           \n block5b_add (Add)    (None, 14, 14  0        ['block5b_drop[0][0]'  Y          \n                      , 112)                  , 'block5a_project_bn             \n                                              [0][0]']                          \n block5c_expand_conv   (None, 14, 14  75264   ['block5b_add[0][0]']  Y          \n (Conv2D)             , 672)                                                    \n block5c_expand_bn (B  (None, 14, 14  2688    ['block5c_expand_conv  N          \n atchNormalization)   , 672)                  [0][0]']                          \n block5c_expand_activ  (None, 14, 14  0       ['block5c_expand_bn[0  Y          \n ation (Activation)   , 672)                  ][0]']                            \n block5c_dwconv (Dept  (None, 14, 14  16800   ['block5c_expand_acti  Y          \n hwiseConv2D)         , 672)                  vation[0][0]']                    \n block5c_bn (BatchNor  (None, 14, 14  2688    ['block5c_dwconv[0][0  N          \n malization)          , 672)                  ]']                               \n block5c_activation (  (None, 14, 14  0       ['block5c_bn[0][0]']   Y          \n Activation)          , 672)                                                    \n block5c_se_squeeze (  (None, 672)   0        ['block5c_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block5c_se_reshape (  (None, 1, 1,   0       ['block5c_se_squeeze[  Y          \n Reshape)             672)                    0][0]']                           \n block5c_se_reduce (C  (None, 1, 1,   18844   ['block5c_se_reshape[  Y          \n onv2D)               28)                     0][0]']                           \n block5c_se_expand (C  (None, 1, 1,   19488   ['block5c_se_reduce[0  Y          \n onv2D)               672)                    ][0]']                            \n block5c_se_excite (M  (None, 14, 14  0       ['block5c_activation[  Y          \n ultiply)             , 672)                  0][0]',                           \n                                               'block5c_se_expand[0             \n                                              ][0]']                            \n block5c_project_conv  (None, 14, 14  75264   ['block5c_se_excite[0  Y          \n  (Conv2D)            , 112)                  ][0]']                            \n block5c_project_bn (  (None, 14, 14  448     ['block5c_project_con  N          \n BatchNormalization)  , 112)                  v[0][0]']                         \n block5c_drop (Dropou  (None, 14, 14  0       ['block5c_project_bn[  Y          \n t)                   , 112)                  0][0]']                           \n block5c_add (Add)    (None, 14, 14  0        ['block5c_drop[0][0]'  Y          \n                      , 112)                  , 'block5b_add[0][0]'             \n                                              ]                                 \n block6a_expand_conv   (None, 14, 14  75264   ['block5c_add[0][0]']  Y          \n (Conv2D)             , 672)                                                    \n block6a_expand_bn (B  (None, 14, 14  2688    ['block6a_expand_conv  N          \n atchNormalization)   , 672)                  [0][0]']                          \n block6a_expand_activ  (None, 14, 14  0       ['block6a_expand_bn[0  Y          \n ation (Activation)   , 672)                  ][0]']                            \n block6a_dwconv_pad (  (None, 17, 17  0       ['block6a_expand_acti  Y          \n ZeroPadding2D)       , 672)                  vation[0][0]']                    \n block6a_dwconv (Dept  (None, 7, 7,   16800   ['block6a_dwconv_pad[  Y          \n hwiseConv2D)         672)                    0][0]']                           \n block6a_bn (BatchNor  (None, 7, 7,   2688    ['block6a_dwconv[0][0  N          \n malization)          672)                    ]']                               \n block6a_activation (  (None, 7, 7,   0       ['block6a_bn[0][0]']   Y          \n Activation)          672)                                                      \n block6a_se_squeeze (  (None, 672)   0        ['block6a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block6a_se_reshape (  (None, 1, 1,   0       ['block6a_se_squeeze[  Y          \n Reshape)             672)                    0][0]']                           \n block6a_se_reduce (C  (None, 1, 1,   18844   ['block6a_se_reshape[  Y          \n onv2D)               28)                     0][0]']                           \n block6a_se_expand (C  (None, 1, 1,   19488   ['block6a_se_reduce[0  Y          \n onv2D)               672)                    ][0]']                            \n block6a_se_excite (M  (None, 7, 7,   0       ['block6a_activation[  Y          \n ultiply)             672)                    0][0]',                           \n                                               'block6a_se_expand[0             \n                                              ][0]']                            \n block6a_project_conv  (None, 7, 7,   129024  ['block6a_se_excite[0  Y          \n  (Conv2D)            192)                    ][0]']                            \n block6a_project_bn (  (None, 7, 7,   768     ['block6a_project_con  N          \n BatchNormalization)  192)                    v[0][0]']                         \n block6b_expand_conv   (None, 7, 7,   221184  ['block6a_project_bn[  Y          \n (Conv2D)             1152)                   0][0]']                           \n block6b_expand_bn (B  (None, 7, 7,   4608    ['block6b_expand_conv  N          \n atchNormalization)   1152)                   [0][0]']                          \n block6b_expand_activ  (None, 7, 7,   0       ['block6b_expand_bn[0  Y          \n ation (Activation)   1152)                   ][0]']                            \n block6b_dwconv (Dept  (None, 7, 7,   28800   ['block6b_expand_acti  Y          \n hwiseConv2D)         1152)                   vation[0][0]']                    \n block6b_bn (BatchNor  (None, 7, 7,   4608    ['block6b_dwconv[0][0  N          \n malization)          1152)                   ]']                               \n block6b_activation (  (None, 7, 7,   0       ['block6b_bn[0][0]']   Y          \n Activation)          1152)                                                     \n block6b_se_squeeze (  (None, 1152)  0        ['block6b_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block6b_se_reshape (  (None, 1, 1,   0       ['block6b_se_squeeze[  Y          \n Reshape)             1152)                   0][0]']                           \n block6b_se_reduce (C  (None, 1, 1,   55344   ['block6b_se_reshape[  Y          \n onv2D)               48)                     0][0]']                           \n block6b_se_expand (C  (None, 1, 1,   56448   ['block6b_se_reduce[0  Y          \n onv2D)               1152)                   ][0]']                            \n block6b_se_excite (M  (None, 7, 7,   0       ['block6b_activation[  Y          \n ultiply)             1152)                   0][0]',                           \n                                               'block6b_se_expand[0             \n                                              ][0]']                            \n block6b_project_conv  (None, 7, 7,   221184  ['block6b_se_excite[0  Y          \n  (Conv2D)            192)                    ][0]']                            \n block6b_project_bn (  (None, 7, 7,   768     ['block6b_project_con  N          \n BatchNormalization)  192)                    v[0][0]']                         \n block6b_drop (Dropou  (None, 7, 7,   0       ['block6b_project_bn[  Y          \n t)                   192)                    0][0]']                           \n block6b_add (Add)    (None, 7, 7,   0        ['block6b_drop[0][0]'  Y          \n                      192)                    , 'block6a_project_bn             \n                                              [0][0]']                          \n block6c_expand_conv   (None, 7, 7,   221184  ['block6b_add[0][0]']  Y          \n (Conv2D)             1152)                                                     \n block6c_expand_bn (B  (None, 7, 7,   4608    ['block6c_expand_conv  N          \n atchNormalization)   1152)                   [0][0]']                          \n block6c_expand_activ  (None, 7, 7,   0       ['block6c_expand_bn[0  Y          \n ation (Activation)   1152)                   ][0]']                            \n block6c_dwconv (Dept  (None, 7, 7,   28800   ['block6c_expand_acti  Y          \n hwiseConv2D)         1152)                   vation[0][0]']                    \n block6c_bn (BatchNor  (None, 7, 7,   4608    ['block6c_dwconv[0][0  N          \n malization)          1152)                   ]']                               \n block6c_activation (  (None, 7, 7,   0       ['block6c_bn[0][0]']   Y          \n Activation)          1152)                                                     \n block6c_se_squeeze (  (None, 1152)  0        ['block6c_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block6c_se_reshape (  (None, 1, 1,   0       ['block6c_se_squeeze[  Y          \n Reshape)             1152)                   0][0]']                           \n block6c_se_reduce (C  (None, 1, 1,   55344   ['block6c_se_reshape[  Y          \n onv2D)               48)                     0][0]']                           \n block6c_se_expand (C  (None, 1, 1,   56448   ['block6c_se_reduce[0  Y          \n onv2D)               1152)                   ][0]']                            \n block6c_se_excite (M  (None, 7, 7,   0       ['block6c_activation[  Y          \n ultiply)             1152)                   0][0]',                           \n                                               'block6c_se_expand[0             \n                                              ][0]']                            \n block6c_project_conv  (None, 7, 7,   221184  ['block6c_se_excite[0  Y          \n  (Conv2D)            192)                    ][0]']                            \n block6c_project_bn (  (None, 7, 7,   768     ['block6c_project_con  N          \n BatchNormalization)  192)                    v[0][0]']                         \n block6c_drop (Dropou  (None, 7, 7,   0       ['block6c_project_bn[  Y          \n t)                   192)                    0][0]']                           \n block6c_add (Add)    (None, 7, 7,   0        ['block6c_drop[0][0]'  Y          \n                      192)                    , 'block6b_add[0][0]'             \n                                              ]                                 \n block6d_expand_conv   (None, 7, 7,   221184  ['block6c_add[0][0]']  Y          \n (Conv2D)             1152)                                                     \n block6d_expand_bn (B  (None, 7, 7,   4608    ['block6d_expand_conv  N          \n atchNormalization)   1152)                   [0][0]']                          \n block6d_expand_activ  (None, 7, 7,   0       ['block6d_expand_bn[0  Y          \n ation (Activation)   1152)                   ][0]']                            \n block6d_dwconv (Dept  (None, 7, 7,   28800   ['block6d_expand_acti  Y          \n hwiseConv2D)         1152)                   vation[0][0]']                    \n block6d_bn (BatchNor  (None, 7, 7,   4608    ['block6d_dwconv[0][0  N          \n malization)          1152)                   ]']                               \n block6d_activation (  (None, 7, 7,   0       ['block6d_bn[0][0]']   Y          \n Activation)          1152)                                                     \n block6d_se_squeeze (  (None, 1152)  0        ['block6d_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block6d_se_reshape (  (None, 1, 1,   0       ['block6d_se_squeeze[  Y          \n Reshape)             1152)                   0][0]']                           \n block6d_se_reduce (C  (None, 1, 1,   55344   ['block6d_se_reshape[  Y          \n onv2D)               48)                     0][0]']                           \n block6d_se_expand (C  (None, 1, 1,   56448   ['block6d_se_reduce[0  Y          \n onv2D)               1152)                   ][0]']                            \n block6d_se_excite (M  (None, 7, 7,   0       ['block6d_activation[  Y          \n ultiply)             1152)                   0][0]',                           \n                                               'block6d_se_expand[0             \n                                              ][0]']                            \n block6d_project_conv  (None, 7, 7,   221184  ['block6d_se_excite[0  Y          \n  (Conv2D)            192)                    ][0]']                            \n block6d_project_bn (  (None, 7, 7,   768     ['block6d_project_con  N          \n BatchNormalization)  192)                    v[0][0]']                         \n block6d_drop (Dropou  (None, 7, 7,   0       ['block6d_project_bn[  Y          \n t)                   192)                    0][0]']                           \n block6d_add (Add)    (None, 7, 7,   0        ['block6d_drop[0][0]'  Y          \n                      192)                    , 'block6c_add[0][0]'             \n                                              ]                                 \n block7a_expand_conv   (None, 7, 7,   221184  ['block6d_add[0][0]']  Y          \n (Conv2D)             1152)                                                     \n block7a_expand_bn (B  (None, 7, 7,   4608    ['block7a_expand_conv  N          \n atchNormalization)   1152)                   [0][0]']                          \n block7a_expand_activ  (None, 7, 7,   0       ['block7a_expand_bn[0  Y          \n ation (Activation)   1152)                   ][0]']                            \n block7a_dwconv (Dept  (None, 7, 7,   10368   ['block7a_expand_acti  Y          \n hwiseConv2D)         1152)                   vation[0][0]']                    \n block7a_bn (BatchNor  (None, 7, 7,   4608    ['block7a_dwconv[0][0  N          \n malization)          1152)                   ]']                               \n block7a_activation (  (None, 7, 7,   0       ['block7a_bn[0][0]']   Y          \n Activation)          1152)                                                     \n block7a_se_squeeze (  (None, 1152)  0        ['block7a_activation[  Y          \n GlobalAveragePooling                         0][0]']                           \n 2D)                                                                            \n block7a_se_reshape (  (None, 1, 1,   0       ['block7a_se_squeeze[  Y          \n Reshape)             1152)                   0][0]']                           \n block7a_se_reduce (C  (None, 1, 1,   55344   ['block7a_se_reshape[  Y          \n onv2D)               48)                     0][0]']                           \n block7a_se_expand (C  (None, 1, 1,   56448   ['block7a_se_reduce[0  Y          \n onv2D)               1152)                   ][0]']                            \n block7a_se_excite (M  (None, 7, 7,   0       ['block7a_activation[  Y          \n ultiply)             1152)                   0][0]',                           \n                                               'block7a_se_expand[0             \n                                              ][0]']                            \n block7a_project_conv  (None, 7, 7,   368640  ['block7a_se_excite[0  Y          \n  (Conv2D)            320)                    ][0]']                            \n block7a_project_bn (  (None, 7, 7,   1280    ['block7a_project_con  N          \n BatchNormalization)  320)                    v[0][0]']                         \n top_conv (Conv2D)    (None, 7, 7,   409600   ['block7a_project_bn[  Y          \n                      1280)                   0][0]']                           \n top_bn (BatchNormali  (None, 7, 7,   5120    ['top_conv[0][0]']     N          \n zation)              1280)                                                     \n top_activation (Acti  (None, 7, 7,   0       ['top_bn[0][0]']       Y          \n vation)              1280)                                                     \n avg_pool (GlobalAver  (None, 1280)  0        ['top_activation[0][0  Y          \n agePooling2D)                                ]']                               \n top_dropout (Dropout  (None, 1280)  0        ['avg_pool[0][0]']     Y          \n )                                                                              \n predictions (Dense)  (None, 1000)   1281000  ['top_dropout[0][0]']  Y          \n================================================================================\nTotal params: 5,330,571\nTrainable params: 5,246,532\nNon-trainable params: 84,039\n________________________________________________________________________________\n\n\n\n\n14.1.3 Konwolucje separowalne\nA gdybym ci powiedział, że istnieje warstwa, której możesz użyć jako zamiennika layer_ conv_2d(), która sprawi, że twój model będzie mniejszy (mniej trenowanych parametrów) i szczuplejszy (mniej operacji zmiennoprzecinkowych) i spowoduje, że wykona kilka punktów procentowych lepiej swoje zadanie? To jest właśnie to, co robi warstwa konwolucji separowalnej wgłębnie (layer_separable_conv_2d() w keras). Warstwa ta wykonuje konwolucję przestrzenną na każdym kanale swojego wejścia, niezależnie, przed zmieszaniem kanałów wyjściowych poprzez konwolucję punktową (konwolucja 1 × 1), jak pokazano na Rysunek 14.8\n\n\n\nRysunek 14.8: Konwolucja separowalna\n\n\nJest to równoznaczne z rozdzieleniem uczenia się cech przestrzennych i uczenia się cech kanałowych. W podobny sposób, jak konwolucja opiera się na założeniu, że wzory w obrazach nie są związane z konkretnymi lokalizacjami, konwolucja separowalna wgłębnie opiera się na założeniu, że lokalizacje przestrzenne w aktywacjach pośrednich są wysoce skorelowane, ale różne kanały są wysoce niezależne . Ponieważ założenie to jest generalnie prawdziwe dla reprezentacji obrazów uczonych przez głębokie sieci neuronowe, służy jako użyteczne założenie, które pomaga modelowi bardziej efektywnie wykorzystać dane treningowe. Model z silniejszymi założeniami dotyczącymi struktury informacji, które będzie musiał przetworzyć, jest lepszym modelem - o ile założenia te są poprawne.\n\n\n\n\n\nKonwolucja separowalna wymaga znacznie mniej parametrów i wymaga mniejszej ilości obliczeń niż zwykła konwolucja, a jednocześnie ma porównywalną moc reprezentacji. W rezultacie otrzymujemy mniejsze modele, które szybciej się zbiegają i są mniej podatne na overfitting. Te zalety stają się szczególnie ważne, gdy trenujesz małe modele od podstaw na ograniczonych danych.\nKonwolucje separowalne wgłębnie są podstawą architektury Xception, wysokowydajnej sieci splotowej.\n\n\n14.1.4 Przykład wykorzystania sieci Xception\nDla przypomnienia, oto zasady architektury sieci splotowej, które poznałeś do tej pory:\n\nTwój model powinien być zorganizowany w powtarzające się bloki warstw, zwykle składających się z wielu warstw konwolucji i warstwy max pooling.\nLiczba filtrów w twoich warstwach powinna rosnąć wraz ze zmniejszaniem się rozmiaru przestrzennych map cech.\nGłębokie i wąskie sieci są lepsze niż szerokie i płytkie.\nWprowadzenie połączeń resztkowych wokół bloków warstw pomaga trenować głębsze sieci.\nKorzystne może być wprowadzenie warstw normalizacji partii po twoich warstwach konwolucji.\nKorzystne może być zastąpienie layer_conv_2d() przez layer_separable_conv_2d(), które są bardziej wydajne pod względem parametrów.\n\nZbierzmy te pomysły razem w jeden model. Jego architektura będzie przypominać sieć Xception. Zastosujemy ją do zadania psy vs. koty z poprzedniego rozdziału.\n\n\nKod\ndata_augmentation &lt;- keras_model_sequential() |&gt;\n  layer_random_flip(\"horizontal\") |&gt;\n  layer_random_rotation(0.1) |&gt;\n  layer_random_zoom(0.2)\n\ninputs &lt;- layer_input(shape = c(180, 180, 3))\n\nx &lt;- inputs |&gt;\n  data_augmentation() |&gt;\n  layer_rescaling(scale = 1 / 255)\n\nx &lt;- x |&gt;\n  layer_conv_2d(32, 5, use_bias = FALSE)\n\nfor (size in c(32, 64, 128, 256, 512)) {\n  residual &lt;- x\n\n  x &lt;- x |&gt;\n    layer_batch_normalization() |&gt;\n    layer_activation(\"relu\") |&gt;\n    layer_separable_conv_2d(size, 3, padding = \"same\", use_bias = FALSE) |&gt;\n    layer_batch_normalization() |&gt;\n    layer_activation(\"relu\") |&gt;\n    layer_separable_conv_2d(size, 3, padding = \"same\", use_bias = FALSE) |&gt;\n    layer_max_pooling_2d(pool_size = 3, strides = 2, padding = \"same\")\n\n  residual &lt;- residual |&gt;\n    layer_conv_2d(size, 1, strides = 2, padding = \"same\", use_bias = FALSE)\n\n  x &lt;- layer_add(list(x, residual))\n}\n\noutputs &lt;- x |&gt;\n  layer_global_average_pooling_2d() |&gt;\n  layer_dropout(0.5) |&gt;\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;- keras_model(inputs, outputs)\n\ntrain_dataset &lt;- image_dataset_from_directory(\n  \"/Users/majerek/Downloads/cats_and_dogs_small/train/\",\n  image_size = c(180, 180),\n  batch_size = 32\n)\n\nvalidation_dataset &lt;- image_dataset_from_directory(\n  \"/Users/majerek/Downloads/cats_and_dogs_small/validation\",\n  image_size = c(180, 180),\n  batch_size = 32\n)\n\ncallbacks &lt;- list(callback_model_checkpoint(filepath = \"models/vgg16_cat_dog.keras\", save_best_only = T))\n\nmodel |&gt;\n  keras::compile(\n    loss = \"binary_crossentropy\",\n    optimizer = \"rmsprop\",\n    metrics = \"accuracy\"\n  )\n\nhistory &lt;- model |&gt;\n  fit(\n    train_dataset,\n    epochs = 90,\n    callbacks = callbacks,\n    validation_data = validation_dataset\n  )\n\n\n\n\nKod\nmodel &lt;- load_model_tf(\"models/vgg16_cat_dog.keras\")\nload(\"models/vgg16_cat_dog_hist.rda\")\nplot(history)\n\n\n\n\n\nNasz nowy model osiąga dokładność testu 90,2%, w porównaniu z 83% dla modelu z poprzedniego rozdziału. Jak widać, stosowanie najlepszych praktyk architektonicznych ma natychmiastowy, znaczący wpływ na wydajność modelu! Chcąc jeszcze bardziej poprawić wydajność, powinieneś zacząć systematycznie dostrajać hiperparametry swojej architektury."
  },
  {
    "objectID": "segmentation.html#co-widzi-sieć-konwolucyjna",
    "href": "segmentation.html#co-widzi-sieć-konwolucyjna",
    "title": "14  Segmentacja obrazów",
    "section": "14.2 Co widzi sieć konwolucyjna?",
    "text": "14.2 Co widzi sieć konwolucyjna?\nPodstawowym problemem podczas budowania aplikacji wizji komputerowej jest kwestia możliwości interpretacji: dlaczego twój klasyfikator uznał, że dany obraz zawiera lodówkę, podczas gdy wszystko, co widzisz, to ciężarówka? Jest to szczególnie istotne w przypadkach użycia, w których głębokie uczenie jest wykorzystywane do uzupełnienia ludzkiej wiedzy, jak na przykład w przypadkach użycia obrazowania medycznego. Zakończymy ten rozdział zapoznając Cię z szeregiem różnych technik wizualizacji tego, czego uczą się sieci splotowe i zrozumienia podejmowanych przez nie decyzji.\n\n\n\n\n\nCzęsto mówi się, że modele głębokiego uczenia są “czarnymi skrzynkami”: uczą się reprezentacji, które są trudne do wyodrębnienia i przedstawienia w formie czytelnej dla człowieka. Chociaż jest to częściowo prawdą w przypadku niektórych typów modeli głębokiego uczenia, to zdecydowanie nie jest to prawdą w przypadku sieci splotowych. Reprezentacje uczone przez sieci konwolucyjne są wygodne w wizualizacji, w dużej mierze dlatego, że są to reprezentacje wizualnych koncepcji. Od 2013 roku opracowano szeroki wachlarz technik wizualizacji i interpretacji tych reprezentacji. Nie będziemy badać ich wszystkich, ale omówimy trzy z najbardziej przystępnych i użytecznych:\n\nWizualizacja pośrednich wyjść sieci (pośrednich aktywacji) - przydatna do zrozumienia, jak kolejne warstwy sicei przekształcają swoje dane wejściowe, oraz do uzyskania pierwszego wyobrażenia o znaczeniu poszczególnych filtrów sieci.\nWizualizacja filtrów sieci splotwych - przydatna do dokładnego zrozumienia, na jaki wzór wizualny lub pojęcie reaguje każdy filtr w sieci.\nWizualizacja map ciepła aktywacji klas w obrazie - przydatna do zrozumienia, które części obrazu zostały zidentyfikowane jako należące do danej klasy, co pozwala na lokalizację obiektów w obrazach.\n\nDo pierwszej metody - wizualizacji aktywacji - użyjemy małej sieci, który wytrenowaliśmy od podstaw na problemie klasyfikacji psy-vs-koty. W przypadku dwóch kolejnych metod użyjemy wstępnie wytrenowanego modelu Xception.\n\n14.2.0.1 Wizualizacja aktywacji pośrednich\nWizualizacja pośrednich aktywacji polega na wyświetleniu wartości zwracanych przez różne warstwy konwolucji i łączenia w modelu, przy określonym wejściu (wyjście warstwy często nazywane jest jej aktywacją, wyjściem funkcji aktywacji). Daje to wgląd w to, jak dane wejściowe są rozkładane na różne filtry uczone przez sieć. Chcemy wizualizować mapy cech o trzech wymiarach: szerokości, wysokości i głębokości (kanały). Każdy kanał koduje względnie niezależne cechy, więc właściwym sposobem wizualizacji tych map cech jest niezależne wykreślenie zawartości każdego kanału jako obrazu 2D. Zacznijmy od załadowania modelu:\n\n\nKod\nmodel &lt;- load_model_tf(\"models/convnet_from_scratch_with_augmentation.keras\")\nmodel\n\n\nModel: \"model\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_1 (InputLayer)               [(None, 180, 180, 3)]           0           \n sequential (Sequential)            (None, 180, 180, 3)             0           \n rescaling (Rescaling)              (None, 180, 180, 3)             0           \n conv2d_4 (Conv2D)                  (None, 178, 178, 32)            896         \n max_pooling2d_3 (MaxPooling2D)     (None, 89, 89, 32)              0           \n conv2d_3 (Conv2D)                  (None, 87, 87, 64)              18496       \n max_pooling2d_2 (MaxPooling2D)     (None, 43, 43, 64)              0           \n conv2d_2 (Conv2D)                  (None, 41, 41, 128)             73856       \n max_pooling2d_1 (MaxPooling2D)     (None, 20, 20, 128)             0           \n conv2d_1 (Conv2D)                  (None, 18, 18, 256)             295168      \n max_pooling2d (MaxPooling2D)       (None, 9, 9, 256)               0           \n conv2d (Conv2D)                    (None, 7, 7, 256)               590080      \n flatten (Flatten)                  (None, 12544)                   0           \n dropout (Dropout)                  (None, 12544)                   0           \n dense (Dense)                      (None, 1)                       12545       \n================================================================================\nTotal params: 991,041\nTrainable params: 991,041\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nNastępnie pobieramy obraz wejściowy - zdjęcie kota, które nie jest częścią obrazów, na których sieć była trenowana.\n\n\nKod\nimg_path &lt;- get_file(\n  fname = \"cat.jpg\",\n  origin = \"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n\nimg_tensor &lt;- img_path %&gt;%\n  tf_read_image(resize = c(180, 180))\n\ndisplay_image_tensor(img_tensor)\n\n\n\n\n\nAby wyodrębnić mapy cech, na które chcemy spojrzeć, stworzymy model Keras, który przyjmuje partie obrazów jako dane wejściowe i który wyprowadza aktywacje wszystkich warstw konwolucji i łączenia.\n\n\nKod\nconv_layer_s3_classname &lt;- class(layer_conv_2d(NULL, 1, 1))[1]\npooling_layer_s3_classname &lt;- class(layer_max_pooling_2d(NULL))[1]\n\nis_conv_layer &lt;- function(x) inherits(x, conv_layer_s3_classname)\nis_pooling_layer &lt;- function(x) inherits(x, pooling_layer_s3_classname)\n\nlayer_outputs &lt;- list()\nfor (layer in model$layers)\n  if (is_conv_layer(layer) || is_pooling_layer(layer))\n    layer_outputs[[layer$name]] &lt;- layer$output\n\nactivation_model &lt;- keras_model(inputs = model$input,\n                                outputs = layer_outputs)\n\n\nPo podaniu obrazu wejściowego, model ten zwraca wartości aktywacji warstw w oryginalnym modelu, w postaci listy. Ten model ma jedno wejście i osiem wyjść: jedno wyjście na każdą aktywację warstwy.\n\n\nKod\nactivations &lt;- activation_model |&gt; \n  predict(img_tensor[tf$newaxis,,,])\nstr(activations)\n\n\nList of 9\n $ conv2d_4       : num [1, 1:178, 1:178, 1:32] 0 0 0 0 0 0 0 0 0 0 ...\n $ max_pooling2d_3: num [1, 1:89, 1:89, 1:32] 0 0 0 0 0 0 0 0 0 0 ...\n $ conv2d_3       : num [1, 1:87, 1:87, 1:64] 0.0393 0.0651 0.0795 0.0466 0.0943 ...\n $ max_pooling2d_2: num [1, 1:43, 1:43, 1:64] 0.0651 0.0795 0.0961 0.0799 0.1202 ...\n $ conv2d_2       : num [1, 1:41, 1:41, 1:128] 0.1349 0.1584 0.1228 0.0913 0 ...\n $ max_pooling2d_1: num [1, 1:20, 1:20, 1:128] 0.16708 0.12279 0.00736 0 0 ...\n $ conv2d_1       : num [1, 1:18, 1:18, 1:256] 0 0 0 0 0 0 0 0 0 0 ...\n $ max_pooling2d  : num [1, 1:9, 1:9, 1:256] 0 0 0 0 0 0 0 0 0 0 ...\n $ conv2d         : num [1, 1:7, 1:7, 1:256] 0 0 0 0 0.102 ...\n\n\nPrzyjrzyjmy się bliżej aktywacjom pierwszej warstwy.\n\n\nKod\nfirst_layer_activation &lt;- activations[[names(layer_outputs)[1]]]\ndim(first_layer_activation)\n\n\n[1]   1 178 178  32\n\n\nJest to mapa funkcji o wymiarach 178 × 178 z 32 kanałami. Spróbujmy wykreślić trzeci kanał aktywacji pierwszej warstwy oryginalnego modelu (patrz Rysunek 14.9).\n\n\nKod\nplot_activations &lt;- function(x, ...) {\n\n  x &lt;- as.array(x)\n\n  if(sum(x) == 0)\n      return(plot(as.raster(\"gray\")))\n\n  rotate &lt;- function(x) t(apply(x, 2, rev))\n  image(rotate(x), asp = 1, axes = FALSE, useRaster = TRUE,\n        col = terrain.colors(256), ...)\n}\n\nplot_activations(first_layer_activation[, , , 3])\n\n\n\n\n\nRysunek 14.9: Wykres wartości trzeciego kanału pierwszej warstwy aktywacji\n\n\n\n\nTen kanał wydaje się kodować detektor krawędzi ukośnych - ale zauważ, że Twoje własne kanały mogą się różnić, ponieważ konkretne filtry uczone przez warstwy konwolucji nie są deterministyczne.\nWykreślmy teraz pełną wizualizację wszystkich aktywacji w sieci. Wyodrębnimy i wykreślimy każdy kanał w każdej z warstw aktywacji, a następnie ułożymy wyniki w jedną dużą siatkę, z kanałami ułożonymi obok siebie.\n\n\nKod\nfor (layer_name in names(layer_outputs)) {\n  layer_output &lt;- activations[[layer_name]]\n\n  n_features &lt;- dim(layer_output) %&gt;% tail(1)\n  par(mfrow = n2mfrow(n_features, asp = 1.75),\n      mar = rep(.1, 4), oma = c(0, 0, 1.5, 0))\n  for (j in 1:n_features)\n    plot_activations(layer_output[, , , j])\n  title(main = layer_name, outer = TRUE)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNależy tu zwrócić uwagę na kilka rzeczy:\n\nPierwsza warstwa działa jako zbiór różnych detektorów krawędzi. Na tym etapie, aktywacje zachowują prawie wszystkie informacje obecne na początkowym obrazie.\nW miarę zagłębiania się, aktywacje stają się coraz bardziej abstrakcyjne i mniej wizualnie interpretowalne. Zaczynają kodować pojęcia wyższego rzędu, takie jak “kocie ucho” czy “kocie oko”. Głębsze prezentacje niosą coraz mniej informacji o wizualnej zawartości obrazu, a coraz więcej informacji związanych z klasą obrazu.\nRozproszenie aktywacji rośnie wraz z głębokością warstwy: w pierwszej warstwie prawie wszystkie filtry są aktywowane przez obraz wejściowy, ale w kolejnych warstwach coraz więcej filtrów jest pustych. Oznacza to, że wzór zakodowany przez filtr nie występuje na obrazie wejściowym.\n\nWykazaliśmy właśnie ważną, uniwersalną cechę reprezentacji uczonych przez głębokie sieci neuronowe: cechy wydobywane przez warstwę stają się coraz bardziej abstrakcyjne wraz z głębokością warstwy. Aktywacje wyższych warstw niosą coraz mniej informacji o konkretnym widzianym wejściu, a coraz więcej informacji o celu (w tym przypadku klasie obrazu: kot czy pies). Głęboka sieć neuronowa działa jak przebieg destylacji informacji, w którym surowe dane (w tym przypadku obrazy RGB) są wielokrotnie przekształcane w taki sposób, że nieistotne informacje są odfiltrowywane (np. specyficzny wygląd wizualny obrazu), a użyteczne informacje są powiększane i udoskonalane (np. klasa obrazu).\n\n\n\n\n\nJest to analogiczne do sposobu, w jaki ludzie i zwierzęta postrzegają świat: po obserwacji sceny przez kilka sekund człowiek może pamiętać, jakie abstrakcyjne obiekty były w niej obecne (rower, drzewo), ale nie może pamiętać konkretnego wyglądu tych obiektów. W rzeczywistości, gdybyś próbował narysować z pamięci ogólny rower, istnieje prawdopodobieństwo, że nie udałoby Ci się to nawet w najmniejszym stopniu, mimo że w swoim życiu widziałeś tysiące rowerów. Twój mózg nauczył się całkowicie abstrahować od informacji wizualnych - przekształcać je w wysokopoziomowe koncepcje wizualne, jednocześnie odfiltrowując nieistotne szczegóły wizualne - co sprawia, że zapamiętanie wyglądu rzeczy wokół Ciebie jest niezwykle trudne.\n\n\n14.2.0.2 Wizualizacja filtrów sieci splotowych\nInnym prostym sposobem na sprawdzenie filtrów wyuczonych przez sieci splotowe jest wyświetlenie wzorca wizualnego, na który każdy filtr ma reagować. Można to zrobić za pomocą spadku gradientu w przestrzeni wejściowej: zastosowanie spadku gradientu do wartości obrazu wejściowego sieci splotowej tak, aby zmaksymalizować odpowiedź określonego filtra, zaczynając od pustego obrazu wejściowego. Wynikowy obraz wejściowy będzie taki, na który wybrany filtr maksymalnie reaguje.\nSpróbujmy to zrobić z filtrami modelu Xception, wytrenowanego na ImageNecie. Proces jest prosty: zbudujemy funkcję straty, która maksymalizuje wartość danego filtra w danej warstwie konwolucji, a następnie użyjemy stochastycznego spadku gradientu, aby dopasować wartości obrazu wejściowego tak, by zmaksymalizować tę wartość aktywacji. Najpierw zainicjujmy model Xception, załadowany wagami wytrenowanymi na zbiorze danych ImageNet.\n\n\nKod\nmodel &lt;- application_xception(\n  weights = \"imagenet\",\n  include_top = FALSE\n)\n\n\nInteresują nas warstwy konwolucyjne modelu - Conv2D i SeparableConv2D. Musimy znać ich nazwy, aby móc pobrać ich dane wyjściowe. Wypiszmy ich nazwy, w kolejności głębokości.\n\n\nKod\nfor (layer in model$layers)\n if(any(grepl(\"Conv2D\", class(layer))))\n   print(layer$name)\n\n\n[1] \"block1_conv1\"\n[1] \"block1_conv2\"\n[1] \"block2_sepconv1\"\n[1] \"block2_sepconv2\"\n[1] \"conv2d_23\"\n[1] \"block3_sepconv1\"\n[1] \"block3_sepconv2\"\n[1] \"conv2d_24\"\n[1] \"block4_sepconv1\"\n[1] \"block4_sepconv2\"\n[1] \"conv2d_25\"\n[1] \"block5_sepconv1\"\n[1] \"block5_sepconv2\"\n[1] \"block5_sepconv3\"\n[1] \"block6_sepconv1\"\n[1] \"block6_sepconv2\"\n[1] \"block6_sepconv3\"\n[1] \"block7_sepconv1\"\n[1] \"block7_sepconv2\"\n[1] \"block7_sepconv3\"\n[1] \"block8_sepconv1\"\n[1] \"block8_sepconv2\"\n[1] \"block8_sepconv3\"\n[1] \"block9_sepconv1\"\n[1] \"block9_sepconv2\"\n[1] \"block9_sepconv3\"\n[1] \"block10_sepconv1\"\n[1] \"block10_sepconv2\"\n[1] \"block10_sepconv3\"\n[1] \"block11_sepconv1\"\n[1] \"block11_sepconv2\"\n[1] \"block11_sepconv3\"\n[1] \"block12_sepconv1\"\n[1] \"block12_sepconv2\"\n[1] \"block12_sepconv3\"\n[1] \"block13_sepconv1\"\n[1] \"block13_sepconv2\"\n[1] \"conv2d_26\"\n[1] \"block14_sepconv1\"\n[1] \"block14_sepconv2\"\n\n\nZauważysz, że separowalne warstwy conv 2D tutaj są nazwane coś jak block6_sepconv1, block7_sepconv2, i tak dalej. Xception jest zorganizowany w bloki, z których każdy zawiera kilka warstw konwolucyjnych. Teraz stwórzmy drugi model, który zwraca wyjście konkretnej warstwy - model ekstraktora cech. Ponieważ nasz model jest modelem zbudowanym za pomocą API, jest on inspekcyjny: możemy zapytać o wyjście jednej z jego warstw i ponownie użyć go w nowym modelu. Nie trzeba kopiować całego kodu Xception.\n\n\nKod\nlayer_name &lt;- \"block3_sepconv1\"\nlayer &lt;- model %&gt;% keras::get_layer(name = layer_name)\nfeature_extractor &lt;- keras_model(inputs = model$input,\n                                 outputs = layer$output)\n\n\nAby użyć tego modelu, wystarczy wywołać go na jakichś danych wejściowych (zauważ, że Xception wymaga, aby dane wejściowe były wstępnie przetworzone za pomocą funkcji xception_preprocess_input()).\n\n\nKod\nactivation &lt;- img_tensor %&gt;%\n   .[tf$newaxis, , , ] %&gt;%\n   xception_preprocess_input() %&gt;%\n   feature_extractor()\n\nstr(activation)\n\n\n&lt;tf.Tensor: shape=(1, 44, 44, 256), dtype=float32, numpy=…&gt;\n\n\nUżyjmy naszego modelu ekstraktora cech do zdefiniowania funkcji, która zwraca wartość skalarną określającą, jak bardzo dany obraz wejściowy “aktywuje” dany filtr w warstwie. Jest to “funkcja straty”, którą będziemy maksymalizować podczas procesu wznoszenia gradientu:\n\n\nKod\ncompute_loss &lt;- function(image, filter_index) {\n  activation &lt;- feature_extractor(image)\n\n  filter_index &lt;- as_tensor(filter_index, \"int32\")\n  filter_activation &lt;-\n      activation[, , , filter_index, style = \"python\"] # aby program wiedział, że kodowanie pierwszego indeksu zaczyna się od 0 - jak w Python\n\n  mean(filter_activation[, 3:-3, 3:-3])\n}\n\n\nUstawmy funkcję krokową gradientu zstępującego, korzystając z funkcji GradientTape(). Nieoczywistą sztuczką pomagającą w płynnym przebiegu procesu spadku gradientu jest normalizacja tensora gradientu poprzez podzielenie go przez jego normę L2 (pierwiastek kwadratowy ze średniej kwadratowej wartości w tensorze). Dzięki temu wielkość aktualizacji obrazu wejściowego jest zawsze w tym samym zakresie.\n\n\nKod\ngradient_ascent_step &lt;-\n  function(image, filter_index, learning_rate) {\n    with(tf$GradientTape() %as% tape, {\n      tape$watch(image)\n      loss &lt;- compute_loss(image, filter_index)\n    })\n    grads &lt;- tape$gradient(loss, image)\n    grads &lt;- tf$math$l2_normalize(grads)\n    image + (learning_rate * grads)\n  }\n\n\nTeraz mamy już wszystkie elementy. Połączmy je w funkcję R, która pobiera jako dane wejściowe nazwę warstwy i indeks filtra i zwraca tensor reprezentujący wzór, który maksymalizuje aktywację określonego filtra. Zauważ, że użyjemy funkcji tf_function(), aby przyspieszyć działanie.\n\n\nKod\nc(img_width, img_height) %&lt;-% c(200, 200)\n\ngenerate_filter_pattern &lt;- tf_function(function(filter_index) {\n  iterations &lt;- 30\n  learning_rate &lt;- 10\n  image &lt;- tf$random$uniform(\n    minval = 0.4, maxval = 0.6,\n    shape = shape(1, img_width, img_height, 3)\n  )\n\n  for (i in seq(iterations))\n      image &lt;- gradient_ascent_step(image, filter_index, learning_rate)\n  image[1, , , ]\n})\n\n\nWynikowy tensor obrazu jest tablicą zmiennoprzecinkową o kształcie (200, 200, 3), z wartościami, które nie mogą być liczbami całkowitymi z zakresu [0, 255]. Dlatego też musimy poddać ten tensor postprocessingowi, aby przekształcić go w obraz możliwy do wyświetlenia. Zrobimy to za pomocą operacji na tensorach i zawiniemy w tf_function(), aby również przyspieszyć.\n\n\nKod\ndeprocess_image &lt;- tf_function(function(image, crop = TRUE) {\n  image &lt;- image - mean(image)\n  image &lt;- image / tf$math$reduce_std(image)\n  image &lt;- (image * 64) + 128\n  image &lt;- tf$clip_by_value(image, 0, 255)\n  if(crop)\n    image &lt;- image[26:-26, 26:-26, ]\n  image\n})\n\ngenerate_filter_pattern(filter_index = as_tensor(2L)) %&gt;%\n deprocess_image() %&gt;%\n  display_image_tensor()\n\n\n\n\n\nZauważ, że złożyliśmy tutaj filter_index z as_tensor(). Robimy to, ponieważ tf_funkcja() kompiluje oddzielną zoptymalizowaną funkcję dla każdego unikalnego sposobu jej wywoływana, a różne stałe liczą się jako unikalna sygnatura wywołania. Wygląda na to, że trzeci filtr w warstwie block3_sepconv1 reaguje na wzór poziomych linii, nieco przypominający wodę.\n\n\nKod\npar(mfrow = c(8, 8))\nfor (i in seq(0, 63)) {\n  generate_filter_pattern(filter_index = as_tensor(i)) %&gt;%\n    deprocess_image() %&gt;%\n    display_image_tensor(plot_margins = rep(.1, 4))\n}\n\n\n\n\n\nRysunek 14.10: Kilka wzorów filtrów dla warstw block2_sepconv1, block4_sepconv1 i block8_sepconv1\n\n\n\n\nTe wizualizacje filtrów (patrz Rysunek 14.10) mówią wiele o tym, jak warstwy sieci splotowej widzą świat: każda warstwa w convnecie uczy się zbioru filtrów w taki sposób, że jej dane wejściowe mogą być wyrażone jako kombinacja filtrów. Jest to podobne do tego, jak transformata Fouriera rozkłada sygnały na bank funkcji cosinusowych. Filtry w tych blokach filtrów sieci splotowej stają się coraz bardziej złożone i wyrafinowane, gdy zagłębiamy się w model:\n\nFiltry z pierwszych warstw w modelu kodują proste krawędzie kierunkowe i kolory (lub kolorowe krawędzie, w niektórych przypadkach).\nFiltry z warstw położonych nieco dalej w górę stosu, takich jak block4_sepconv1, kodują proste tekstury wykonane z kombinacji krawędzi i kolorów.\nFiltry z wyższych warstw zaczynają przypominać tekstury występujące w naturalnych obrazach: pióra, oczy, liście i tak dalej.\n\n\n\n14.2.0.3 Wizualizacja map ciepła aktywacji klasowych\nPrzedstawimy jeszcze jedną technikę wizualizacji - przydatną do zrozumienia, które części danego obrazu doprowadziły sieci splotowe do ostatecznej decyzji klasyfikacyjnej. Jest to pomocne w “debugowaniu” procesu decyzyjnego sieci konwolucyjnych, szczególnie w przypadku błędu klasyfikacji (problem nazywany interpretacją modelu). Może to również pozwolić na zlokalizowanie konkretnych obiektów na obrazie.\n\n\n\n\n\nTa ogólna kategoria technik nazywana jest wizualizacją mapy aktywacji klas (ang. Class Activation Map - CAM) i polega na tworzeniu map ciepła aktywacji klas na obrazach wejściowych. Mapa cieplna aktywacji klas to dwuwymiarowa siatka wyników związanych z konkretną klasą wyjściową, obliczona dla każdej lokalizacji na dowolnym obrazie wejściowym, wskazująca jak ważna jest każda lokalizacja w odniesieniu do rozważanej klasy. Na przykład, biorąc pod uwagę obrazek wprowadzony do sieci splotowej psy-vs-koty, wizualizacja CAM pozwoli nam wygenerować mapę ciepła dla klasy “kot”, wskazując jak podobne do kotów są różne części obrazka, a także mapę ciepła dla klasy “pies”, wskazując jak podobne do psów są części obrazka.\nKonkretna implementacja, której użyjemy, to ta opisana w artykule zatytułowanym “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization”(Selvaraju i in. 2020).\nGrad-CAM polega na tym, że bierzemy wyjściową mapę cech warstwy konwolucji, daną obrazowi wejściowemu, i ważymy każdy kanał w tej mapie cech przez gradient klasy względem kanału. Intuicyjnie, jednym ze sposobów zrozumienia tej sztuczki jest wyobrażenie sobie, że ważymy przestrzenną mapę “jak intensywnie obraz wejściowy aktywuje różne kanały” przez “jak ważny jest każdy kanał w odniesieniu do klasy”, w wyniku czego otrzymujemy przestrzenną mapę “jak intensywnie obraz wejściowy aktywuje klasę”. Zademonstrujmy tę technikę przy użyciu wstępnie wytrenowanego modelu Xception.\n\n\n\nKod\nmodel &lt;- application_xception(weights = \"imagenet\")\n\n\nRozważmy obraz dwóch słoni afrykańskich pokazany na Rysunek 14.11. Przekształćmy ten obraz w coś, co model Xception może odczytać: model był trenowany na obrazach o rozmiarze 299 × 299, wstępnie przetworzonych zgodnie z kilkoma regułami, które są spakowane w funkcji użytkowej xception_preprocess_input().\n\n\nKod\nimg_path &lt;- get_file(\n  fname = \"elephant.jpg\",\n  origin = \"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\nimg_tensor &lt;- tf_read_image(img_path, resize = c(299, 299))\npreprocessed_img &lt;- img_tensor[tf$newaxis, , , ] %&gt;%\n  xception_preprocess_input()\n\ndisplay_image_tensor(img_tensor)\n\n\n\n\n\nRysunek 14.11: Przykładowy obraz słoni\n\n\n\n\nMożesz teraz uruchomić wstępnie wytrenowaną sieć na obrazie i zdekodować jej wektor predykcji z powrotem do formatu czytelnego dla człowieka:\n\n\nKod\npreds &lt;- predict(model, preprocessed_img)\nstr(preds)\n\n\n num [1, 1:1000] 5.51e-06 2.75e-05 1.73e-05 1.19e-05 1.15e-05 ...\n\n\nKod\nimagenet_decode_predictions(preds, top=3)[[1]]\n\n\n  class_name class_description      score\n1  n02504458  African_elephant 0.90519816\n2  n01871265            tusker 0.05259845\n3  n02504013   Indian_elephant 0.01615973\n\n\nTrzy najlepsze klasy przewidywane dla tego obrazu to:\n\nSłoń afrykański (z 90% prawdopodobieństwem)\nTusker (z 5% prawdopodobieństwem)\nSłoń indyjski (z niemal 2% prawdopodobieństwem)\n\nSieć rozpoznała obraz jako zawierający nieokreśloną ilość słoni afrykańskich. Wpisem w wektorze predykcji, który został maksymalnie aktywowany, jest wpis odpowiadający klasie “słoń afrykański”, o indeksie 387:\n\n\nKod\nwhich.max(preds[1, ])\n\n\n[1] 387\n\n\nAby zwizualizować, które części obrazu są najbardziej podobne do afrykańskich słoni, skonfigurujmy proces Grad-CAM. Najpierw tworzymy model, który mapuje obraz wejściowy na aktywacje ostatniej warstwy konwolucyjnej.\n\n\nKod\nlast_conv_layer_name &lt;- \"block14_sepconv2_act\"\nclassifier_layer_names &lt;- c(\"avg_pool\", \"predictions\")\nlast_conv_layer &lt;- model %&gt;% get_layer(last_conv_layer_name)\nlast_conv_layer_model &lt;- keras_model(model$inputs,\n                                     last_conv_layer$output)\n\n\nPo drugie, tworzymy model, który odwzorowuje aktywacje ostatniej warstwy konwolucyjnej na końcowe predykcje klas.\n\n\nKod\nclassifier_input &lt;- layer_input(batch_shape = last_conv_layer$output$shape)\n\nx &lt;- classifier_input\nfor (layer_name in classifier_layer_names)\n x &lt;- get_layer(model, layer_name)(x)\n\nclassifier_model &lt;- keras_model(classifier_input, x)\n\n\nNastępnie obliczamy gradient najwyższej przewidywanej klasy dla naszego obrazu wejściowego w odniesieniu do aktywacji ostatniej warstwy konwolucji.\n\n\nKod\nwith (tf$GradientTape() %as% tape, {\n  last_conv_layer_output &lt;- last_conv_layer_model(preprocessed_img)\n  tape$watch(last_conv_layer_output)\n  preds &lt;- classifier_model(last_conv_layer_output)\n  top_pred_index &lt;- tf$argmax(preds[1, ])\n  top_class_channel &lt;- preds[, top_pred_index, style = \"python\"]\n})\n\ngrads &lt;- tape$gradient(top_class_channel, last_conv_layer_output)\n\n\nTeraz zastosujemy łączenie i ważenie ważności do tensora gradientu, aby uzyskać naszą mapę ciepła aktywacji klas.\n\n\nKod\npooled_grads &lt;- mean(grads, axis = c(1,2,3), keepdims = T)\nheatmap &lt;- (last_conv_layer_output * pooled_grads) |&gt; \n  mean(axis = -1) %&gt;%\n  .[1,,]\n\npar(mar = c(0, 0, 0, 0))\nplot_activations(heatmap)\n\n\n\n\n\nNa koniec, nałóżmy mapę ciepła aktywacji na oryginalny obraz. Wycinamy wartości mapy ciepła do palety kolorów, a następnie konwertujemy do obiektu rastrowego R. Zwróć uwagę, że upewniliśmy się, że przekazaliśmy alfa = .4 do palety, tak abyśmy nadal widzieli oryginalny obraz, gdy nałożymy na niego mapę ciepła. (Zobacz Rysunek 14.12)\n\n\nKod\npal &lt;- hcl.colors(256, palette = \"Spectral\", alpha = .4, rev = TRUE)\nheatmap &lt;- as.array(heatmap)\nheatmap[] &lt;- pal[cut(heatmap, 256)]\nheatmap &lt;- as.raster(heatmap)\n\nimg &lt;- tf_read_image(img_path, resize = NULL)\ndisplay_image_tensor(img)\nrasterImage(heatmap, 0, 0, ncol(img), nrow(img), interpolate = FALSE)\n\n\n\n\n\nRysunek 14.12: Mapa cieplna aktywacji klasy słonia afrykańskiego na zdjęciu testowym\n\n\n\n\nTa technika wizualizacji odpowiada na dwa ważne pytania:\n\nDlaczego sieć uznała, że ten obraz zawiera słonia afrykańskiego?\nGdzie na obrazie znajduje się słoń afrykański?\n\nW szczególności interesujące jest to, że uszy słoniowego malca są silnie aktywowane: prawdopodobnie w ten sposób sieć potrafi odróżnić słonie afrykańskie od indyjskich.\n\n\n\n\n\n\n\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, i Jian Sun. 2015. „Deep Residual Learning for Image Recognition”. arXiv. https://doi.org/10.48550/arXiv.1512.03385.\n\n\nIoffe, Sergey, i Christian Szegedy. 2015. „Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”. arXiv. https://doi.org/10.48550/arXiv.1502.03167.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, i Dhruv Batra. 2020. „Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization”. International Journal of Computer Vision 128 (2): 336–59. https://doi.org/10.1007/s11263-019-01228-7."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Barnard, Stephen T., and Martin A. Fischler. 1982. “Computational\nStereo.” ACM Computing Surveys 14 (4):\n553–72. https://doi.org/10.1145/356893.356896.\n\n\nBarrow, H. G., and J. M. Tenenbaum. 1981. “Computational\nVision.” Proceedings of the IEEE 69 (5): 572–95. https://doi.org/10.1109/PROC.1981.12026.\n\n\nBlake, Andrew, and Michael Isard. 2012. Active\nContours: The Application of\nTechniques from Graphics, Vision,\nControl Theory and Statistics to Visual\nTracking of Shapes in Motion.\nSpringer Science & Business Media.\n\n\nBlake, Andrew, Andrew Zisserman, and Greg Knowles. 1985. “Surface\nDescriptions from Stereo and Shading.” Image and Vision\nComputing, Papers from the 1985 Alvey Computer Vision\nand Image Interpretation Meeting, 3 (4): 183–91. https://doi.org/10.1016/0262-8856(85)90006-X.\n\n\nBurger, Wilhelm, and Mark J. Burge. 2016. Digital Image\nProcessing: An Algorithmic Introduction Using\nJava. Texts in Computer Science.\nLondon: Springer. https://doi.org/10.1007/978-1-4471-6684-9.\n\n\nCanny, John. 1986. “A Computational Approach to\nEdge Detection.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence PAMI-8 (6): 679–98. https://doi.org/10.1109/TPAMI.1986.4767851.\n\n\nChollet, Francois, and J. J. Allaire. 2018. Deep\nLearning with R. Manning\nPublications.\n\n\nCiresan, Dan C, Ueli Meier, Jonathan Masci, Luca M Gambardella, and\nJurgen Schmidhuber. n.d. “Flexible, High Performance\nConvolutional Neural Networks for Image\nClassification.”\n\n\n“Cooperative Computation of Stereo\nDisparity | Science.” n.d.\nhttps://www.science.org/doi/10.1126/science.968482.\n\n\nDavis, Larry S. 1975. “A Survey of Edge Detection\nTechniques.” Computer Graphics and Image Processing 4\n(3): 248–70. https://doi.org/10.1016/0146-664X(75)90012-X.\n\n\n“Deep Learning with Python, Second\nEdition.” n.d. Manning Publications.\nhttps://www.manning.com/books/deep-learning-with-python-second-edition.\n\n\nDev, Parvati. 1975. “Perception of Depth Surfaces in Random-Dot\nStereograms : A Neural Model.” International Journal of\nMan-Machine Studies 7 (4): 511–28. https://doi.org/10.1016/S0020-7373(75)80030-7.\n\n\nFelzenszwalb, Pedro F., and Daniel P. Huttenlocher. 2005.\n“Pictorial Structures for Object\nRecognition.” International Journal of Computer\nVision 61 (1): 55–79. https://doi.org/10.1023/B:VISI.0000042934.15159.49.\n\n\nFergus, R., P. Perona, and A. Zisserman. 2007. “Weakly\nSupervised Scale-Invariant Learning of Models\nfor Visual Recognition.” International Journal\nof Computer Vision 71 (3): 273–303. https://doi.org/10.1007/s11263-006-8707-x.\n\n\nFischler, M., and O. Firschein. 1987. “Readings in Computer\nVision: Issues, Problems, Principles, and Paradigms.” In.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. Illustrated edition. Cambridge,\nMassachusetts: The MIT Press.\n\n\nHanson, Allen. 1978. Computer Vision Systems.\nElsevier.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.\n“Deep Residual Learning for Image\nRecognition.” arXiv. https://doi.org/10.48550/arXiv.1512.03385.\n\n\nHorn, Berthold K P. n.d. “Obtaining Shape from\nShading Information.”\n\n\nIoffe, Sergey, and Christian Szegedy. 2015. “Batch\nNormalization: Accelerating Deep Network\nTraining by Reducing Internal Covariate\nShift.” arXiv. https://doi.org/10.48550/arXiv.1502.03167.\n\n\nKass, Michael, Andrew Witkin, and Demetri Terzopoulos. 1988.\n“Snakes: Active Contour Models.”\nInternational Journal of Computer Vision 1 (4): 321–31. https://doi.org/10.1007/BF00133570.\n\n\nKetkar, Nikhil, and Eder Santana. 2017. Deep Learning with\nPython. Vol. 1. Springer.\n\n\nKirsch, Russell A. 1971. “Computer Determination of the\nConstituent Structure of Biological Images.” Computers and\nBiomedical Research 4 (3): 315–28. https://doi.org/10.1016/0010-4809(71)90034-6.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.\n“ImageNet Classification with Deep Convolutional Neural\nNetworks.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep\nLearning.” Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nMalladi, R., J. A. Sethian, and B. C. Vemuri. 1995. “Shape\nModeling with Front Propagation: A Level Set Approach.” IEEE\nTransactions on Pattern Analysis and Machine Intelligence 17 (2):\n158–75. https://doi.org/10.1109/34.368173.\n\n\nMarr, D., and E. Hildreth. 1980. “Theory of Edge\nDetection.” Proceedings of the Royal Society of London.\nSeries B, Biological Sciences 207 (1167): 187–217. https://doi.org/10.1098/rspb.1980.0020.\n\n\n“Mind as Machine: A History of Cognitive Science.” 2007.\nChoice Reviews Online 44 (11). https://doi.org/10.5860/choice.44-6202.\n\n\nMundy, Joseph L., and Andrew Zisserman, eds. 1992. Geometric\nInvariance in Computer Vision. Cambridge, MA, USA:\nMIT Press.\n\n\nNalwa, Vishvjit S., and Thomas O. Binford. 1986. “On\nDetecting Edges.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence PAMI-8 (6): 699–714. https://doi.org/10.1109/TPAMI.1986.4767852.\n\n\n“Optical and Electro-Optical Information\nProcessing.” n.d. MIT Press.\n\n\n“Picture Processing and Psychopictorics\n- 1st Edition.” n.d.\nhttps://www.elsevier.com/books/picture-processing-and-psychopictorics/lipkin/978-0-12-451550-5.\n\n\nPonce, Jean, Martial Hebert, Cordelia Schmid, and Andrew Zisserman.\n2007. Toward Category-Level Object Recognition.\nSpringer.\n\n\nRoberts, Lawrence G. 1980. Machine Perception of Three-dimensional Solids. Garland\nPub.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015.\n“U-Net: Convolutional Networks for\nBiomedical Image Segmentation.” arXiv.\nhttps://doi.org/10.48550/arXiv.1505.04597.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2020.\n“Grad-CAM: Visual Explanations from\nDeep Networks via Gradient-based\nLocalization.” International Journal of Computer\nVision 128 (2): 336–59. https://doi.org/10.1007/s11263-019-01228-7.\n\n\nSimonyan, Karen, and Andrew Zisserman. 2014. “Very Deep\nConvolutional Networks for Large-Scale Image Recognition.” https://doi.org/10.48550/ARXIV.1409.1556.\n\n\nSzeliski, Richard. 2022. Computer Vision:\nAlgorithms and Applications. Texts in\nComputer Science. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-030-34372-9.\n\n\nWinston, Patrick Henry. 1976. “The Psychology of Computer\nVision.” Pattern Recognition 8 (3): 193. https://doi.org/10.1016/0031-3203(76)90020-0.\n\n\nWitkin, Andrew P. 1981. “Recovering Surface Shape and Orientation\nfrom Texture.” Artificial Intelligence 17 (1): 17–45. https://doi.org/10.1016/0004-3702(81)90019-9.\n\n\nWoodham, Robert J. 1981. “Analysing Images of Curved\nSurfaces.” Artificial Intelligence 17 (1): 117–40. https://doi.org/10.1016/0004-3702(81)90022-9.\n\n\nZhang, T. Y., and C. Y. Suen. 1984. “A Fast Parallel Algorithm for\nThinning Digital Patterns.” Communications of the ACM 27\n(3): 236–39. https://doi.org/10.1145/357994.358023.\n\n\nZhou, Zongwei, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and\nJianming Liang. 2018. “UNet++: A Nested U-Net\nArchitecture for Medical Image Segmentation.”\narXiv. https://doi.org/10.48550/arXiv.1807.10165."
  }
]